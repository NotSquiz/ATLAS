# ATLAS Research Analysis: R22-R24 Skill System Patterns

**Date:** January 2026
**Scope:** Verification, ReAct/CoT patterns, Hybrid neural-symbolic architecture
**Purpose:** Validate master plan Section 14-18 against new research

---

## 1. Executive Summary

**Key Findings:**

1. **Self-Assessment Engine is fundamentally broken** - R22 proves that LLM self-verification without external grounding *degrades* quality. The master plan's "Self-Assessment System" must be replaced with externally-grounded verification.

2. **Process supervision delivers 5-8% accuracy improvement** - Verifying each step beats end-only checking. The current design only checks at the end.

3. **DAG-based parallel execution provides 3.6x speedup** - The master plan shows sequential workflow steps; R23 proves parallel execution is critical for meeting the 3-second budget.

4. **CLAUDE.md compound learning is missing entirely** - Boris Cherny's most powerful pattern for preventing recurring errors has no equivalent in the current design.

5. **Local LLM recommendation conflicts** - R24 recommends Qwen3-4B (2.75GB VRAM), R15 recommended Qwen 2.5 7B. Need resolution.

---

## 2. Validation Matrix

| Master Plan Element | R22-R24 Finding | Status |
|---------------------|-----------------|--------|
| Intent Router (40% semantic, 30% keyword, 30% pattern) | R24 confirms hybrid approach, adds confidence thresholds (0.9/0.7 bands) | ‚úÖ CONFIRMED + ENHANCE |
| Self-Assessment Engine (heuristics + LLM reflection) | R22: "Intrinsic self-correction without external feedback often degrades performance" | ‚ùå NEEDS REPLACEMENT |
| Workflow Execution (sequential steps) | R23: DAG-based parallel execution achieves 3.6x speedup | ‚ö†Ô∏è NEEDS REVISION |
| Proactive Triggers (4 types) | R23/R24 confirm pattern, recommend clearer trigger conditions | ‚úÖ CONFIRMED |
| Skill Schema (dataclass with slots) | R24: Add confidence field, verification hooks | ‚úÖ CONFIRMED + ENHANCE |
| MCP Tool Integration | R22: Add circuit breakers, retry with exponential backoff | ‚ö†Ô∏è NEEDS ENHANCEMENT |
| Memory System (SQLite + sqlite-vec) | R24 mentions ChromaDB but sqlite-vec verified in R12 | ‚úÖ STICK WITH SQLITE-VEC |
| Voice Latency Budget (<3s) | R23 confirms achievable with proper architecture | ‚úÖ CONFIRMED |

---

## 3. Gaps Discovered

### Gap 1: No External Verification Architecture

**What the master plan has:**
```
Self-Assessment System:
1. Heuristic checks (fast)
2. LLM self-reflection
3. Quality gate (score < 0.7 ‚Üí revise)
4. Revision loop
```

**What R22 proves:**
> "A foundational paper from ICLR 2024 confirms that intrinsic self-correction‚Äîasking a model to verify its own outputs without external feedback‚Äîoften degrades performance rather than improving it."

**Required change:** Replace self-assessment with externally-grounded verification:
- Tool execution results (tests, linters)
- API response validation (nutrition databases, exercise contraindications)
- Multi-source cross-reference
- Structured output schema validation (Pydantic)

### Gap 2: No DAG-Based Parallel Execution

**What the master plan has:**
```
Skill Executor:
‚îú‚îÄ‚îÄ Step 1: garmin_api.get_recovery_stats
‚îú‚îÄ‚îÄ Step 2: Transform injuries
‚îú‚îÄ‚îÄ Step 3: memory_query ‚Üí Recent workouts
‚îî‚îÄ‚îÄ Step 4: workout_generator
```

**What R23 proves:**
Steps 1 and 3 have NO dependencies - they should execute in parallel. R23 shows this pattern:

```python
plan_dag = {
    "t1": {"tool": "garmin_api", "deps": []},
    "t2": {"tool": "user_profile", "deps": []},    # Parallel with t1
    "t3": {"tool": "filter_exercises", "deps": ["t1", "t2"]},  # Waits
    "t4": {"tool": "generate_plan", "deps": ["t3"]}
}
```

**Impact:** 3.6x speedup on multi-tool tasks. Critical for 3-second budget.

### Gap 3: No Circuit Breaker Pattern

**What the master plan has:** Nothing about tool failure handling.

**What R22 recommends:**
```python
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10)
)
async def verified_tool_call(inputs):
    result = await execute_tool(inputs)
    if not validate_result(result):
        raise VerificationError("Output failed validation")
    return result
```

Plus circuit breakers: "After 5 consecutive failures to a service, stop calling it for 60 seconds to prevent cascade failures."

### Gap 4: No CLAUDE.md Compound Learning

**What the master plan has:** Nothing equivalent.

**What R24 recommends:**
Multi-tier configuration system:
- **System CLAUDE.md**: Core behaviors, safety constraints
- **User CLAUDE.md** (~/.atlas/context.md): Personal preferences, corrections
- **Domain CLAUDE.md**: Domain-specific knowledge (health, content)
- **Local overrides**: Session-specific context

Key insight: "Verification feedback loops dramatically improve output quality. Cherny notes that giving the AI a way to verify its work yields 2-3x quality improvement."

Learning capture rules:
- Only preferences demonstrated 3+ times across separate sessions graduate to persistent memory
- Distinguish one-off instructions from general preferences

### Gap 5: No Confidence-Based Routing Thresholds

**What the master plan has:** Confidence scoring but no action thresholds.

**What R24 recommends:**
```python
def route_by_confidence(state) -> str:
    if state["confidence"] > 0.9: return "execute"   # Auto-proceed
    if state["confidence"] > 0.7: return "confirm"   # Implicit confirmation
    return "clarify"                                  # Explicit question
```

### Gap 6: No Checkpointing/Recovery

**What the master plan has:** No fault tolerance for multi-step workflows.

**What R24 recommends:**
LangGraph's SqliteSaver for automatic checkpointing:
- Every node completion persists state
- Workflows resume from last checkpoint on failure
- Time-travel debugging for replaying from any state

```python
config = {
    "configurable": {
        "thread_id": "session_123",
        "checkpoint_id": "specific_checkpoint_uuid"
    }
}
graph.invoke(None, config=config)  # Fork from checkpoint
```

### Gap 7: Process Supervision Missing

**What the master plan has:** End-only quality gate.

**What R22 proves:**
> "Process reward models (PRMs) achieve 78.2% accuracy versus 72.4% for outcome reward models (ORMs)‚Äîa 5.8 percentage point improvement. The performance gap widens as solution complexity increases."

For ATLAS: Verify each phase of workout planning, not just the final output.

---

## 4. Conflicts & Resolutions

### Conflict 1: Local LLM Model Choice

| Source | Recommendation | VRAM |
|--------|----------------|------|
| R15 (ANALYSIS_R10-R15) | Qwen 2.5 7B Q4_K_M | ~4GB |
| R24 | Qwen3-4B Q4_K_M | ~2.75GB |

**Resolution: üîÑ ADAPT**
Qwen3-4B leaves more VRAM headroom (~1.25GB extra). R24 claims it "matches or exceeds older 70B+ models on reasoning benchmarks." However, R24's research date is ambiguous.

**Action:** Create R25 prompt to benchmark Qwen3-4B vs Qwen 2.5 7B specifically for ATLAS tasks. For now, start with Qwen3-4B (safer on VRAM).

### Conflict 2: Vector Database

| Source | Recommendation |
|--------|----------------|
| R12 (thorough analysis) | sqlite-vec |
| R24 (passing mention) | ChromaDB for semantic memory |

**Resolution: ‚ùå SKIP ChromaDB**
R12 conducted a thorough analysis. sqlite-vec v0.1.6 supports filtered vector search and fits RAM constraints. ChromaDB adds Python dependency weight. Stick with sqlite-vec.

### Conflict 3: Framework Choice

| Source | Recommendation |
|--------|----------------|
| Master Plan Section 6 | "Start with Claude native subagents. Add LangGraph only if complexity demands." |
| R23/R24 | Heavy LangGraph StateGraph references |

**Resolution: üîÑ ADAPT - Use Both**
They're complementary, not competing:
- **Claude Agent SDK**: For LLM calls, subagents, tool orchestration
- **LangGraph StateGraph**: For complex multi-step workflows requiring checkpointing and conditional branching

Pattern:
```
Simple skill (1-2 tools) ‚Üí Native Python + asyncio
Complex workflow (3+ tools, conditions) ‚Üí LangGraph StateGraph
```

### Conflict 4: STT Model

| Source | Recommendation |
|--------|----------------|
| R11, Master Plan | Moonshine Base (speed) or faster-whisper turbo (accuracy) |
| R24 | faster-whisper small (~1GB VRAM) |

**Resolution: ‚úÖ STICK WITH MASTER PLAN**
Moonshine Base is specifically optimized for edge deployment and latency (<700ms). faster-whisper "small" isn't the same as "turbo INT8". Keep Moonshine Base as default.

---

## 5. Categorized Recommendations

### ‚úÖ ADOPT (Use exactly as recommended)

| Finding | Source | Implementation |
|---------|--------|----------------|
| External verification pattern | R22 | Replace self-assessment with tool-grounded verification |
| Confidence thresholds (0.9/0.7/clarify) | R24 | Add to intent router |
| Circuit breaker + exponential backoff | R22 | Wrap all MCP tool calls |
| Process supervision | R22 | Verify each workflow step, not just end |
| CLAUDE.md system | R24 | Create multi-tier learning configuration |
| Hard limits (5 iterations, 2.5s voice) | R23 | Add to workflow executor |
| Streaming TTS from first tokens | R23 | Implement in voice pipeline |
| Immediate acknowledgment (<500ms) | R23 | "Let me check..." pattern |
| Reflect on failure only | R23 | Don't reflect every step |
| Observation masking | R23 | Replace old observations with placeholders |

### üîÑ ADAPT (Use with modifications for constraints)

| Finding | Source | Adaptation |
|---------|--------|------------|
| LangGraph StateGraph | R24 | Use only for complex workflows (3+ tools with conditions) |
| DAG-based parallel execution | R23 | Use asyncio.gather for simple parallelism, full DAG for complex |
| Qwen3-4B | R24 | Test against Qwen 2.5 7B before committing |
| Two-wave adversarial verification | R22 | Run sequentially (not parallel) for RAM constraints |
| MemGPT-style tiered memory | R23 | Enhance existing sqlite-vec with priority tiers |
| SqliteSaver checkpointing | R24 | Add to multi-step workflows only |

### ‚è∏Ô∏è DEFER (P2+ priority)

| Finding | Source | Reason |
|---------|--------|--------|
| Multi-agent verification (parallel) | R22 | RAM intensive, needs multiple model instances |
| Tree of Thoughts | R23 | Background tasks only, 3-10s+ latency |
| LATS (Monte Carlo Tree Search) | R23 | 10-60s+ latency, not voice-compatible |
| ReAcTree | R23 | 5-15s+ latency |
| DSPy declarative signatures | R24 | Adds complexity, evaluate post-MVP |
| Intent rewriting (DPO-trained) | R24 | Requires fine-tuning, not MVP |
| Semantic caching (30-40% reduction) | R23 | Good optimization, but adds complexity |

### ‚ùå SKIP (Doesn't fit architecture)

| Finding | Source | Reason |
|---------|--------|--------|
| ChromaDB | R24 | sqlite-vec already chosen, well-analyzed |
| Full LangGraph for everything | R23/R24 | Overkill for simple skills |
| Pure ToT/LATS for voice | R23 | Latency incompatible with 3-second budget |
| Fine-tuned self-correction | R22 | Requires training infrastructure |

### üîç NEEDS MORE RESEARCH (R25+)

| Gap | Proposed Research |
|-----|-------------------|
| Qwen3-4B vs Qwen 2.5 7B | Benchmark both on ATLAS-specific tasks |
| Supp.AI integration | Verify free API tier, integration patterns |
| Constitutional AI at inference | Practical implementation within latency budget |

---

## 6. Master Plan Revisions

### Section 14: Skill & Workflow Architecture

**Replace Self-Assessment System with Verification Engine:**

```
BEFORE:
Self-Assessment System:
1. HEURISTIC CHECKS (fast)
2. LLM SELF-REFLECTION
3. QUALITY GATE (score < 0.7 ‚Üí revise)
4. REVISION LOOP

AFTER:
Verification Engine (External Grounding):
1. SCHEMA VALIDATION (Pydantic, instant)
   ‚îî‚îÄ‚îÄ Structured output matches expected types
2. DOMAIN VERIFICATION (API-grounded)
   ‚îî‚îÄ‚îÄ Workout: contraindication DB, load progression rules
   ‚îî‚îÄ‚îÄ Health: Supp.AI drug interactions, safety bounds
   ‚îî‚îÄ‚îÄ Recipe: ingredient completeness, dietary compliance
   ‚îî‚îÄ‚îÄ Content: platform fit rules, readability scores
3. CROSS-REFERENCE (tool-based)
   ‚îî‚îÄ‚îÄ Compare against user history, preferences
4. QUALITY GATE
   ‚îî‚îÄ‚îÄ If fails ‚Üí attempt auto-fix with specific feedback
   ‚îî‚îÄ‚îÄ Max 2 iterations, then present with caveats
```

**Add DAG-Based Parallel Execution:**

```python
# BEFORE (sequential)
Step 1: garmin_api.get_recovery_stats
Step 2: user_profile.get_injuries
Step 3: memory_query.recent_workouts
Step 4: workout_generator.create_plan

# AFTER (DAG-parallel)
workflow_dag = {
    "garmin": {"tool": "garmin_api", "deps": []},
    "injuries": {"tool": "user_profile", "deps": []},
    "history": {"tool": "memory_query", "deps": []},
    "generate": {"tool": "workout_generator", "deps": ["garmin", "injuries", "history"]},
    "verify": {"tool": "verify_workout", "deps": ["generate"]}
}
# Steps 1-3 execute in parallel ‚Üí Step 4 ‚Üí Step 5
# Estimated speedup: 2-3x
```

**Add Circuit Breaker Configuration:**

```python
TOOL_CONFIG = {
    "circuit_breaker": {
        "failure_threshold": 5,
        "cooldown_seconds": 60,
        "timeout_seconds": 10
    },
    "retry": {
        "max_attempts": 3,
        "backoff_multiplier": 1,
        "max_wait": 10
    }
}
```

### Section 15: Implementation Phases (Revised)

**Phase 1 additions:**
- [ ] Create `~/.atlas/ATLAS.md` (system-level learning file)
- [ ] Implement basic DAG executor with asyncio.gather
- [ ] Add Pydantic schema validation for all tool outputs

**Phase 3 additions (rename: "Verification & Resilience"):**
- [ ] External verification APIs integration (Supp.AI for health)
- [ ] Circuit breaker wrapper for all MCP tools
- [ ] Process supervision hooks at each workflow step
- [ ] SqliteSaver checkpointing for multi-step workflows

**Phase 4 additions (rename: "Compound Learning"):**
- [ ] User-specific `~/.atlas/user.md` preferences file
- [ ] Domain-specific learning files (health.md, content.md)
- [ ] Graduation logic: 3+ occurrences ‚Üí persistent preference
- [ ] Error pattern capture and prevention

### Section 16: Critical Files to Create

**Add:**
```
/home/squiz/ATLAS/
‚îú‚îÄ‚îÄ atlas/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ verification.py       # External verification engine
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dag_executor.py       # DAG-based parallel execution
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ circuit_breaker.py    # Tool resilience wrapper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ checkpointing.py      # SqliteSaver integration
‚îÇ   ‚îú‚îÄ‚îÄ learning/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ atlas_md.py           # CLAUDE.md-style learning
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preference_graduate.py # Temp ‚Üí persistent logic
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ ATLAS.md                  # System behaviors (git-tracked)
‚îÇ   ‚îî‚îÄ‚îÄ domain/
‚îÇ       ‚îú‚îÄ‚îÄ health.md             # Health domain learnings
‚îÇ       ‚îî‚îÄ‚îÄ content.md            # Content domain learnings
‚îî‚îÄ‚îÄ ~/.atlas/
    ‚îú‚îÄ‚îÄ user.md                   # User preferences (local)
    ‚îî‚îÄ‚îÄ local_overrides.md        # Session-specific (ephemeral)
```

### New Section 18: Verification Strategies by Domain

**Workout Verification:**
```python
def verify_workout(plan, user_profile):
    checks = {
        "load_progression": max_weekly_increase(plan) <= 0.15,
        "recovery_adequate": check_muscle_recovery(plan, 48),
        "contraindications": no_flagged_exercises(plan, user_profile.conditions),
        "equipment_available": all_equipment_owned(plan, user_profile.equipment),
        "muscle_balance": check_symmetry(plan)
    }
    return all(checks.values()), checks
```

**Health Protocol Verification:**
- Query Supp.AI (2,044 supplements √ó 2,866 drugs)
- Classify severity: Major (block) ‚Üí Moderate (warn) ‚Üí Minor (inform)
- Always display: "Not medical advice. Consult healthcare provider."

**Recipe Verification:**
- NLP check: all ingredients appear in steps
- Dietary compliance matrix (vegetarian, vegan, keto, allergen)
- Food safety: cooking temperatures/times

**Content Verification:**
- Platform fit: duration, format, hook presence
- Readability: Flesch-Kincaid matched to audience
- Factual: cross-reference cited sources

---

## 7. Additional Research Needed

**Full research prompts:** `/home/squiz/ATLAS/docs/research/RESEARCH_PROMPTS_R25-R26.md`

### R25: Local LLM Selection for Resource-Constrained Personal AI

Comprehensive comparison of Qwen3-4B vs Qwen 2.5 7B vs alternatives:
- 10 sections covering VRAM usage, latency, quality benchmarks
- ATLAS-specific task evaluation (intent classification, slot extraction, workout generation)
- Voice latency budget analysis
- Memory optimization techniques (Flash Attention, KV cache quantization)
- Quantization comparison (Q4_K_M vs Q5_K_M vs Q8_0)
- Integration guidance (Ollama, llama.cpp configuration)
- Hybrid local/cloud routing decision criteria
- The morning scenario as benchmark test

### R26: Verification API Integration for Personal AI Domains

Practical integration guidance for external verification across all domains:
- Health & supplement verification (Supp.AI, DrugBank, FDA databases)
- Exercise & workout verification (contraindication databases, safety rules)
- Nutrition & recipe verification (USDA, Nutritionix, dietary compliance)
- Content creation verification (plagiarism, AI detection, factual checking)
- Caching & rate limiting strategy
- Offline fallback architecture for safety-critical domains
- Privacy & compliance (FDA CDS guidance, HIPAA)
- Budget allocation within $5/month for verification APIs
- Latency budget (<500ms per verification)
- Unified VerificationEngine interface design

### R27: LangGraph Integration Patterns for Voice-First Agents

When and how to integrate LangGraph with Claude Agent SDK + MCP:
- LangGraph vs Claude Agent SDK comparison
- Decision criteria for when LangGraph overhead is justified
- StateGraph patterns within 3-second voice budget
- MCP + LangGraph integration patterns
- Checkpointing with SqliteSaver
- Confidence-based conditional routing
- Parallel execution with Send API
- Complete morning workout example

---

## 8. Morning Scenario Revisited

**Original (Master Plan):**
```
1. Voice Layer ‚Üí Transcribe speech
2. Intent Router ‚Üí Match "health.morning_workout"
3. Slot Extraction ‚Üí day=monday, injury=shoulder, types=[yoga, calisthenics]
4. Skill Executor ‚Üí Sequential steps
5. Self-Assessment ‚Üí Score and present
6. Persona Layer ‚Üí Format response
7. Voice Output ‚Üí Speak
```

**Revised (Post R22-R24):**
```
1. Voice Layer ‚Üí Transcribe speech
2. Intent Router ‚Üí Match "health.morning_workout" (confidence 0.94 > 0.9 ‚Üí execute)
3. Slot Extraction ‚Üí day=monday, injury=shoulder, types=[yoga, calisthenics]
4. Immediate Acknowledgment ‚Üí "Let me prepare that workout." (<500ms)
5. DAG Executor (parallel):
   ‚îú‚îÄ‚îÄ garmin_api ‚Üí recovery=72, hrv=52
   ‚îú‚îÄ‚îÄ user_profile ‚Üí injury history
   ‚îî‚îÄ‚îÄ memory_query ‚Üí recent Monday workouts
6. Generate Plan ‚Üí workout_generator (waits for all above)
7. External Verification:
   ‚îú‚îÄ‚îÄ Schema validation (Pydantic)
   ‚îú‚îÄ‚îÄ Contraindication check (no overhead movements with shoulder injury)
   ‚îú‚îÄ‚îÄ Load progression check (<15% increase)
   ‚îî‚îÄ‚îÄ Equipment availability check
8. If verification fails ‚Üí auto-fix with specific feedback (max 2 iterations)
9. Persona Layer ‚Üí Format as Lethal Gentleman
10. Streaming TTS ‚Üí Start speaking from first tokens
11. Log to ATLAS.md ‚Üí If user corrects anything, capture for future

Total target: <3 seconds (achievable with parallel execution + streaming)
```

---

## 9. Summary: Build Readiness

| Aspect | Before R22-R24 | After R22-R24 |
|--------|----------------|---------------|
| Self-Assessment | ‚ùå Fundamentally flawed | ‚úÖ Replaced with external verification |
| Workflow Execution | ‚ö†Ô∏è Sequential, slow | ‚úÖ DAG-parallel, 2-3x faster |
| Tool Resilience | ‚ùå No failure handling | ‚úÖ Circuit breakers + retry |
| Learning System | ‚ùå Missing | ‚úÖ CLAUDE.md compound learning |
| Confidence Routing | ‚ö†Ô∏è No action thresholds | ‚úÖ 0.9/0.7/clarify bands |
| Checkpointing | ‚ùå No recovery | ‚úÖ SqliteSaver for complex workflows |
| Process Supervision | ‚ùå End-only | ‚úÖ Per-step verification |

**Verdict:** The master plan was approximately 70% correct. R22-R24 identified critical gaps in verification architecture, parallel execution, and compound learning that would have caused significant quality and latency issues.

**Next Step:** Revise master plan Sections 14-18 with findings above, then proceed to implementation.

---

## Appendix: Key Code Patterns from R22-R24

### External Verification (R22)
```python
def verify_workout(plan, user_profile):
    checks = {
        "load_progression": max_weekly_increase(plan) <= 0.15,
        "recovery_adequate": check_muscle_recovery(plan, 48),
        "contraindications": no_flagged_exercises(plan, user_profile.conditions),
        "equipment_available": all_equipment_owned(plan, user_profile.equipment),
    }
    return all(checks.values()), checks
```

### DAG Executor (R23)
```python
async def execute_dag(dag):
    results = {}
    for level in topological_sort(dag):
        level_tasks = [execute(task, results) for task in level]
        level_results = await asyncio.gather(*level_tasks)
        results.update(zip([t.id for t in level], level_results))
    return results
```

### Confidence Routing (R24)
```python
def route_by_confidence(state) -> str:
    if state["confidence"] > 0.9: return "execute"
    if state["confidence"] > 0.7: return "confirm"
    return "clarify"
```

### Circuit Breaker (R22)
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10))
async def verified_tool_call(inputs):
    result = await execute_tool(inputs)
    if not validate_result(result):
        raise VerificationError("Output failed validation")
    return result
```

### Streaming Acknowledgment (R23)
```python
async def stream_with_acknowledgment(query, websocket):
    await tts.speak("Let me check...")  # Within 500ms
    async for event in agent.astream_events({"messages": [query]}):
        if event["event"] == "on_llm_stream":
            await tts.stream_chunk(event["data"]["chunk"])
```
