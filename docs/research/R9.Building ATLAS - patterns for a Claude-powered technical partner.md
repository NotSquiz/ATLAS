# Building ATLAS: patterns for a Claude-powered technical partner

Claude API combined with MCP servers provides a robust foundation for building ATLAS as an intelligent coding partner. The architecture leverages **MCP's tool-first design** for integrating development workflows, **SQLite for persistent memory** across sessions, and **AST-based code analysis** for deep repository understanding—all optimized for the 16GB RAM constraint before hybrid scaling to local LLMs.

This report covers concrete implementation patterns for codebase understanding, code generation, quality assurance integration, debugging, and project-specific learning, with emphasis on Claude API and MCP-specific approaches.

---

## Capability matrix by project type

ATLAS capabilities differ between Python and TypeScript projects based on available tooling and analysis approaches. The matrix below maps each capability to language-specific implementations.

| Capability | Python Implementation | TypeScript Implementation |
|------------|----------------------|--------------------------|
| **AST Analysis** | Built-in `ast` module + tree-sitter | ts-morph (TypeScript Compiler API wrapper) |
| **Dependency Graphs** | `ast-scope`, `modulegraph`, networkx | ts-morph `findReferences()`, TypeScript project references |
| **Linting** | Ruff (10-100x faster than alternatives) | ESLint with programmatic API |
| **Testing** | pytest with `--json-report` | Jest with `--json` output |
| **Type Checking** | mypy, pyright | TypeScript strict mode (`tsc --noEmit`) |
| **Security Scanning** | Bandit, Safety (dependencies) | npm audit, Snyk |
| **Code Formatting** | Ruff format | Prettier via ESLint |
| **Embedding Model** | all-MiniLM-L6-v2 (384 dims, memory-efficient) | Same model via ONNX runtime |
| **Vector DB** | ChromaDB or LanceDB (embedded, disk-persistent) | Same - JavaScript bindings available |

### Python-specific patterns (60% of ATLAS core)

Python's introspection capabilities enable rich code analysis. The `ast` module provides full access to parsed syntax trees without external dependencies:

```python
import ast
from typing import Dict, Set

class CodeAnalyzer(ast.NodeVisitor):
    def __init__(self):
        self.functions: Dict[str, dict] = {}
        self.call_graph: Dict[str, Set[str]] = {}
        self.current_function = None
    
    def visit_FunctionDef(self, node: ast.FunctionDef):
        self.functions[node.name] = {
            'args': [arg.arg for arg in node.args.args],
            'returns': ast.unparse(node.returns) if node.returns else None,
            'docstring': ast.get_docstring(node),
            'lineno': node.lineno,
        }
        prev = self.current_function
        self.current_function = node.name
        self.call_graph[node.name] = set()
        self.generic_visit(node)
        self.current_function = prev
    
    def visit_Call(self, node: ast.Call):
        if self.current_function and isinstance(node.func, ast.Name):
            self.call_graph[self.current_function].add(node.func.id)
        self.generic_visit(node)
```

For cross-file dependency tracking, combine AST analysis with module resolution:

```python
from pathlib import Path
import networkx as nx

class ProjectDependencyAnalyzer:
    def __init__(self, root: str):
        self.root = Path(root)
        self.graph = nx.DiGraph()
    
    def analyze(self):
        modules = {self._path_to_module(p): p for p in self.root.rglob("*.py")}
        for module, filepath in modules.items():
            imports = self._extract_imports(filepath)
            for imp in imports:
                if imp in modules:
                    self.graph.add_edge(module, imp)
        return self.graph
```

### TypeScript-specific patterns (40% for web/mobile)

ts-morph wraps the TypeScript Compiler API for type-aware analysis:

```typescript
import { Project, SourceFile } from "ts-morph";

const project = new Project({ tsConfigFilePath: "tsconfig.json" });

function extractFunctions(sourceFile: SourceFile) {
    return sourceFile.getFunctions().map(func => ({
        name: func.getName(),
        parameters: func.getParameters().map(p => ({
            name: p.getName(),
            type: p.getType().getText()
        })),
        returnType: func.getReturnType().getText(),
        isExported: func.isExported(),
    }));
}

// Find all references across the codebase
function findAllUsages(symbolName: string) {
    const symbol = project.getSourceFiles()
        .flatMap(sf => sf.getFunctions())
        .find(f => f.getName() === symbolName);
    return symbol?.findReferences() || [];
}
```

---

## Integration points via MCP

MCP servers provide the bridge between Claude and development tools. Each integration follows a consistent pattern: wrap the CLI tool, parse structured output, and return actionable results.

### Test runner integration (pytest and jest)

**Pytest MCP tool** with structured JSON output:

```python
from mcp.server import Server
from mcp.types import Tool, TextContent
import subprocess, json

server = Server("test-runner")

@server.list_tools()
async def list_tools():
    return [Tool(
        name="run_pytest",
        description="Run pytest with filtering and coverage. Returns structured results with pass/fail counts and failure details.",
        inputSchema={
            "type": "object",
            "properties": {
                "target": {"type": "string", "default": "."},
                "filter": {"type": "string", "description": "-k expression"},
                "coverage": {"type": "boolean", "default": False},
            }
        }
    )]

@server.call_tool()
async def call_tool(name: str, arguments: dict):
    if name == "run_pytest":
        cmd = ["pytest", arguments.get("target", "."), "--tb=short", "--json-report", "--json-report-file=-"]
        if arguments.get("filter"):
            cmd.extend(["-k", arguments["filter"]])
        if arguments.get("coverage"):
            cmd.extend(["--cov=src", "--cov-report=json"])
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        return parse_pytest_output(result.stdout, result.stderr)
```

**Jest MCP tool** with equivalent structure:

```typescript
server.registerTool("run_jest", {
    description: "Run Jest tests with JSON output for structured results",
    inputSchema: {
        testPath: z.string().optional(),
        testNamePattern: z.string().optional(),
        coverage: z.boolean().default(false),
    }
}, async ({ testPath, testNamePattern, coverage }) => {
    const args = ["jest", "--json"];
    if (testPath) args.push(testPath);
    if (testNamePattern) args.push("-t", testNamePattern);
    if (coverage) args.push("--coverage", "--coverageReporters=json-summary");
    
    const result = await execAsync(args.join(" "));
    return { content: [{ type: "text", text: formatJestResults(JSON.parse(result.stdout)) }] };
});
```

### Linting integration (ruff and eslint)

**Ruff MCP tool** for Python—10-100x faster than flake8/black combined:

```python
@server.list_tools()
async def list_tools():
    return [
        Tool(
            name="lint_python",
            description="Run Ruff linter with optional auto-fix. Returns issues grouped by file with rule codes and suggested fixes.",
            inputSchema={
                "type": "object",
                "properties": {
                    "path": {"type": "string", "default": "."},
                    "fix": {"type": "boolean", "default": False},
                    "select": {"type": "array", "items": {"type": "string"}, "description": "Rule codes to enable"},
                }
            }
        )
    ]

async def lint_python(args: dict):
    cmd = ["ruff", "check", args.get("path", "."), "--output-format=json"]
    if args.get("fix"):
        cmd.append("--fix")
    if args.get("select"):
        cmd.extend(["--select", ",".join(args["select"])])
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    issues = json.loads(result.stdout) if result.stdout else []
    
    return format_lint_results(issues, fixed=args.get("fix", False))
```

**ESLint programmatic API** for TypeScript—avoids CLI overhead:

```typescript
import { ESLint } from "eslint";

server.registerTool("lint_typescript", {
    description: "Run ESLint with auto-fix option. Returns errors and warnings with rule IDs and fix suggestions.",
    inputSchema: {
        patterns: z.array(z.string()).default(["."]),
        fix: z.boolean().default(false),
    }
}, async ({ patterns, fix }) => {
    const eslint = new ESLint({ fix, extensions: [".ts", ".tsx"] });
    const results = await eslint.lintFiles(patterns);
    
    if (fix) await ESLint.outputFixes(results);
    
    const summary = {
        errors: results.reduce((sum, r) => sum + r.errorCount, 0),
        warnings: results.reduce((sum, r) => sum + r.warningCount, 0),
        fixable: results.reduce((sum, r) => sum + r.fixableErrorCount + r.fixableWarningCount, 0),
    };
    
    return { content: [{ type: "text", text: formatESLintResults(results, summary) }] };
});
```

### Pre-commit and CI/CD awareness

**Pre-commit hook runner** for catching issues before push:

```python
@server.list_tools()
async def list_tools():
    return [
        Tool(
            name="run_precommit",
            description="Run pre-commit hooks on staged or all files. Useful to verify code will pass CI checks.",
            inputSchema={
                "type": "object",
                "properties": {
                    "all_files": {"type": "boolean", "default": False},
                    "hook_ids": {"type": "array", "items": {"type": "string"}},
                }
            }
        ),
        Tool(
            name="parse_ci_config",
            description="Parse GitHub Actions workflows to understand what checks will run remotely.",
            inputSchema={"type": "object"}
        )
    ]
```

**CI configuration parser** to predict failures:

```python
import yaml
from pathlib import Path

async def parse_ci_config(args: dict):
    workflows = []
    for yml in Path(".github/workflows").glob("*.yml"):
        with open(yml) as f:
            config = yaml.safe_load(f)
        
        workflows.append({
            "file": yml.name,
            "name": config.get("name"),
            "triggers": list(config.get("on", {}).keys()),
            "jobs": [
                {
                    "name": name,
                    "steps": [s.get("run", s.get("uses", "")) for s in job.get("steps", [])]
                }
                for name, job in config.get("jobs", {}).items()
            ]
        })
    
    return format_ci_summary(workflows)
```

### Security scanning integration

**Bandit for Python** and **npm audit for Node**:

```python
# Python security scanning
async def run_security_scan(args: dict):
    # Bandit for code vulnerabilities
    bandit_cmd = ["bandit", "-r", args.get("path", "."), "-f", "json"]
    bandit_result = subprocess.run(bandit_cmd, capture_output=True, text=True)
    
    # Safety for dependency vulnerabilities
    safety_cmd = ["safety", "check", "--json"]
    safety_result = subprocess.run(safety_cmd, capture_output=True, text=True)
    
    return combine_security_results(
        json.loads(bandit_result.stdout),
        json.loads(safety_result.stdout)
    )
```

---

## Memory schema for persistent project knowledge

SQLite with FTS5 provides **sub-millisecond search** across project memories, decisions, and learned patterns. The schema below supports all memory types ATLAS needs.

### Core memory tables

```sql
PRAGMA journal_mode = WAL;
PRAGMA foreign_keys = ON;

-- Core memories (decisions, patterns, conventions, interactions)
CREATE TABLE memories (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    hash TEXT UNIQUE NOT NULL,           -- Content hash for deduplication
    content TEXT NOT NULL,
    title TEXT,
    memory_type TEXT DEFAULT 'general',  -- 'decision', 'pattern', 'convention', 'debug_session'
    confidence REAL DEFAULT 1.0,         -- 0.0-1.0, decays over time
    source TEXT,                         -- Origin of this memory
    file_paths TEXT,                     -- JSON array of related files
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    is_active INTEGER DEFAULT 1
);

-- Full-text search with Porter stemming
CREATE VIRTUAL TABLE memories_fts USING fts5(
    content, title, tokenize='porter unicode61'
);

-- Auto-sync FTS index
CREATE TRIGGER memories_ai AFTER INSERT ON memories BEGIN
    INSERT INTO memories_fts(rowid, content, title) VALUES (new.id, new.content, new.title);
END;

CREATE TRIGGER memories_ad AFTER DELETE ON memories BEGIN
    INSERT INTO memories_fts(memories_fts, rowid, content, title) 
    VALUES ('delete', old.id, old.content, old.title);
END;
```

### Architecture decision records

```sql
CREATE TABLE decisions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    adr_number INTEGER UNIQUE,
    title TEXT NOT NULL,
    status TEXT DEFAULT 'proposed',      -- 'proposed', 'accepted', 'deprecated', 'superseded'
    context TEXT NOT NULL,               -- Problem or need
    decision TEXT NOT NULL,              -- What was decided
    consequences TEXT,                   -- Trade-offs
    alternatives TEXT,                   -- JSON array
    superseded_by INTEGER REFERENCES decisions(id),
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    decided_at DATETIME
);

-- Link decisions to code locations
CREATE TABLE decision_code_links (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    decision_id INTEGER NOT NULL REFERENCES decisions(id) ON DELETE CASCADE,
    file_path TEXT NOT NULL,
    start_line INTEGER,
    end_line INTEGER,
    link_type TEXT DEFAULT 'implements'  -- 'implements', 'affects', 'violates'
);
```

### Code entity graph

```sql
-- Code entities for dependency tracking
CREATE TABLE code_entities (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    entity_type TEXT NOT NULL,           -- 'file', 'function', 'class', 'module'
    name TEXT NOT NULL,
    qualified_name TEXT,                 -- Full path for disambiguation
    file_path TEXT,
    start_line INTEGER,
    end_line INTEGER,
    signature TEXT,
    docstring TEXT,
    hash TEXT,                           -- Content hash for change detection
    last_seen DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Relationships between entities
CREATE TABLE code_relationships (
    source_id INTEGER NOT NULL REFERENCES code_entities(id) ON DELETE CASCADE,
    target_id INTEGER NOT NULL REFERENCES code_entities(id) ON DELETE CASCADE,
    relationship_type TEXT NOT NULL,     -- 'calls', 'imports', 'inherits', 'uses'
    weight REAL DEFAULT 1.0,
    PRIMARY KEY (source_id, target_id, relationship_type)
);

-- Recursive call graph query using CTE
-- Example: Find all functions called by a given function (transitive)
-- WITH RECURSIVE call_graph AS (
--     SELECT target_id, 1 as depth FROM code_relationships 
--     WHERE source_id = :start_id AND relationship_type = 'calls'
--     UNION ALL
--     SELECT cr.target_id, cg.depth + 1 FROM code_relationships cr
--     INNER JOIN call_graph cg ON cr.source_id = cg.target_id
--     WHERE cr.relationship_type = 'calls' AND cg.depth < 10
-- )
-- SELECT DISTINCT * FROM call_graph;
```

### Project conventions and patterns

```sql
CREATE TABLE conventions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    convention_type TEXT NOT NULL,       -- 'naming', 'formatting', 'structure', 'pattern'
    category TEXT,                       -- 'function', 'class', 'file', 'variable'
    pattern TEXT NOT NULL,               -- Regex or description
    description TEXT,
    examples TEXT,                       -- JSON array from codebase
    counter_examples TEXT,               -- JSON array of violations
    confidence REAL DEFAULT 0.5,
    occurrence_count INTEGER DEFAULT 1,
    source TEXT,                         -- 'detected', 'user_defined', 'feedback'
    is_active INTEGER DEFAULT 1,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Track success/failure of approaches in debugging
CREATE TABLE approach_outcomes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    problem_description TEXT NOT NULL,
    approach_tried TEXT NOT NULL,
    outcome TEXT NOT NULL,               -- 'success', 'failure', 'partial'
    root_cause TEXT,
    solution TEXT,
    lessons_learned TEXT,
    file_paths TEXT,                     -- JSON array
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Debug session memory for similar error recognition
CREATE TABLE debug_sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    error_signature TEXT,                -- Hash/pattern of error
    error_message TEXT NOT NULL,
    stack_trace TEXT,
    investigation_steps TEXT,            -- JSON array
    resolution TEXT,
    resolution_type TEXT,                -- 'fix', 'workaround', 'known_issue'
    times_encountered INTEGER DEFAULT 1,
    last_encountered DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

### Conversation and session tracking

```sql
CREATE TABLE sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT UNIQUE NOT NULL,
    project_path TEXT,
    started_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    ended_at DATETIME,
    summary TEXT,                        -- AI-generated summary
    files_modified TEXT,                 -- JSON array
    decisions_made TEXT                  -- JSON array of decision IDs
);

CREATE TABLE conversation_turns (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT NOT NULL REFERENCES sessions(session_id) ON DELETE CASCADE,
    turn_number INTEGER NOT NULL,
    role TEXT NOT NULL,                  -- 'user', 'assistant'
    content TEXT NOT NULL,
    content_summary TEXT,                -- Compressed for retrieval
    files_referenced TEXT,               -- JSON array
    importance REAL DEFAULT 0.5,         -- For context prioritization
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Code feedback for learning
CREATE TABLE code_feedback (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT,
    generated_code TEXT NOT NULL,
    file_path TEXT,
    feedback_type TEXT NOT NULL,         -- 'accepted', 'rejected', 'modified'
    modification TEXT,
    rejection_reason TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

### Efficient retrieval queries

```sql
-- Hybrid search: FTS5 + recency + confidence weighting
WITH scored AS (
    SELECT 
        m.*,
        bm25(memories_fts) as text_score,
        julianday('now') - julianday(m.created_at) as age_days
    FROM memories m
    JOIN memories_fts ON m.id = memories_fts.rowid
    WHERE memories_fts MATCH :query AND m.is_active = 1
),
ranked AS (
    SELECT *,
        (text_score * -1 + confidence - age_days/30.0) as final_score,
        SUM(LENGTH(content)) OVER (ORDER BY final_score DESC) as running_length
    FROM scored
)
SELECT * FROM ranked WHERE running_length < :max_chars ORDER BY final_score DESC;
```

---

## Example workflows

### Workflow 1: intelligent code review

This workflow uses MCP tools to gather context, run checks, and provide substantive review.

**Step 1: Gather context** — Use file tools to read changed files and understand dependencies

```python
# System prompt for code review
REVIEW_SYSTEM_PROMPT = """<role>
You are a senior engineer reviewing code changes. Your review should be thorough but pragmatic.
</role>

<review_process>
1. First, use read_file to examine each changed file
2. Use find_references to understand how changed code is used elsewhere  
3. Run lint_python or lint_typescript to check for style issues
4. Run run_pytest or run_jest to verify tests pass
5. Check for security issues with run_security_scan
6. Only then provide your review with specific, actionable feedback
</review_process>

<anti_patterns>
- Don't suggest changes without understanding the context
- Don't flag "issues" that are project conventions
- Don't nitpick formatting if linter passes
</anti_patterns>"""
```

**Step 2: Execute review pipeline**

```python
async def review_changes(changed_files: list[str], client: anthropic.Anthropic):
    tools = [
        {"type": "text_editor_20250429", "name": "text_editor"},
        # Custom MCP tools
        {"name": "lint_python", ...},
        {"name": "run_pytest", ...},
        {"name": "find_references", ...},
    ]
    
    messages = [{
        "role": "user",
        "content": f"Review these changed files: {changed_files}. "
                   f"Run linting and relevant tests before providing feedback."
    }]
    
    # Tool loop - Claude will call tools as needed
    while True:
        response = client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=8192,
            system=REVIEW_SYSTEM_PROMPT,
            tools=tools,
            messages=messages
        )
        
        tool_uses = [b for b in response.content if b.type == "tool_use"]
        if not tool_uses:
            return response  # Final review response
        
        # Execute tools and continue
        messages.append({"role": "assistant", "content": response.content})
        tool_results = [execute_tool(t) for t in tool_uses]
        messages.append({"role": "user", "content": tool_results})
```

### Workflow 2: test-driven development

Claude generates tests first, then implementation, with continuous verification.

**Step 1: Generate test skeleton matching project style**

```python
# Analyze existing tests to match conventions
async def analyze_test_style(test_dir: str) -> dict:
    patterns = {"fixtures": [], "assertions": [], "naming": []}
    
    for test_file in Path(test_dir).rglob("test_*.py"):
        content = test_file.read_text()
        if "@pytest.fixture" in content:
            patterns["fixtures"].append("pytest fixtures")
        if "assert " in content:
            patterns["assertions"].append("plain assert")
        # Extract test function naming patterns
        tree = ast.parse(content)
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef) and node.name.startswith("test_"):
                patterns["naming"].append(node.name)
    
    return patterns
```

**Step 2: TDD conversation flow**

```python
TDD_PROMPTS = [
    "Write tests for {feature}. Match the project's test style. Don't implement yet.",
    "Run the tests and confirm they fail as expected.",
    "Now implement the minimum code to make tests pass.",
    "Run tests again. If any fail, debug and fix. If all pass, we're done."
]

async def tdd_workflow(feature_description: str, client):
    conversation = []
    
    for prompt_template in TDD_PROMPTS:
        prompt = prompt_template.format(feature=feature_description)
        conversation.append({"role": "user", "content": prompt})
        
        response = await run_with_tools(client, conversation)
        conversation.append({"role": "assistant", "content": response.content})
        
        # Check if tests passed in the final step
        if "all pass" in response.content[-1].text.lower():
            break
    
    return conversation
```

### Workflow 3: debugging with memory

This workflow uses persistent memory to recognize similar errors and apply past solutions.

**Step 1: Check for similar past errors**

```python
async def debug_with_memory(error_message: str, stack_trace: str, db: sqlite3.Connection):
    # Search for similar past errors
    cursor = db.execute("""
        SELECT error_signature, resolution, resolution_type, times_encountered
        FROM debug_sessions
        WHERE error_message LIKE ? OR error_signature = ?
        ORDER BY times_encountered DESC, last_encountered DESC
        LIMIT 5
    """, (f"%{error_message[:100]}%", hash_error(error_message)))
    
    similar_errors = cursor.fetchall()
    
    if similar_errors:
        context = "Similar errors encountered before:\n"
        for err in similar_errors:
            context += f"- {err['resolution_type']}: {err['resolution']}\n"
    else:
        context = "No similar errors found in memory."
    
    return context
```

**Step 2: Debug conversation with tool access**

```python
DEBUG_SYSTEM_PROMPT = """<role>
You are debugging an error. Use available tools to investigate.
</role>

<process>
1. Read the error message and stack trace carefully
2. Use read_file to examine the failing code
3. Use find_references to check how the failing function is called
4. Form a hypothesis about the root cause
5. Use run_pytest with a filter to test your hypothesis
6. If needed, use text_editor to apply a fix
7. Verify the fix works by running tests again
</process>

<memory_context>
{past_error_context}
</memory_context>"""

async def debug_error(error: str, stack_trace: str, client, db):
    past_context = await debug_with_memory(error, stack_trace, db)
    
    response = await run_with_tools(
        client,
        messages=[{
            "role": "user", 
            "content": f"Debug this error:\n```\n{error}\n```\n\nStack trace:\n```\n{stack_trace}\n```"
        }],
        system=DEBUG_SYSTEM_PROMPT.format(past_error_context=past_context)
    )
    
    # Store successful resolution in memory
    if "fixed" in response.content[-1].text.lower():
        store_debug_session(db, error, stack_trace, extract_resolution(response))
    
    return response
```

### Workflow 4: refactoring with safety checks

**Step 1: Build dependency graph before changes**

```python
async def safe_refactor(target_symbol: str, new_implementation: str, client, db):
    # Find all usages before refactoring
    usages = db.execute("""
        SELECT ce.file_path, ce.start_line, ce.name
        FROM code_entities ce
        JOIN code_relationships cr ON ce.id = cr.source_id
        WHERE cr.target_id = (SELECT id FROM code_entities WHERE name = ?)
    """, (target_symbol,)).fetchall()
    
    # Create safety checklist
    checklist = f"""
    ## Refactoring: {target_symbol}
    
    ### Files that will be affected:
    {chr(10).join(f"- [ ] {u['file_path']}:{u['start_line']}" for u in usages)}
    
    ### Pre-refactor checks:
    - [ ] All tests pass
    - [ ] Linting passes
    - [ ] Type checking passes
    """
    
    return checklist, usages
```

**Step 2: Execute with verification at each step**

```python
REFACTOR_PROMPTS = """
<refactoring_rules>
1. Run all tests before any changes to establish baseline
2. Make one change at a time
3. Run tests after each change
4. If tests fail, revert immediately
5. Update all usages found in the dependency graph
6. Run type checker after all changes
</refactoring_rules>
"""
```

---

## Claude API best practices for ATLAS

### System prompt structure

Use XML tags for clear organization and include project-specific context:

```python
def build_system_prompt(project_context: str, conventions: str) -> str:
    return f"""<role>
You are ATLAS, a technical partner AI for this codebase.
</role>

<project_context>
{project_context}
</project_context>

<conventions>
{conventions}
</conventions>

<investigation_rules>
Never speculate about code you haven't read. If asked about a file, 
use read_file BEFORE answering. Always verify your assumptions.
</investigation_rules>

<anti_overengineering>
Make minimal changes. Don't add features beyond what's requested.
Don't refactor "while you're there." Trust existing code.
</anti_overengineering>"""
```

### Tool description quality

Good tool descriptions dramatically improve routing accuracy. Include: what the tool does, when to use it, what it returns, and any limitations.

```python
Tool(
    name="run_pytest",
    description="""Run pytest on Python test files with optional filtering and coverage.

Use when: You need to verify tests pass, debug a failing test, or check coverage.
Returns: Structured JSON with pass/fail counts, failure details with tracebacks, and coverage percentage if requested.
Limitations: Timeout after 60 seconds. Cannot run tests requiring external services.

Parameters:
- target: Test file, directory, or test::function pattern
- filter: -k expression to filter test names  
- coverage: Enable coverage collection (slower)""",
    inputSchema={...}
)
```

### Context window management

With **200K standard tokens** (1M extended), prioritize context inclusion:

```python
async def assemble_context(query: str, db: sqlite3.Connection) -> str:
    context_budget = 150_000  # Leave room for response
    context_parts = []
    
    # 1. Repository map (always include, ~5K tokens)
    repo_map = generate_repo_map()
    context_parts.append(("repo_map", repo_map, 5000))
    
    # 2. Relevant memories (variable, ~2-10K tokens)
    memories = search_memories(query, db, max_results=10)
    context_parts.append(("memories", format_memories(memories), estimate_tokens(memories)))
    
    # 3. Relevant code files (variable, up to remaining budget)
    relevant_files = search_code(query, db)
    for file_path, relevance in relevant_files:
        content = read_file(file_path)
        tokens = estimate_tokens(content)
        if sum(p[2] for p in context_parts) + tokens < context_budget:
            context_parts.append((file_path, content, tokens))
    
    return assemble_xml_context(context_parts)
```

### Prompt caching for efficiency

Cache static context (codebase structure, conventions) to reduce latency **2-10x** and costs up to **90%**:

```python
response = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=4096,
    system=[{
        "type": "text",
        "text": large_codebase_context,
        "cache_control": {"type": "ephemeral"}  # Cache this block
    }],
    messages=[{"role": "user", "content": user_query}]
)
```

---

## MCP server architecture for ATLAS

### Recommended structure

```
atlas-mcp/
├── src/
│   ├── server.py           # Main entry point
│   ├── tools/
│   │   ├── testing.py      # pytest, jest wrappers
│   │   ├── linting.py      # ruff, eslint, mypy
│   │   ├── git.py          # git operations
│   │   ├── files.py        # read, write, search
│   │   └── security.py     # bandit, npm audit
│   ├── resources/
│   │   └── project.py      # Project file access
│   └── utils/
│       ├── subprocess.py   # Safe execution helpers
│       └── parsing.py      # Output parsers
├── pyproject.toml
└── claude_desktop_config.json
```

### Security requirements

**Never use `shell=True`** in subprocess calls—this prevents command injection:

```python
# ❌ Vulnerable
subprocess.run(f"ls {user_input}", shell=True)

# ✅ Safe - arguments are escaped automatically
subprocess.run(["ls", user_input])
```

**Validate all file paths** to prevent directory traversal:

```python
from pathlib import Path

ALLOWED_ROOT = Path("/workspace").resolve()

def validate_path(user_path: str) -> Path:
    requested = (ALLOWED_ROOT / user_path).resolve()
    if not str(requested).startswith(str(ALLOWED_ROOT)):
        raise ValueError("Path outside allowed directory")
    return requested
```

---

## Conclusion

ATLAS can be built as a capable technical partner using Claude API and MCP servers with these key architectural decisions:

**Codebase understanding** relies on AST analysis (Python `ast` module, ts-morph for TypeScript) combined with tree-sitter for incremental parsing. The repository map pattern—ranking code by PageRank centrality and fitting to context budget—provides efficient whole-codebase awareness without exceeding token limits.

**Persistent memory** in SQLite with FTS5 enables sub-millisecond retrieval of past decisions, debug sessions, and learned conventions. The schema separates memories by type (decisions, patterns, debug sessions) while maintaining relationships through foreign keys and a code entity graph.

**QA integration via MCP** wraps pytest, jest, ruff, and eslint as tools with structured JSON output. The key pattern: parse tool output into actionable formats rather than raw text, enabling Claude to make informed decisions about fixes.

**Learning over time** happens through feedback loops: track which generated code was accepted/rejected/modified, update convention confidence scores, and store successful debugging approaches for similar future errors.

The 16GB RAM constraint is manageable with embedded databases (ChromaDB/LanceDB), memory-efficient embeddings (384-dim all-MiniLM-L6-v2), and lazy loading of detailed ASTs. Future hybrid architecture with DeepSeek R1 32B on Mac Mini can offload long-running analysis while Claude handles interactive sessions.