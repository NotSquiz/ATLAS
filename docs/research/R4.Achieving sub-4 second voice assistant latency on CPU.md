# Achieving sub-4 second voice assistant latency on CPU

Your **6-15 second latency can realistically drop to 1.5-3 seconds** through model optimization, streaming architecture, and intelligent parallelization. The key insight: streaming and overlapping operations matter more than raw model speed—time-to-first-audio is what users perceive as "fast."

## The optimized latency budget

| Component | Current | Optimized | Strategy |
|-----------|---------|-----------|----------|
| STT | 2-4s | **0.5-1.0s** | Moonshine tiny or faster-whisper base.en+int8 |
| LLM | 3-8s | **0.3-0.8s** (TTFT) | Claude 3.5 Haiku streaming |
| TTS | 1-3s | **0.2-0.4s** | Piper or optimized Kokoro |
| **Total** | **6-15s** | **1.5-3.0s** | Overlapped pipeline |

The breakthrough: with streaming, LLM and TTS operations overlap significantly. Users hear the first audio **800-1500ms** after speaking ends, even though the full response takes longer.

## STT: the fastest path to accurate transcription

Your current faster-whisper small model with INT8 is solid but oversized for voice assistant commands. **Moonshine tiny delivers 5x speedup** with comparable accuracy for clear speech.

**Recommended configuration for sub-1 second STT:**

```python
# Option A: Moonshine (fastest - 0.3-0.6s for 3-5s audio)
import moonshine
text = moonshine.transcribe_with_onnx("audio.wav", "moonshine/tiny")

# Option B: Optimized faster-whisper (0.5-1.0s)
from faster_whisper import WhisperModel

model = WhisperModel(
    "base.en",              # Balance of speed and accuracy
    device="cpu",
    compute_type="int8",
    cpu_threads=4           # Match physical cores
)

segments, _ = model.transcribe(
    audio_buffer,
    beam_size=1,            # Greedy decoding: 2-3x speedup
    language="en",
    vad_filter=True,
    vad_parameters={
        "min_silence_duration_ms": 300,  # Faster silence trimming
        "speech_pad_ms": 100,
        "threshold": 0.5
    },
    condition_on_previous_text=False,
    word_timestamps=False   # Disable if not needed
)
```

**Model accuracy tradeoffs** (Word Error Rate on clean English):
- **tiny.en**: 12.7% WER, fastest
- **base.en**: 8-10% WER, good balance
- **small.en**: 5-7% WER, your current model

For voice commands with clear audio, tiny or base models are sufficient. The **5-7 percentage point WER increase** is rarely noticeable in practice for command-style inputs.

## LLM: Claude Haiku streaming eliminates the wait

Claude 3.5 Haiku's **0.36 second time-to-first-token** versus Sonnet's 0.64s makes it the right choice for voice. Combined with streaming, users start hearing responses before generation completes.

**Streaming response pattern:**

```python
from anthropic import AsyncAnthropic

client = AsyncAnthropic()

async def stream_to_tts(user_input: str, tts_queue: asyncio.Queue):
    """Stream LLM tokens directly to TTS sentence queue"""
    sentence_buffer = ""
    
    async with client.messages.stream(
        max_tokens=150,  # Keep concise for voice
        model="claude-3-5-haiku-20241022",
        system="Voice assistant. Reply in 1-2 short sentences. No markdown.",
        messages=[{"role": "user", "content": user_input}]
    ) as stream:
        async for token in stream.text_stream:
            sentence_buffer += token
            
            # Emit complete sentences to TTS immediately
            if any(sentence_buffer.rstrip().endswith(p) for p in '.!?'):
                await tts_queue.put(sentence_buffer.strip())
                sentence_buffer = ""
    
    # Flush remaining text
    if sentence_buffer.strip():
        await tts_queue.put(sentence_buffer.strip())
    await tts_queue.put(None)  # Signal completion
```

**Prompt optimization cuts latency by 20-40%:**
- Every 500 input tokens adds ~25ms TTFB
- Keep system prompts under 100 tokens
- Request concise responses explicitly
- `max_tokens=150` is ideal for spoken responses

**Semantic caching for frequent queries** can yield 60-90% latency reduction on cache hits. GPTCache achieves **61-68% hit rates** in production voice assistants:

```python
from gptcache import cache
from gptcache.embedding import Onnx
from gptcache.manager import manager_factory
from gptcache.similarity_evaluation import SearchDistanceEvaluation

cache.init(
    pre_embedding_func=lambda x: x["messages"][-1]["content"],
    embedding_func=Onnx(),
    data_manager=manager_factory("sqlite", "faiss"),
    similarity_evaluation=SearchDistanceEvaluation(),
)
```

For maximum speed, **Groq's Llama 3.3 70B** achieves 0.1-0.3s TTFB at 275 tokens/second—roughly 2x faster than Claude Haiku, though with different capabilities.

## TTS: Piper outperforms Kokoro on CPU latency

Your Kokoro ONNX setup achieves good quality but **~500ms is its CPU floor** according to maintainer benchmarks. For latency-critical responses, **Piper delivers 100-300ms** with comparable neural quality.

**Piper TTS implementation:**

```bash
# One-time setup
wget https://github.com/rhasspy/piper/releases/latest/download/piper_linux_x86_64.tar.gz
tar -xzf piper_linux_x86_64.tar.gz
```

```python
import subprocess
import asyncio

class PiperTTS:
    def __init__(self, model_path="en_US-amy-medium.onnx"):
        self.model = model_path
    
    async def synthesize(self, text: str) -> bytes:
        proc = await asyncio.create_subprocess_exec(
            "./piper",
            "--model", self.model,
            "--output-raw",
            stdin=asyncio.subprocess.PIPE,
            stdout=asyncio.subprocess.PIPE
        )
        audio, _ = await proc.communicate(text.encode())
        return audio
```

**If keeping Kokoro**, optimize ONNX Runtime settings:

```python
import onnxruntime as ort

sess_options = ort.SessionOptions()
sess_options.intra_op_num_threads = 4       # Parallelize within ops
sess_options.inter_op_num_threads = 1       # Sequential between ops
sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
sess_options.enable_mem_pattern = True
sess_options.enable_cpu_mem_arena = True

session = ort.InferenceSession("kokoro-v1.0.onnx", sess_options, 
                               providers=['CPUExecutionProvider'])
```

**Pre-generate common phrases** to eliminate synthesis entirely for frequent responses:

```python
CACHED_PHRASES = {
    "greeting": "Hello, how can I help you?",
    "confirm": "Got it.",
    "processing": "Let me check that for you.",
    "thanks": "You're welcome!",
}

# Synthesize at startup, serve from cache
```

## Pipeline architecture that hides latency

The critical optimization is **overlapping operations**. Start TTS while LLM generates; start LLM while STT finalizes. This async generator pattern enables streaming at every stage:

```python
import asyncio
from dataclasses import dataclass
from typing import AsyncIterator
import time

@dataclass
class PipelineMetrics:
    request_id: str
    stt_start: float = 0
    llm_first_token: float = 0
    tts_first_audio: float = 0
    
    @property
    def time_to_first_audio_ms(self) -> float:
        return (self.tts_first_audio - self.stt_start) * 1000

class StreamingVoicePipeline:
    def __init__(self, stt_model, llm_client, tts_engine):
        self.stt = stt_model
        self.llm = llm_client
        self.tts = tts_engine
        self.tts_queue = asyncio.Queue()
    
    async def process(self, audio_data: bytes) -> AsyncIterator[bytes]:
        """Full streaming pipeline with overlap"""
        metrics = PipelineMetrics(request_id=str(time.time()))
        metrics.stt_start = time.perf_counter()
        
        # STT
        transcript = await self.stt.transcribe(audio_data)
        
        # Start LLM streaming and TTS consumption concurrently
        llm_task = asyncio.create_task(
            self._stream_llm(transcript, metrics)
        )
        
        # Yield TTS audio as sentences complete
        async for audio_chunk in self._consume_tts_queue(metrics):
            yield audio_chunk
        
        await llm_task
        self._log_metrics(metrics)
    
    async def _stream_llm(self, text: str, metrics: PipelineMetrics):
        """Stream LLM tokens to TTS queue"""
        sentence_buffer = ""
        first_token = True
        
        async with self.llm.messages.stream(
            max_tokens=150,
            model="claude-3-5-haiku-20241022",
            messages=[{"role": "user", "content": text}]
        ) as stream:
            async for token in stream.text_stream:
                if first_token:
                    metrics.llm_first_token = time.perf_counter()
                    first_token = False
                
                sentence_buffer += token
                if any(sentence_buffer.rstrip().endswith(p) for p in '.!?'):
                    await self.tts_queue.put(sentence_buffer.strip())
                    sentence_buffer = ""
        
        if sentence_buffer.strip():
            await self.tts_queue.put(sentence_buffer.strip())
        await self.tts_queue.put(None)
    
    async def _consume_tts_queue(self, metrics: PipelineMetrics) -> AsyncIterator[bytes]:
        """Synthesize and yield audio as sentences arrive"""
        first_audio = True
        
        while True:
            sentence = await self.tts_queue.get()
            if sentence is None:
                break
            
            audio = await self.tts.synthesize(sentence)
            
            if first_audio:
                metrics.tts_first_audio = time.perf_counter()
                first_audio = False
            
            yield audio
```

## Latency instrumentation for continuous improvement

Track every component with JSONL logging to identify bottlenecks:

```python
import json
import time
from pathlib import Path

class LatencyTracker:
    def __init__(self, log_path: str = "latency.jsonl"):
        self.log_path = Path(log_path)
        self.metrics = []
    
    def log(self, metrics: dict):
        metrics["timestamp"] = time.time()
        with open(self.log_path, "a") as f:
            f.write(json.dumps(metrics) + "\n")
        self.metrics.append(metrics)
    
    def percentiles(self, field: str) -> dict:
        values = sorted(m[field] for m in self.metrics if field in m)
        n = len(values)
        return {
            "p50": values[n // 2] if values else 0,
            "p95": values[int(n * 0.95)] if values else 0,
            "count": n
        }

# Example log entry
tracker.log({
    "request_id": "abc123",
    "stt_ms": 850,
    "llm_ttft_ms": 340,
    "tts_first_ms": 280,
    "total_ttfa_ms": 1470,  # Time to first audio
    "total_ms": 2800
})
```

**Target P95 latencies:**
- STT: <1000ms
- LLM TTFT: <500ms
- TTS first chunk: <400ms
- **Time to first audio: <1800ms**

## Implementation roadmap

**Week 1 (immediate gains):**
1. Switch to Claude 3.5 Haiku with streaming (30-40% LLM improvement)
2. Add beam_size=1 and VAD tuning to faster-whisper (40-50% STT improvement)
3. Implement sentence-level TTS streaming from LLM tokens

**Week 2 (model optimization):**
1. Evaluate Moonshine tiny vs faster-whisper base.en for your use case
2. Test Piper TTS against Kokoro for latency comparison
3. Pre-warm all models at startup

**Week 3 (advanced optimization):**
1. Implement semantic caching with GPTCache
2. Pre-generate top 10-20 common responses
3. Add full pipeline instrumentation with JSONL logging

**Week 4 (polish):**
1. Implement barge-in detection (interruption handling)
2. Add adaptive buffering for smooth audio playback
3. Set up P50/P95 monitoring dashboard

## Benchmark methodology

Test with standardized audio samples representing your use cases:

```python
TEST_CASES = [
    ("short_command.wav", "turn on the lights", 2.5),      # 2.5s audio
    ("question.wav", "what's the weather today", 3.0),     # 3.0s audio
    ("complex.wav", "set a reminder for tomorrow at 3pm", 4.5),  # 4.5s audio
]

async def benchmark_pipeline(pipeline, iterations=20):
    results = []
    
    for audio_file, expected_text, audio_duration in TEST_CASES:
        for _ in range(iterations):
            audio_data = load_audio(audio_file)
            
            start = time.perf_counter()
            first_audio_time = None
            
            async for audio_chunk in pipeline.process(audio_data):
                if first_audio_time is None:
                    first_audio_time = time.perf_counter()
                # Play audio_chunk
            
            end = time.perf_counter()
            
            results.append({
                "test_case": audio_file,
                "ttfa_ms": (first_audio_time - start) * 1000,
                "total_ms": (end - start) * 1000
            })
    
    return analyze_results(results)
```

With these optimizations, your ATLAS voice pipeline should consistently achieve **1.5-3 second total latency** with **800-1500ms time-to-first-audio**—well within your <4 second target and providing a responsive, natural conversation experience.