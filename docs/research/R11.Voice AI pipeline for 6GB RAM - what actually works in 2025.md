# Voice AI pipeline for 6GB RAM: what actually works in 2025

Sub-3 second end-to-end latency on your hardware is **achievable but tight**. The optimal stack is **Moonshine/faster-whisper turbo + Kokoro-82M + Silero VAD**, but WSL2 audio latency is the hidden bottleneck—expect **50-300ms overhead** from audio alone. A hybrid approach (Windows audio capture, WSL2 ML inference) delivers the best real-world results.

Your RTX 3050 Ti's **4GB VRAM works fine** in WSL2—CUDA support is mature. The real constraint is audio routing, not GPU access. Here's the complete technical breakdown with measured numbers.

---

## Speech-to-text: faster-whisper turbo wins on accuracy-per-VRAM

**faster-whisper** (v1.2.1, October 2025) remains the production standard with **19k+ GitHub stars** and excellent CTranslate2 optimization. The game-changer for your hardware is **large-v3-turbo INT8**: only **1.5GB VRAM** with **1.92% WER**—better accuracy than large-v3 at one-third the memory.

| Model | VRAM (INT8) | Latency (10s audio) | WER | Status |
|-------|-------------|---------------------|-----|--------|
| Moonshine Tiny | ~1GB | <500ms | 12.7% | ✅ Stable |
| Moonshine Base | ~1.2GB | <700ms | 10.1% | ✅ Stable |
| faster-whisper small | ~1.5GB | ~800ms | ~8% | ✅ Stable |
| **faster-whisper turbo INT8** | **~1.5GB** | **~1.5s** | **1.9%** | ✅ Stable |
| distil-large-v3 INT8 | ~1.5GB | ~1.2s | 2.4% | ✅ Stable |
| NVIDIA Parakeet 0.6B | ~2GB | <100ms | 6.1% | ✅ (NeMo) |

**beam_size=1 vs default**: Cuts latency by **2-3x** with only +0.5-1.5% WER penalty. Essential for hitting your latency target.

**Moonshine** (Useful Sensors, October 2024) is the dark horse: designed for edge devices with **5-15x speed improvement** over Whisper at equivalent accuracy. The 27M parameter Tiny model processes utterances in **under 500ms** while consuming ~1GB VRAM. Full ONNX runtime support makes it deployment-ready. The catch: English-only and weaker on clips under 1 second.

**Distil-Whisper** integrates directly with faster-whisper (`model_size="distil-large-v3"`) and excels at long-form transcription with its chunked algorithm. For conversational turn-taking with short utterances, faster-whisper turbo or Moonshine outperform it.

**NVIDIA Parakeet TDT 0.6B** achieves the lowest WER on Open ASR benchmarks (**6.05%**) with an extraordinary **RTFx of 3380** (transcribes 60 minutes in 1 second). However, it requires the NeMo framework—significantly more setup complexity than faster-whisper's pip install.

---

## Text-to-speech: Kokoro-82M delivers near-human quality at 300ms

**Kokoro-82M** ranks **#1 on TTS Spaces Arena** with quality surpassing XTTS-v2 and MetaVoice while fitting comfortably in your constraints. The ~350MB model achieves **sub-300ms latency** on GPU with full ONNX support via `NeuML/kokoro-base-onnx`.

| Model | Latency (~50 words) | RAM | VRAM | Quality | Streaming | License |
|-------|---------------------|-----|------|---------|-----------|---------|
| **Kokoro-82M** | **<300ms (GPU)** | ~500MB | ~1-2GB | ★★★★★ | Yes | Apache 2.0 |
| Piper TTS | <1s (CPU) | <256MB | N/A | ★★★☆☆ | Yes | MIT |
| MeloTTS | <1s | ~400MB | Optional | ★★★★☆ | Limited | MIT |
| XTTS-v2 | <200ms (streaming) | ~4.6GB | ~2.7GB | ★★★★★ | Yes | Non-commercial |

Kokoro-FastAPI (4.2k stars) provides production deployment with OpenAI-compatible endpoints, tested on **Windows 11 WSL2 with NVIDIA GPUs**. Streaming first-token latency hits **~300ms at chunksize 400**.

**Piper TTS** is your fallback for absolute minimum resources: **<256MB RAM**, 60MB models, sub-1-second generation on CPU alone. The `en_US-lessac-high` voice is the most natural-sounding, though noticeably more robotic than Kokoro.

**XTTS-v2** technically fits your 4GB VRAM but consumes **4.6GB system RAM** plus ~2.7GB VRAM—marginal on your hardware. The Coqui company shut down in early 2024, but **Idiap Research maintains an active fork** (`pip install coqui-tts`). Non-commercial license restricts production use.

**MeloTTS** (MyShell.ai) offers a lightweight ~200MB alternative with MIT licensing and OpenVINO optimization for NPU deployment—worth considering if you want to offload TTS to integrated graphics.

---

## VAD remains Silero's domain, but TEN VAD challenges end-of-speech detection

**Silero VAD v6.2** (November 2025) processes audio at **<1ms per 30ms chunk**—effectively zero latency overhead. The 2.2MB ONNX model achieves **87.7% TPR at 5% FPR**, four times better than WebRTC VAD.

Optimal configuration for conversational turn-taking:

```python
threshold = 0.5          # Increase to 0.6-0.7 in noisy environments
min_speech_duration_ms = 250    # Filter short noise bursts
min_silence_duration_ms = 400   # Balance responsiveness vs clipping
speech_pad_ms = 100            # Prevent clipping utterance edges
```

**TEN VAD** (released 2025) claims faster speech-to-non-speech detection than Silero's "several hundred milliseconds" delay on transitions. At **306KB** versus Silero's 2.16MB, it's worth testing if you experience sluggish turn-taking. However, it's less battle-tested.

**WebRTC VAD**: Avoid. Its 50% TPR means it misses half of speech frames—unusable for quality voice interaction.

**PyAnnote Audio VAD**: Requires 6-8GB VRAM minimum. Not viable for your constraints.

The critical insight: VAD processing time is negligible; your **actual latency comes from `min_silence_duration_ms`**. Setting 400ms means you wait 400ms after speech ends before triggering STT. This is the primary tuning lever for responsiveness versus utterance completeness.

---

## WSL2 GPU works perfectly; audio is the hidden latency killer

**CUDA support in WSL2 is mature and stable.** Your RTX 3050 Ti (Ampere, compute capability 8.6) works with full CUDA 12.x support. No Linux NVIDIA driver installation needed—the Windows Game Ready/Studio driver handles everything.

Setup requires only:
```bash
# WSL-specific CUDA toolkit (NOT cuda-drivers)
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt install cuda-toolkit-12-4

# PyTorch with CUDA
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

**faster-whisper CUDA works in WSL2**—confirmed by multiple production users. Use INT8 quantization on 4GB VRAM; small/base/turbo models fit comfortably. Sporadic "CUDA out of memory" errors relate to WSL2 memory management, not actual VRAM exhaustion.

**Audio is the problem.** WSLg provides PulseAudio via RDP transport with inherent buffering. Real-world latency: **50-300ms typical**, with reports of spikes to 10+ seconds under load. Microsoft GitHub issues #607, #908, #1113 document ongoing problems.

| Audio Approach | Latency | Reliability |
|----------------|---------|-------------|
| WSLg PulseAudio (default) | 50-300ms+ | Inconsistent |
| Windows PulseAudio server | ~30-50ms | Good |
| USB passthrough via USBIPD | ~20-40ms | Complex setup |
| Native Windows | ~10-20ms | Best |

**PipeWire is not supported** in WSLg despite community requests. Installing PipeWire in your distro breaks WSLg audio completely—disable it with `systemctl --user mask pipewire pipewire-pulse wireplumber`.

---

## Realistic latency: sub-3 seconds requires a hybrid architecture

Given your components, here's the honest breakdown:

| Scenario | VAD | STT | LLM* | TTS | Audio I/O | Total |
|----------|-----|-----|------|-----|-----------|-------|
| **Optimistic** | 300ms | 500ms (Moonshine) | 500ms | 300ms (Kokoro) | 100ms | **~1.7s** |
| **Realistic** | 400ms | 1.2s (turbo INT8) | 800ms | 400ms | 200ms | **~3.0s** |
| **Pessimistic** | 600ms | 2.0s | 1.5s | 700ms | 500ms | **~5.3s** |

*LLM latency assumes a quantized local model like Llama 3.2 3B or Phi-3.5 Mini

**Sub-3 second is achievable** with aggressive optimization:
- Moonshine Tiny for STT (~500ms)
- Kokoro streaming with first-chunk playback (~300ms to first audio)
- beam_size=1 for STT
- min_silence_duration_ms=300 for VAD (risk: clipping mid-thought)
- Hybrid audio: Windows capture → WSL2 processing → Windows playback

**The hybrid architecture** eliminates WSL2's audio bottleneck:

```
Windows 11
├── Audio capture (sounddevice/PyAudio native): ~10ms latency
├── Audio playback (native): ~10ms latency
└── TCP socket communication with WSL2

WSL2 Ubuntu
├── Receive audio chunks via socket
├── Silero VAD + Moonshine/faster-whisper on CUDA
├── Local LLM inference
├── Kokoro TTS generation
└── Return audio via socket
```

This adds ~5-10ms socket overhead but saves 100-300ms on audio I/O.

---

## Component recommendations ranked by your constraints

**STT (pick one):**
1. **Moonshine Base** — Best latency (<700ms), 10% WER, fits 4GB VRAM easily
2. **faster-whisper turbo INT8** — Best accuracy (1.9% WER), ~1.5s latency, 1.5GB VRAM
3. **faster-whisper small INT8** — Balanced option, multilingual support

**TTS (pick one):**
1. **Kokoro-82M via ONNX** — Best quality, sub-300ms GPU latency, streaming support
2. **Piper `en_US-lessac-high`** — Minimal resources, <1s CPU latency, slightly robotic

**VAD:**
1. **Silero VAD** — Production standard, <1ms overhead, well-documented
2. **TEN VAD** — Consider if Silero's end-of-speech detection feels slow

**Architecture:**
1. **Hybrid Windows/WSL2** — Best latency, more complex
2. **Full WSL2** — Simpler, accept 100-300ms audio overhead

---

## Conclusion: achievable with tradeoffs

Your hardware can run a responsive local voice assistant. The **optimal stack—Moonshine + Kokoro + Silero VAD on WSL2 CUDA—delivers sub-3 second latency** in favorable conditions. The critical tradeoffs:

- **WSL2 audio adds 100-300ms minimum**; hybrid architecture eliminates this but adds complexity
- **Accuracy vs speed**: Moonshine (12.7% WER, <500ms) versus faster-whisper turbo (1.9% WER, ~1.5s)
- **VRAM headroom**: With STT + TTS both on GPU, you're using ~2.5GB of 4GB—leaves room for a small LLM but not much else

All recommended components are **production-stable as of January 2025**, not bleeding edge. faster-whisper, Silero VAD, and Piper have years of production use. Kokoro-82M and Moonshine are newer (late 2024) but actively maintained with MIT/Apache licensing.

Start with the simpler full-WSL2 approach to validate the pipeline, then migrate to hybrid if the audio latency is unacceptable for your use case.