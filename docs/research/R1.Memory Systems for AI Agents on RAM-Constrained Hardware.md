# Memory Systems for AI Agents on RAM-Constrained Hardware

Voice-first AI agents can operate effectively on severely constrained hardware by combining SQLite with lightweight embeddings, spaced repetition-inspired consolidation, and careful context injection strategies. The architecture outlined here targets **4-6GB usable RAM**, achieves **<15ms embedding latency** via ONNX-optimized models, and maintains a clean migration path to production-grade infrastructure like Qdrant and Memgraph.

## SQLite becomes a capable vector database with the right schema

The foundation of this memory system is a unified SQLite database storing episodic, semantic, and core memories with embedded vectors. The **sqlite-vec** extension (successor to sqlite-vss) provides vector search with only **30MB default RAM usage**—well within your constraints. Combined with FTS5 for text search, you get hybrid retrieval without external dependencies.

```sql
-- Core Memory: Always-available identity and preferences
CREATE TABLE core_memory (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    memory_type TEXT NOT NULL CHECK(memory_type IN ('persona', 'human', 'system')),
    key TEXT NOT NULL,
    value TEXT NOT NULL,
    importance REAL DEFAULT 0.5,
    created_at TEXT DEFAULT (datetime('now')),
    updated_at TEXT DEFAULT (datetime('now')),
    embedding BLOB,
    UNIQUE(memory_type, key)
);

-- Episodic Memory: Timestamped experiences and conversations
CREATE TABLE episodic_memory (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT NOT NULL,
    timestamp TEXT DEFAULT (datetime('now')),
    observation TEXT,
    action TEXT,
    result TEXT,
    context_tags TEXT,  -- JSON array
    importance REAL DEFAULT 0.5,
    access_count INTEGER DEFAULT 0,
    last_accessed TEXT,
    stability REAL DEFAULT 1.0,      -- FSRS stability parameter
    retrievability REAL DEFAULT 1.0, -- Current memory strength
    embedding BLOB
);

-- Semantic Memory: Extracted facts and knowledge (triple store)
CREATE TABLE semantic_memory (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    subject TEXT NOT NULL,
    predicate TEXT NOT NULL,
    object TEXT NOT NULL,
    confidence REAL DEFAULT 1.0,
    source_episode_ids TEXT,  -- JSON array of source IDs
    created_at TEXT DEFAULT (datetime('now')),
    last_reinforced TEXT,
    embedding BLOB,
    metadata TEXT
);

-- FTS5 for text search with BM25 ranking
CREATE VIRTUAL TABLE episodic_fts USING fts5(
    observation, action, result, context_tags,
    content='episodic_memory',
    content_rowid='id',
    tokenize='porter unicode61 remove_diacritics 2'
);

-- sqlite-vec virtual table for vector search
CREATE VIRTUAL TABLE memory_vectors USING vec0(
    memory_id INTEGER PRIMARY KEY,
    embedding float[384],
    memory_type TEXT PARTITION KEY
);
```

**FTS5 performance on <100K records** shows single-digit millisecond latency for basic MATCH queries. BM25 ranking adds minimal overhead, though `ORDER BY rank` can be expensive at scale—use LIMIT early and avoid JOINs with FTS5 tables.

### Hybrid search with Reciprocal Rank Fusion

The most effective retrieval combines keyword and semantic search using **Reciprocal Rank Fusion (RRF)**. This approach first retrieves candidates from both FTS5 and vector search, then fuses rankings with the formula `score = 1/(k + rank)` where k=60 is standard:

```python
def hybrid_search(query: str, query_embedding: np.ndarray, conn, k: int = 10):
    # Stage 1: Get candidates from both methods
    fts_results = conn.execute("""
        SELECT rowid, bm25(episodic_fts) as score
        FROM episodic_fts WHERE episodic_fts MATCH ?
        LIMIT ?
    """, (query, k * 3)).fetchall()
    
    vec_results = conn.execute("""
        SELECT memory_id, distance FROM memory_vectors
        WHERE embedding MATCH ? AND k = ?
    """, (query_embedding.tobytes(), k * 3)).fetchall()
    
    # Stage 2: RRF fusion
    rrf_scores = {}
    for rank, (idx, _) in enumerate(fts_results, 1):
        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1.0 / (60 + rank)
    for rank, (idx, _) in enumerate(vec_results, 1):
        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1.0 / (60 + rank)
    
    return sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:k]
```

For systems without sqlite-vec, pure Python brute-force cosine similarity achieves **~5ms per query on 100K pre-normalized embeddings** using NumPy—acceptable for your latency budget.

## all-MiniLM-L6-v2 delivers optimal performance for your constraints

After evaluating embedding models under 100MB, **all-MiniLM-L6-v2 with ONNX quantization** emerges as the clear winner for RAM-constrained systems.

| Model | Size | Dims | MTEB Score | CPU Latency | RAM |
|-------|------|------|------------|-------------|-----|
| **all-MiniLM-L6-v2 (ONNX int8)** | ~63MB | 384 | ~56 | **10-15ms** | ~100MB |
| all-MiniLM-L6-v2 (PyTorch) | ~86MB | 384 | ~56 | 20-26ms | ~150MB |
| bge-small-en-v1.5 | ~66MB | 384 | ~62 | 15-20ms | ~120MB |
| e5-small-v2 | ~66MB | 384 | ~58 | 15-20ms | ~120MB |
| paraphrase-MiniLM-L3-v2 | ~50MB | 384 | ~50 | 8-12ms | ~80MB |

ONNX quantization provides **2x speedup** over PyTorch defaults. Configure with AVX512 or AVX2 depending on your CPU:

```python
from sentence_transformers import SentenceTransformer, export_dynamic_quantized_onnx_model

model = SentenceTransformer("all-MiniLM-L6-v2", backend="onnx")
export_dynamic_quantized_onnx_model(
    model=model,
    quantization_config="avx512_vnni",  # or "avx2" for older CPUs
    model_name_or_path="./optimized_model"
)

# Pre-compute embeddings at ingestion (float16 saves 50% storage)
corpus_embeddings = model.encode(
    documents, 
    normalize_embeddings=True,  # Pre-normalize for dot product
    batch_size=32
).astype(np.float16)
```

**Memory budget for 100K records:** ~73MB for embeddings (float16), ~100MB for model, ~50MB for FTS5 index, plus ~100MB working memory. **Total: ~350MB**—leaving 3.5GB+ headroom in your 4GB usable RAM.

## FSRS-inspired forgetting curves determine what to remember

The **Free Spaced Repetition Scheduler (FSRS)** provides the most accurate memory retention model, outperforming SM-2 by 20-30% in research. Adapt it for AI memory by tracking stability and retrievability per-memory:

**Retrievability formula (FSRS-4.5+):**
$$R(t,S) = \left(1 + \frac{19}{81} \cdot \frac{t}{S}\right)^{-0.5}$$

Where `t` = time since last access, `S` = stability (interval at which R=90%).

```python
import math
from datetime import datetime

def calculate_memory_strength(memory: dict, current_time: datetime) -> tuple:
    """Calculate FSRS-inspired retrievability and importance score."""
    time_since_access = (current_time - memory['last_accessed']).days
    stability = memory.get('stability', 1.0)
    access_count = memory.get('access_count', 0)
    
    # FSRS retrievability formula
    FACTOR = 19/81
    DECAY = -0.5
    retrievability = (1 + FACTOR * (time_since_access / max(stability, 0.1))) ** DECAY
    
    # Composite importance score
    importance = (
        0.4 * retrievability +
        0.3 * min(1.0, access_count / 10) +
        0.2 * memory.get('confidence', 0.5) +
        0.1 * (1.0 if memory.get('type') == 'semantic' else 0.5)
    )
    return importance, retrievability

def update_stability_on_access(memory: dict, recall_quality: int) -> float:
    """Update stability using SM-2 inspired formula (0-5 quality rating)."""
    ef = memory.get('easiness_factor', 2.5)
    new_ef = ef + (0.1 - (5 - recall_quality) * (0.08 + (5 - recall_quality) * 0.02))
    new_ef = max(1.3, new_ef)
    
    if recall_quality >= 3:
        new_stability = memory['stability'] * new_ef
    else:
        new_stability = 1.0  # Reset on failed recall
    
    return new_stability, new_ef
```

### Weekly consolidation extracts semantic facts from episodes

Run batch consolidation weekly to extract patterns and prune low-importance memories. Use a three-tier strategy: **keep** (importance >0.7), **summarize** (0.3-0.7), **archive** (<0.3):

```python
def weekly_consolidation(conn, llm_client):
    """Episodic → Semantic consolidation with forgetting curves."""
    # 1. Update all retrievability scores
    conn.execute("""
        UPDATE episodic_memory SET
            retrievability = POWER(1 + (19.0/81.0) * 
                (julianday('now') - julianday(last_accessed)) / stability, -0.5)
    """)
    
    # 2. Extract patterns from frequent observations
    patterns = conn.execute("""
        SELECT observation, COUNT(*) as freq 
        FROM episodic_memory 
        WHERE timestamp > datetime('now', '-7 days')
        GROUP BY observation HAVING freq >= 3
    """).fetchall()
    
    # 3. LLM-assisted semantic extraction
    recent_episodes = conn.execute("""
        SELECT * FROM episodic_memory 
        WHERE timestamp > datetime('now', '-7 days')
        ORDER BY importance DESC LIMIT 100
    """).fetchall()
    
    semantic_facts = llm_client.extract_facts(recent_episodes)
    
    # 4. Prune or summarize low-importance memories
    low_importance = conn.execute("""
        SELECT * FROM episodic_memory WHERE importance < 0.3
    """).fetchall()
    
    for memory in low_importance:
        if memory['importance'] < 0.1:
            archive_and_delete(conn, memory)
        else:
            summarized = llm_client.summarize(memory)
            update_with_summary(conn, memory, summarized)
```

## Context injection requires careful positioning to avoid the middle

The "Lost in the Middle" phenomenon (Liu et al., 2024) shows LLMs perform significantly worse when key information sits in the middle of context. **Position your most relevant memories at the beginning and end** using a "sandwich" ordering strategy.

**Recommended token allocation for Claude API:**
- System prompt: **10%** (~10K tokens for 100K context)
- Retrieved memories: **50%** (~50K tokens)
- Conversation history: **25%** (~25K tokens)
- Current query + output buffer: **15%**

Target **50-60% total context utilization**—research shows diminishing returns beyond 70%.

```python
def order_memories_sandwich(memories: list[dict]) -> list[dict]:
    """Order memories to mitigate lost-in-the-middle effect."""
    sorted_mems = sorted(memories, key=lambda x: x['score'], reverse=True)
    n = len(sorted_mems)
    
    # Top third at beginning, middle third at end, bottom third in middle
    top = sorted_mems[:n//3]
    middle = sorted_mems[n//3:2*n//3]
    bottom = sorted_mems[2*n//3:]
    
    return top + bottom + middle

def combine_recency_relevance(similarity: float, timestamp: datetime, 
                              current: datetime, lambda_rel: float = 0.6) -> float:
    """Balance semantic relevance with temporal recency."""
    age_hours = (current - timestamp).total_seconds() / 3600
    half_life_hours = 7 * 24  # 7 days
    
    recency = math.exp(-math.log(2) * max(0, age_hours - 24) / half_life_hours)
    return lambda_rel * similarity + (1 - lambda_rel) * recency
```

For diversity, apply **Maximal Marginal Relevance (MMR)** with λ=0.7 to avoid retrieving semantically redundant memories.

## NetworkX handles knowledge graphs under 100K nodes efficiently

For file-based knowledge graphs, **NetworkX** provides adequate performance at your scale with ~500MB-1GB RAM for 100K nodes. Use JSON node-link format for Memgraph compatibility:

```json
{
  "nodes": [
    {"id": "user_1", "labels": ["Person"], "properties": {"name": "Alice", "age": 30}}
  ],
  "edges": [
    {"source": "user_1", "target": "project_1", "type": "WORKS_ON", 
     "properties": {"role": "lead", "since": "2024-01"}}
  ]
}
```

**Common Cypher patterns translate directly to NetworkX:**

```python
import networkx as nx

class GraphQueries:
    def __init__(self, G: nx.DiGraph):
        self.G = G
    
    # MATCH (n:Person) RETURN n
    def match_by_label(self, label: str) -> list:
        return [n for n, d in self.G.nodes(data=True) 
                if label in d.get('labels', [])]
    
    # MATCH (a)-[:KNOWS]->(b) RETURN a, b
    def match_pattern(self, edge_type: str) -> list:
        return [(u, v) for u, v, d in self.G.edges(data=True) 
                if d.get('type') == edge_type]
    
    # MATCH path = (a)-[*1..3]->(b) RETURN path
    def variable_path(self, source: str, target: str, max_hops: int = 3):
        return list(nx.all_simple_paths(self.G, source, target, cutoff=max_hops))
```

Store the graph in JSON and build SQLite indexes for fast lookups:

```sql
CREATE TABLE graph_edges (
    source_id TEXT, target_id TEXT, type TEXT, properties TEXT
);
CREATE INDEX idx_edges_source ON graph_edges(source_id);
CREATE INDEX idx_edges_type ON graph_edges(type);
```

## Migration to Qdrant and Memgraph requires schema discipline

The schemas above are designed for clean migration. Key principles to maintain:

- **Embeddings**: Store as float32 BLOBs (Qdrant compatible)
- **Consistent vector dimensions**: 384 dims throughout
- **Unique IDs**: Required for Qdrant point updates
- **JSON metadata**: Maps directly to Qdrant payloads
- **Property graph model**: Labels + properties structure matches Memgraph

**Export for Qdrant:**
```python
def export_for_qdrant(conn):
    rows = conn.execute("""
        SELECT id, embedding, observation, importance, timestamp
        FROM episodic_memory WHERE embedding IS NOT NULL
    """).fetchall()
    
    return [{
        "id": row['id'],
        "vector": np.frombuffer(row['embedding'], dtype=np.float32).tolist(),
        "payload": {"observation": row['observation'], 
                    "importance": row['importance'],
                    "timestamp": row['timestamp']}
    } for row in rows]
```

**Export for Memgraph via GQLAlchemy:**
```python
from gqlalchemy.transformations.translators.nx_translator import NxTranslator

translator = NxTranslator()
for cypher_query in translator.to_cypher_queries(networkx_graph):
    memgraph.execute(cypher_query)
```

## Conclusion

This architecture enables sophisticated AI agent memory on a laptop with **16GB RAM and no GPU for local inference**. The key insights are:

- **sqlite-vec + FTS5** provides capable hybrid vector/text search with 30MB overhead
- **all-MiniLM-L6-v2 ONNX int8** achieves 10-15ms embedding latency with ~100MB RAM
- **FSRS retrievability formulas** determine what to remember vs forget without ML infrastructure
- **Sandwich ordering** of retrieved memories mitigates the lost-in-the-middle phenomenon
- **NetworkX + JSON** handles knowledge graphs under 100K nodes with clean Memgraph migration path

The total memory footprint for 100K records lands around **350-500MB active RAM**—leaving substantial headroom for your voice pipeline (faster-whisper, kokoro-onnx) while maintaining sub-200ms latency for all memory operations.