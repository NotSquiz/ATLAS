# Integrating external knowledge sources for ATLAS via MCP servers

**Building a memory-efficient coding assistant requires strategic API integration with aggressive caching.** For a 16GB laptop with only 4-6GB usable RAM, the optimal architecture combines exa.ai for code search, ref.tools for documentation, and a SQLite cache layer that reduces API calls by 60-80%. This report provides complete integration patterns, cache schemas, and configuration examples for Python, TypeScript, React, and Next.js development.

The key insight: **ref.tools delivers 60-95% fewer tokens than alternatives**, making it ideal for RAM-constrained environments, while exa.ai's `get_code_context_exa` tool provides targeted code examples at $5 per 1,000 queries. Combined with intelligent caching, total monthly costs for individual developers typically stay under $20.

## exa.ai delivers targeted code context at predictable costs

Exa.ai's MCP server provides seven tools, but for coding assistance, only `get_code_context_exa` matters. This tool searches billions of GitHub repos, docs pages, and Stack Overflow posts, returning exactly the few hundred tokens needed rather than entire documents.

**Rate limits and pricing** form the foundation of any caching strategy:

| Search Type | Cost per 1K Requests | Latency | Best Use Case |
|------------|---------------------|---------|---------------|
| Auto/Fast | $5 (1-25 results) | <350ms | Real-time coding queries |
| Neural | $5-25 | Medium | Semantic understanding |
| Deep | $15 | ~3.5s | Complex research questions |
| Content extraction | +$1/1K pages | Low | Full text retrieval |

The rate limit of **5 queries per second** on `/search` means burst protection is essential. For ATLAS, cache common patterns aggressively—documentation queries rarely change and benefit from 24-hour TTLs.

**MCP server configuration for code-only access:**

```json
{
  "mcpServers": {
    "exa": {
      "command": "npx",
      "args": ["-y", "exa-mcp-server", "tools=get_code_context_exa"],
      "env": {
        "EXA_API_KEY": "${EXA_API_KEY}"
      }
    }
  }
}
```

For even lower latency, use the hosted endpoint at `https://mcp.exa.ai/mcp?tools=get_code_context_exa&exaApiKey=YOUR_KEY`, eliminating Node.js process overhead.

**Query patterns optimized for ATLAS's target stack:**

```python
# Python best practices query
PYTHON_QUERY = {
    "query": "Python asyncio SQLite connection pooling patterns",
    "type": "auto",
    "category": "github",
    "numResults": 10,
    "includeDomains": ["github.com", "docs.python.org", "stackoverflow.com"]
}

# TypeScript/React patterns
REACT_QUERY = {
    "query": "React 19 useActionState form validation TypeScript",
    "type": "neural",
    "numResults": 10,
    "includeDomains": ["react.dev", "github.com", "typescript-eslint.io"]
}

# Next.js App Router patterns  
NEXTJS_QUERY = {
    "query": "Next.js 15 Server Actions authentication middleware",
    "type": "neural",
    "numResults": 10,
    "includeDomains": ["nextjs.org", "github.com", "vercel.com"]
}
```

The `includeDomains` parameter is crucial—it improves relevance and reduces wasted results. Always include official documentation domains first.

## ref.tools maximizes token efficiency for documentation

Ref.tools solves the "context rot" problem—AI coding assistants hallucinate when documentation exceeds context windows. Its session-aware search returns the most relevant **~5,000 tokens** per page read, compared to 10,000+ tokens from alternatives like Context7.

**Three core tools** define the ref.tools workflow:

- **`ref_search_documentation`**: Searches 1,000+ documentation sites, GitHub repos, and private sources
- **`ref_read_url`**: Fetches URLs from search results as clean markdown, prioritizing relevant sections
- **`ref_search_web`**: Fallback general web search (can be disabled)

**Pricing is credit-based:** 200 free credits that never expire, then $9/month for 1,000 credits. Most individual developers never exceed the free tier—roughly 20 tool calls per week.

**Configuration with fallback disabled** (recommended for predictable behavior):

```json
{
  "mcpServers": {
    "ref": {
      "type": "http",
      "url": "https://api.ref.tools/mcp?apiKey=${REF_API_KEY}&disable_search_web=true"
    }
  }
}
```

**Query patterns for ATLAS's stack:**

```
# Python documentation
"Python FastAPI dependency injection patterns documentation"
"SQLAlchemy 2.0 async session management best practices"

# TypeScript/React
"TypeScript Zod schema validation custom error messages"
"React Server Components data fetching patterns"

# Next.js
"Next.js 15 App Router middleware authentication JWT"
"Next.js Server Actions form validation with useActionState"

# React Native/Expo
"Expo SDK 52 camera permissions managed workflow"
"React Native navigation TypeScript patterns"
```

**Fallback strategy when ref.tools is unavailable:**

1. **Primary**: ref.tools (best token efficiency)
2. **Secondary**: Context7 at `https://mcp.context7.com/mcp` (free, ~10K tokens per lookup)
3. **Tertiary**: Docfork via `npx docfork` (MIT licensed, 9,000+ libraries)
4. **Emergency**: Direct documentation URLs with exa.ai crawling

## SQLite cache schema optimizes for RAM-constrained environments

The cache layer serves three purposes: reduce API costs, enforce rate limits, and provide offline fallback during outages. For 4-6GB usable RAM, configure SQLite's page cache at **64MB maximum**.

**Initialization pragmas for performance:**

```sql
PRAGMA journal_mode = WAL;       -- Better concurrent read/write
PRAGMA synchronous = NORMAL;      -- Balanced durability/speed
PRAGMA cache_size = -64000;       -- 64MB page cache
PRAGMA temp_store = MEMORY;       -- Memory-backed temp tables
```

**Core cache schema:**

```sql
-- API Response Cache with TTL and access tracking
CREATE TABLE api_cache (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    cache_key TEXT NOT NULL UNIQUE,
    endpoint TEXT NOT NULL,
    request_hash TEXT NOT NULL,
    response_data TEXT NOT NULL,
    status_code INTEGER NOT NULL,
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    expires_at DATETIME NOT NULL,
    last_accessed DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    access_count INTEGER DEFAULT 1,
    etag TEXT,
    source TEXT CHECK(source IN ('exa', 'ref', 'direct'))
);

CREATE INDEX idx_cache_key ON api_cache(cache_key);
CREATE INDEX idx_cache_expires ON api_cache(expires_at);
CREATE INDEX idx_cache_source ON api_cache(source, endpoint);

-- Rate Limiting with sliding window
CREATE TABLE rate_limits (
    identifier TEXT PRIMARY KEY,
    request_count INTEGER NOT NULL DEFAULT 0,
    window_start REAL NOT NULL,
    denied_count INTEGER NOT NULL DEFAULT 0
);

-- Version tracking for documentation freshness
CREATE TABLE doc_versions (
    framework TEXT PRIMARY KEY,
    current_version TEXT NOT NULL,
    last_checked DATETIME NOT NULL,
    changelog_url TEXT,
    last_major_change DATETIME
);

-- Cached style rules per project
CREATE TABLE style_rules (
    project_path TEXT NOT NULL,
    rule_type TEXT NOT NULL,
    rule_data TEXT NOT NULL,
    source TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (project_path, rule_type)
);
```

**TTL recommendations by query type:**

| Query Category | TTL | Rationale |
|---------------|-----|-----------|
| API reference docs | 7-30 days | Stable between releases |
| Code examples | 24 hours | Patterns evolve slowly |
| Best practices | 12 hours | Community consensus shifts |
| Framework versions | 1 hour | Track latest releases |
| News/trending | 15 minutes | Time-sensitive |

**Python cache manager implementation:**

```python
import sqlite3
import hashlib
import json
from datetime import datetime, timedelta
from typing import Optional

class ATLASCache:
    TTL_DOCS = 604800       # 7 days
    TTL_CODE = 86400        # 24 hours
    TTL_PRACTICES = 43200   # 12 hours
    TTL_VERSIONS = 3600     # 1 hour
    
    def __init__(self, db_path: str = "atlas_cache.db"):
        self.db_path = db_path
        self._init_db()
    
    def _generate_key(self, source: str, query: str, params: dict) -> str:
        normalized = json.dumps({"q": query, **params}, sort_keys=True)
        return hashlib.sha256(f"{source}:{normalized}".encode()).hexdigest()[:32]
    
    async def get_or_fetch(
        self, 
        source: str, 
        query: str, 
        params: dict,
        fetch_fn,
        ttl: int = None
    ) -> dict:
        cache_key = self._generate_key(source, query, params)
        
        # Check cache
        cached = self._get_cached(cache_key)
        if cached:
            return {**cached, "_cached": True, "_age": self._get_age(cache_key)}
        
        # Check rate limit before fetch
        if not self._check_rate_limit(source):
            stale = self._get_stale(cache_key)
            if stale:
                return {**stale, "_stale": True}
            raise RateLimitError(f"Rate limit exceeded for {source}")
        
        # Fetch and cache
        result = await fetch_fn(query, params)
        ttl = ttl or self._infer_ttl(query, source)
        self._set_cached(cache_key, source, query, result, ttl)
        return result
    
    def _infer_ttl(self, query: str, source: str) -> int:
        """Infer appropriate TTL based on query content."""
        query_lower = query.lower()
        if any(kw in query_lower for kw in ["version", "latest", "release"]):
            return self.TTL_VERSIONS
        if any(kw in query_lower for kw in ["docs", "reference", "api"]):
            return self.TTL_DOCS
        if any(kw in query_lower for kw in ["best practice", "pattern", "convention"]):
            return self.TTL_PRACTICES
        return self.TTL_CODE
```

## Cost and latency analysis guides caching decisions

**Exa.ai cost model:**

For a typical coding assistant making 50 queries/day:
- Search queries: 50 × 30 days = 1,500/month → **$7.50**
- Content extraction (10% of queries): 150 → **$0.15**
- **Monthly total: ~$8 without caching**

With **60% cache hit rate** (achievable for documentation queries):
- Actual API calls: 600/month → **$3.20/month**

**Ref.tools cost model:**

Free tier (200 credits) covers ~200 tool invocations. At 20 calls/week, free tier lasts **2.5 months**. Basic plan ($9/month for 1,000 credits) provides ample headroom.

**Latency comparison:**

| Operation | Cold (no cache) | Warm (cached) |
|-----------|-----------------|---------------|
| Exa auto search | 200-400ms | <5ms |
| Exa deep search | 3-4s | <5ms |
| Ref.tools search | 300-600ms | <5ms |
| Ref.tools read | 500-1000ms | <5ms |
| SQLite lookup | N/A | <1ms |

**When to query vs. use cached:**

- **Always query fresh**: Version checks, security advisories, trending patterns
- **Prefer cache**: API signatures, stable documentation, code examples
- **Hybrid**: Best practices (check age, refresh if >12 hours)

## Version-aware recommendations prevent outdated advice

ATLAS must detect when its training data or cached content is stale. **Key version differences** for the target stack:

**Python 3.12 vs 3.9:**
- PEP 695 type parameter syntax (`def func[T](x: T) -> T`)
- Removed `distutils` (use setuptools)
- More flexible f-strings (PEP 701)
- Removed `asynchat`, `asyncore`, `imp` modules

**React 18 vs 19:**
- React 19: `use()` API, `useActionState`, `useFormStatus`, `useOptimistic`
- React 19: Document metadata support, refs as props
- React 18: Concurrent features, `startTransition`, Suspense for SSR

**Next.js 14 vs 15:**
- v15: Caching defaults to **uncached** (breaking change)
- v15: Async request APIs, React Compiler experimental
- v14: Stable Server Actions, Partial Prerendering preview

**GitHub release feeds for monitoring:**

```
React:      https://github.com/facebook/react/releases.atom
Next.js:    https://github.com/vercel/next.js/releases.atom
TypeScript: https://github.com/microsoft/TypeScript/releases.atom
Expo:       https://github.com/expo/expo/releases.atom
Python:     https://github.com/python/cpython/releases.atom
Ruff:       https://github.com/astral-sh/ruff/releases.atom
```

**Version tracking table update strategy:**

```python
async def check_framework_versions():
    frameworks = {
        "react": "https://api.github.com/repos/facebook/react/releases/latest",
        "next": "https://api.github.com/repos/vercel/next.js/releases/latest",
        "typescript": "https://api.github.com/repos/microsoft/TypeScript/releases/latest",
        "expo": "https://api.github.com/repos/expo/expo/releases/latest"
    }
    
    for name, url in frameworks.items():
        release = await fetch_json(url)
        version = release["tag_name"].lstrip("v")
        
        # Update tracking table
        db.execute("""
            INSERT OR REPLACE INTO doc_versions 
            (framework, current_version, last_checked, changelog_url)
            VALUES (?, ?, datetime('now'), ?)
        """, (name, version, release["html_url"]))
        
        # Invalidate stale cache entries
        db.execute("""
            UPDATE api_cache SET expires_at = datetime('now')
            WHERE source IN ('exa', 'ref') 
            AND response_data LIKE ?
        """, (f'%{name}%',))
```

## MCP server wrapper pattern with circuit breaker protection

A resilient MCP wrapper needs three layers: **rate limiting**, **caching**, and **circuit breaking**. The circuit breaker prevents cascading failures when external APIs are down.

**Complete Python MCP server implementation:**

```python
from mcp.server import Server
from mcp.server.stdio import StdioServerTransport
import asyncio

server = Server("atlas-knowledge-server", "1.0.0")
cache = ATLASCache("./atlas_cache.db")
exa_breaker = CircuitBreaker(failure_threshold=5, recovery_timeout=60)
ref_breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=30)

@server.list_tools()
async def list_tools():
    return [
        {
            "name": "search_code_patterns",
            "description": "Search for code patterns, best practices, and documentation",
            "inputSchema": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "languages": {
                        "type": "array", 
                        "items": {"type": "string"},
                        "default": ["python", "typescript"]
                    },
                    "source": {
                        "type": "string",
                        "enum": ["auto", "exa", "ref"],
                        "default": "auto"
                    }
                },
                "required": ["query"]
            }
        },
        {
            "name": "get_framework_docs",
            "description": "Retrieve documentation for Python, React, Next.js, or Expo",
            "inputSchema": {
                "type": "object", 
                "properties": {
                    "framework": {
                        "type": "string",
                        "enum": ["python", "react", "nextjs", "expo", "typescript"]
                    },
                    "topic": {"type": "string"},
                    "version": {"type": "string", "default": "latest"}
                },
                "required": ["framework", "topic"]
            }
        }
    ]

@server.call_tool()
async def call_tool(name: str, arguments: dict):
    if name == "search_code_patterns":
        return await search_with_fallback(
            arguments["query"],
            arguments.get("languages", ["python", "typescript"]),
            arguments.get("source", "auto")
        )
    elif name == "get_framework_docs":
        return await get_docs_cached(
            arguments["framework"],
            arguments["topic"],
            arguments.get("version", "latest")
        )

async def search_with_fallback(query: str, languages: list, source: str):
    """Search with automatic source selection and fallback."""
    
    # Source selection logic
    if source == "auto":
        # Use ref.tools for documentation, exa for code examples
        if any(kw in query.lower() for kw in ["docs", "api", "reference"]):
            source = "ref"
        else:
            source = "exa"
    
    try:
        if source == "ref":
            result = await ref_breaker.execute(
                lambda: cache.get_or_fetch("ref", query, {}, ref_search)
            )
        else:
            domains = get_domains_for_languages(languages)
            result = await exa_breaker.execute(
                lambda: cache.get_or_fetch("exa", query, {"domains": domains}, exa_search)
            )
        return {"content": [{"type": "text", "text": json.dumps(result, indent=2)}]}
        
    except CircuitBreakerOpen:
        # Try alternate source
        alt_source = "exa" if source == "ref" else "ref"
        try:
            result = await cache.get_or_fetch(alt_source, query, {}, 
                ref_search if alt_source == "ref" else exa_search)
            return {"content": [{"type": "text", "text": json.dumps(result, indent=2)}]}
        except Exception:
            # Return stale cache as last resort
            stale = cache._get_stale(cache._generate_key(source, query, {}))
            if stale:
                return {"content": [{"type": "text", "text": json.dumps({**stale, "_stale": True})}]}
            return {"content": [{"type": "text", "text": "Service temporarily unavailable"}], "isError": True}

def get_domains_for_languages(languages: list) -> list:
    """Map languages to authoritative domains."""
    domain_map = {
        "python": ["docs.python.org", "github.com", "stackoverflow.com"],
        "typescript": ["typescriptlang.org", "github.com", "typescript-eslint.io"],
        "react": ["react.dev", "github.com", "reactnative.dev"],
        "nextjs": ["nextjs.org", "vercel.com", "github.com"],
        "expo": ["docs.expo.dev", "github.com", "reactnative.dev"]
    }
    domains = set()
    for lang in languages:
        domains.update(domain_map.get(lang, []))
    return list(domains)
```

## Code convention enforcement through external style guides

ATLAS can extract and enforce style conventions from authoritative sources. The key tools per language:

**Python:** Ruff (30x faster than Black, 99.9% compatible)
```toml
# pyproject.toml
[tool.ruff]
line-length = 88
target-version = "py312"

[tool.ruff.lint]
select = ["E4", "E7", "E9", "F", "I", "UP", "B", "SIM"]

[tool.ruff.format]
quote-style = "double"
```

**TypeScript:** ESLint + typescript-eslint + Prettier
```javascript
// eslint.config.mjs
import tseslint from '@typescript-eslint/eslint-plugin';
import eslintConfigPrettier from 'eslint-config-prettier';

export default [
  ...tseslint.configs.strictTypeChecked,
  ...tseslint.configs.stylisticTypeChecked,
  eslintConfigPrettier,
];
```

**When to suggest vs. auto-apply:**

- **Auto-apply**: Formatting (whitespace, quotes, imports), clear style violations
- **Suggest**: Architectural patterns, naming conventions, type annotations
- **Never auto-apply**: Security-related changes, breaking API modifications

**Style rule caching pattern:**

```python
async def get_project_conventions(project_path: str) -> dict:
    """Extract conventions from config files or fetch defaults."""
    
    # Check for existing config
    config_files = ["pyproject.toml", ".eslintrc.js", "tsconfig.json"]
    local_rules = {}
    
    for config in config_files:
        path = f"{project_path}/{config}"
        if os.path.exists(path):
            local_rules[config] = parse_config(path)
    
    # Cache in SQLite for fast lookup
    db.execute("""
        INSERT OR REPLACE INTO style_rules (project_path, rule_type, rule_data, source)
        VALUES (?, 'local', ?, 'config_files')
    """, (project_path, json.dumps(local_rules)))
    
    # Fetch language-specific defaults if missing
    for lang in detect_languages(project_path):
        if lang not in local_rules:
            defaults = await cache.get_or_fetch(
                "ref", 
                f"{lang} official style guide conventions",
                {},
                ref_search,
                ttl=604800  # 7 days
            )
            local_rules[lang] = defaults
    
    return local_rules
```

## Conclusion

Building ATLAS's knowledge integration layer requires balancing **token efficiency** (ref.tools), **search quality** (exa.ai), and **memory constraints** (SQLite with 64MB page cache). The critical architectural decisions:

1. **Use ref.tools as primary** for documentation—60-95% fewer tokens than alternatives
2. **Reserve exa.ai for code search**—its `get_code_context_exa` returns precisely what's needed
3. **Cache aggressively** with TTLs based on content volatility (7 days for docs, 12 hours for practices)
4. **Implement circuit breakers** to prevent cascading failures when APIs are unavailable
5. **Track framework versions** via GitHub release feeds to invalidate stale cache

The total monthly cost for individual use stays under **$15** with proper caching—$8 for exa.ai, free tier for ref.tools. The SQLite cache provides sub-millisecond lookups while staying within the 4-6GB RAM budget, and the circuit breaker pattern ensures ATLAS degrades gracefully during API outages.