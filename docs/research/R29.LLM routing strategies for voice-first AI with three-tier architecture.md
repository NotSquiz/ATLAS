# LLM routing strategies for voice-first AI with three-tier architecture

A hybrid classification approach combining **rule-based filtering, local embeddings, and cascade verification** achieves optimal cost-quality-latency balance for the ATLAS system. Research shows this architecture can reduce API costs by **70-85%** while maintaining **95%+ quality** on complex queries. With the $5-10/month budget, ATLAS can comfortably handle **50-100 queries/day** using Haiku 4.5 for the middle tier, reserving local Qwen2.5-3B for simple commands (**40-50%** of queries) and Claude Agent SDK for complex multi-tool tasks (**10-15%**).

---

## Classification approaches ranked by effectiveness

The 2024-2025 research landscape offers five distinct classification strategies, each with specific accuracy and latency tradeoffs for voice-first systems requiring **<3s end-to-end latency**.

**Rule-based heuristics** provide the fastest option at **<1ms latency** with **60-75% accuracy**. Pattern matching on question words, query length, and keyword lists handles obvious cases effectively. Queries under 5 tokens route to local; keywords like "analyze," "plan," or "research" escalate to the Agent tier. The primary failure modes involve ambiguous queries ("apple" = fruit vs company) and context-dependent requests. This approach works as a first-pass filter but lacks nuance for edge cases.

**Embedding-based classification** using all-MiniLM-L6-v2 (22M parameters, 384-dimensional embeddings) adds **20-30ms latency** while improving accuracy to **70-80%**. The model requires only ~90MB memory and achieves 14.7ms per 1K tokens—well within the latency budget. Training requires **100-500 labeled examples per tier** for robust clustering; K-means with 3 centroids maps directly to the three-tier architecture. Cosine similarity to centroids at inference time provides confidence scores for escalation decisions.

**Local LLM classification** with Qwen2.5-3B achieves **75-85% accuracy** in **100-200ms** when quantized to INT4. Recent research on layer-enhanced classification (arXiv 2412.13435) demonstrates that small LLM hidden states combined with logistic regression surpass GPT-4o on classification tasks. A few-shot prompt template with ~80-100 tokens achieves this accuracy:

```python
classify_prompt = """Classify query complexity: simple, moderate, or complex.
Examples:
"What time is it?" → simple
"How do I reset my password?" → moderate  
"Help me plan a marketing strategy for Q2" → complex
Query: "{user_query}"
Classification:"""
```

**Cascade/speculative approaches** from FrugalGPT (Chen et al., 2023) achieve **up to 98% cost reduction** while matching GPT-4 performance. The core pattern: query cheapest model first, use a DistilBERT-based scorer to evaluate response quality, escalate if score < threshold. The challenge lies in confidence calibration—small LLMs are "frustratingly bad at knowing if their own answers are wrong." Solutions include token-level logits (quantile rather than mean), semantic similarity via NLI models, and hidden state classifiers trained on intermediate layer activations.

**Haiku micro-classifier** costs approximately **$0.0001 per classification** (~100 tokens input, ~10 tokens output) versus **$0.0017** for a full response. The break-even point occurs when classification routes **>6%** of queries to cheaper tiers—easily achievable given **40-50%** of queries are simple commands.

| Approach | Accuracy | Latency | Cost/Query | Best Use Case |
|----------|----------|---------|------------|---------------|
| Rule-based | 60-75% | <1ms | $0 | First-pass filter |
| MiniLM embedding | 70-80% | 20-30ms | ~$0.00001 | Balanced default |
| Qwen2.5-3B classifier | 75-85% | 100-200ms | ~$0.0001 | When accuracy critical |
| Haiku micro-classifier | 85-90% | ~300ms | ~$0.0001 | Edge cases only |
| Full cascade | 80-90% | Variable | Variable | Maximum cost savings |

---

## Query taxonomy maps directly to routing tiers

Analysis of Amazon MASSIVE (1M+ utterances, 60 intents), SNIPS (14,484 utterances, 7 intents), and voice assistant usage studies reveals consistent distribution patterns that inform tier assignment.

**Tier 1 (Local Qwen2.5-3B)** handles **40-50%** of all queries—latency-critical commands requiring **<200ms TTFT**:

- Timer/alarm commands: "Set timer for 5 minutes" (slot extraction: duration)
- Media playback control: "Pause," "Skip," "Volume up"
- Smart home control: "Turn on lights," "Set thermostat to 72"
- Basic math: "What's 15 times 12?"
- System commands: "Stop," "Cancel," "Go back"
- Quick confirmations: "Yes," "No," "Repeat"

**Tier 2 (Haiku 4.5)** covers **35-40%** of queries—moderate complexity with **<500ms TTFT** acceptable:

- Weather queries: "What's the weather in Seattle tomorrow?"
- Quick knowledge lookup: "Who won the World Series?"
- Simple reminders: "Remind me to call John at 3pm"
- Contact-based commands: "Call Mom," "Text Sarah"
- Unit conversions, translations, basic explanations

**Tier 3 (Claude Agent SDK)** handles **10-15%** of queries—quality-critical tasks where **<6s** is acceptable:

- Complex planning: "Plan a trip to Paris for next month"
- Safety-critical queries: Medical, legal, financial advice
- Multi-tool tasks: Web search + API calls combined
- Research/analysis: "Compare iPhone vs Android pros and cons"
- Creative tasks requiring context

**Edge cases requiring special handling**: Ambiguous pronouns ("Play it again"), compound queries ("Set a timer and play music"), follow-up questions ("What about tomorrow?"), and emotional queries ("I'm feeling sad") should default to Tier 2 or Tier 3 for safety.

---

## Cost projections fit comfortably within budget

**Claude Haiku 4.5 pricing** (verified January 2026): **$1/MTok input, $5/MTok output**. Prompt caching achieves **90% savings** on repeated content at $0.10/MTok for cache hits. Batch processing provides **50% discount** for non-urgent queries.

Average cost per query type:
- Simple (100 input + 75 output tokens): **$0.000475**
- Medium (300 + 300 tokens): **$0.0018**
- Complex (500 + 600 tokens): **$0.0035**

**Query capacity at budget levels:**

| Usage Level | Queries/Day | Monthly Count | Cost Without Local | Cost With 40% Local |
|-------------|-------------|---------------|-------------------|---------------------|
| Light | 20 | 600 | $0.90/mo | $0.54/mo |
| Medium | 50 | 1,500 | $2.70/mo | $1.62/mo |
| Heavy | 100 | 3,000 | $5.40/mo | $3.24/mo |
| Intensive | 150 | 4,500 | $8.10/mo | $4.86/mo |

**Optimized heavy usage (100 queries/day):**
- 40 local (simple): $0
- 30 Haiku cached: $0.90/month
- 25 Haiku standard: $1.50/month  
- 5 Agent SDK (free via Max Plan): $0

**Total: ~$2.40/month** for 100 queries/day with hybrid routing—well within $5-10 budget.

**Budget allocation strategy**: Rolling weekly average outperforms fixed daily limits. Allocate 60% to base operations, 20% flex reserve for complex queries, 20% emergency reserve. Implement graceful degradation: at **75%** budget utilization, prefer local for simple queries; at **90%**, API only for critical queries; at **100%**, local-only mode until reset.

---

## Adaptive routing through Thompson Sampling

**Thompson Sampling** emerges as the optimal starting algorithm for three reasons: strong empirical performance across LLM routing benchmarks, natural exploration-exploitation balance without tuning ε, and O(d√T) regret bounds ensuring convergence.

The PILOT approach (Panda et al., 2025) achieves **93% of GPT-4 performance at 25% cost** on RouterBench by constructing a shared embedding space for queries and LLMs, initializing from human preference data. It adapts to distribution shifts within **50 requests**.

**Feedback signals** for routing quality split into explicit (thumbs up/down, 1-3% response rate) and implicit (more valuable):
- Rephrasing: User rewords request → negative signal
- Follow-up questions: Incomplete response detected
- Re-asking same question: Strong negative signal
- Task completion: Whether user achieved goal
- Session abandonment: Quick exit indicates failure

**Minimum data requirements:**
- Pre-launch: Use offline-trained router (RouteLLM weights)
- Initial burn-in: 50-100 queries with high exploration (ε=0.2)
- Begin adaptation: 100-500 queries for parameter updates
- Per-category thresholds: 200+ examples per category

**Safeguards against bad adaptations:**
1. Minimum exploration floor: Never drop below ε=0.02
2. Model diversity constraint: Each tier receives minimum 5-10% traffic
3. Consecutive failure circuit breaker: Disable tier after 5 failures
4. Threshold drift limits: Cap changes at ±0.05 per update
5. Replay buffer: Mix 20-30% historical data with recent updates

---

## Implementation architecture for ATLAS

The recommended architecture uses a **three-layer classification pipeline** within the **<50ms routing budget**:

```python
from dataclasses import dataclass
from typing import Optional
import numpy as np

@dataclass
class RoutingDecision:
    tier: str  # 'local', 'haiku', 'agent'
    confidence: float
    category: str
    bypass_reason: Optional[str] = None

class ATLASRouter:
    def __init__(self, embedding_model, cluster_centroids: dict, thresholds: dict):
        self.embedder = embedding_model  # MiniLM-L6-v2
        self.centroids = cluster_centroids
        self.thresholds = thresholds
        
        # Rule-based patterns
        self.local_patterns = ['timer', 'alarm', 'weather', 'what time', 
                               'calculate', 'convert', 'stop', 'pause', 'play']
        self.agent_patterns = ['analyze', 'research', 'plan', 'compare', 
                               'help me with', 'create a', 'design']
        self.safety_patterns = ['medical', 'legal', 'emergency', 'health', 
                                'symptoms', 'medication']
    
    def route(self, query: str) -> RoutingDecision:
        query_lower = query.lower()
        token_count = len(query.split())
        
        # Layer 1: Rule-based filter (~1ms)
        # Safety-critical always goes to agent
        if any(p in query_lower for p in self.safety_patterns):
            return RoutingDecision('agent', 1.0, 'safety', 'safety_keyword')
        
        # Simple patterns to local
        if any(p in query_lower for p in self.local_patterns) and token_count < 8:
            return RoutingDecision('local', 0.9, 'simple_command', 'pattern_match')
        
        # Complex patterns to agent
        if any(p in query_lower for p in self.agent_patterns) or token_count > 25:
            return RoutingDecision('agent', 0.85, 'complex', 'pattern_match')
        
        # Layer 2: Embedding classification (~25ms)
        embedding = self.embedder.encode(query)
        similarities = {
            tier: np.dot(embedding, centroid) / (
                np.linalg.norm(embedding) * np.linalg.norm(centroid)
            ) for tier, centroid in self.centroids.items()
        }
        
        best_tier = max(similarities, key=similarities.get)
        confidence = similarities[best_tier]
        
        # High confidence → route directly
        if confidence > self.thresholds['high_confidence']:
            return RoutingDecision(best_tier, confidence, 'embedding_match')
        
        # Low confidence → default to haiku (safe middle ground)
        return RoutingDecision('haiku', confidence, 'uncertain')

# Configuration
ROUTING_CONFIG = {
    'thresholds': {
        'high_confidence': 0.85,
        'escalation_confidence': 0.7,
        'local_complexity_max': 0.4,
        'agent_complexity_min': 0.75,
    },
    'budget': {
        'daily_usd': 0.25,  # $7.50/month ÷ 30
        'alert_50_pct': 0.125,
        'alert_75_pct': 0.1875,
        'alert_90_pct': 0.225,
    },
    'latency_targets_ms': {
        'local': 200,
        'haiku': 500,
        'agent': 6000,
    }
}
```

**Cost tracking schema** for SQLite:

```sql
CREATE TABLE llm_usage (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    query_hash TEXT NOT NULL,
    tier TEXT NOT NULL,
    model TEXT NOT NULL,
    input_tokens INTEGER NOT NULL,
    output_tokens INTEGER NOT NULL,
    cost_usd REAL NOT NULL,
    latency_ms INTEGER NOT NULL,
    routing_confidence REAL,
    category TEXT,
    quality_score REAL,
    escalated BOOLEAN DEFAULT FALSE,
    escalation_reason TEXT
);

CREATE INDEX idx_timestamp ON llm_usage(timestamp);
CREATE INDEX idx_tier ON llm_usage(tier);

CREATE VIEW daily_summary AS
SELECT 
    DATE(timestamp) as date,
    tier,
    COUNT(*) as requests,
    SUM(cost_usd) as daily_cost,
    AVG(latency_ms) as avg_latency,
    SUM(CASE WHEN escalated THEN 1 ELSE 0 END) as escalations
FROM llm_usage
GROUP BY DATE(timestamp), tier;
```

---

## Failure handling ensures system resilience

**Circuit breaker pattern** prevents cascade failures when APIs become unavailable:

```python
from tenacity import retry, stop_after_attempt, wait_random_exponential
import pybreaker

# Per-tier circuit breakers
haiku_breaker = pybreaker.CircuitBreaker(
    fail_max=5,
    reset_timeout=60,
    exclude=[ValueError, AuthenticationError]
)

@haiku_breaker
@retry(
    stop=stop_after_attempt(3),
    wait=wait_random_exponential(multiplier=1, max=30),
    retry=retry_if_exception_type((RateLimitError, TimeoutError))
)
async def call_haiku(query: str, timeout: float = 5.0):
    async with asyncio.timeout(timeout):
        return await anthropic_client.messages.create(
            model="claude-3-5-haiku-20241022",
            max_tokens=500,
            messages=[{"role": "user", "content": query}]
        )
```

**Graceful degradation tiers:**

| Scenario | Haiku Available | Agent Available | Action |
|----------|-----------------|-----------------|--------|
| Normal | ✓ | ✓ | Route per classification |
| Haiku down | ✗ | ✓ | Local for simple, Agent for complex |
| Agent down | ✓ | ✗ | Local for simple, Haiku for all else |
| All APIs down | ✗ | ✗ | Local-only mode with disclaimer |
| Budget exhausted | — | — | Local-only until reset |

**429 rate limit handling**: Parse `Retry-After` header, implement exponential backoff with jitter (base 1s, max 60s), track rate limit events for budget pacing adjustment.

---

## Testing strategy validates routing accuracy

**Benchmark dataset construction**: Collect 500-1000 queries across categories, labeled by actual tier performance (silver labels from A/B testing). Use MASSIVE and SNIPS for intent classification validation, custom dataset for routing accuracy.

**Unit testing with mocks:**

```python
from litellm import completion
import pytest

def test_routing_simple_query():
    router = ATLASRouter(...)
    decision = router.route("Set a timer for 5 minutes")
    assert decision.tier == 'local'
    assert decision.confidence > 0.8

def test_haiku_mock_response():
    response = completion(
        model="claude-3-5-haiku-20241022",
        messages=[{"role": "user", "content": "Test"}],
        mock_response="Mocked response for testing"
    )
    assert "Mocked" in response.choices[0].message.content

@pytest.mark.vcr()  # Records/replays API cassettes
def test_full_routing_integration():
    result = atlas.process_query("What's the weather in Seattle?")
    assert result.tier_used in ['local', 'haiku']
```

**A/B testing framework**: Route 10% of traffic randomly to validate router accuracy. Track quality metrics (task completion, follow-ups, user corrections) by routing decision. Compare actual performance against router predictions.

**Evaluation metrics:**
- **APGR (Average Performance Gap Recovered)**: Measures cost-performance balance
- **Routing accuracy**: Percentage correctly routed (validated by tier performance)
- **Escalation rate**: Percentage escalated from initial tier
- **Cost per successful query**: Total cost ÷ successful completions

---

## Recommended implementation roadmap

**Phase 1 (Week 1-2): Foundation**
- Implement rule-based router with keyword patterns
- Set up SQLite cost tracking
- Configure Ollama with Qwen2.5-3B (INT4 quantized)
- Establish Haiku API integration with retry logic

**Phase 2 (Week 3-4): Classification**
- Train MiniLM-L6-v2 embeddings on 200+ labeled queries
- Compute cluster centroids for three tiers
- Implement confidence-based routing
- Add circuit breakers for API failures

**Phase 3 (Week 5-6): Optimization**
- Enable prompt caching for system prompts (90% savings)
- Implement Thompson Sampling for adaptive routing
- Build feedback collection for implicit signals
- Set up A/B testing infrastructure

**Phase 4 (Ongoing): Refinement**
- Analyze routing accuracy weekly
- Adjust thresholds based on feedback
- Expand labeled dataset from production traffic
- Tune per-category thresholds as data accumulates

The three-tier ATLAS architecture with hybrid classification achieves the target **<3s end-to-end latency** for voice queries while staying well within the **$5-10/month budget**. Rule-based filtering handles 40-50% of queries at near-zero latency, embedding classification routes 35-40% to Haiku with high confidence, and only 10-15% require the full Agent SDK. Thompson Sampling enables continuous improvement as usage patterns emerge, while circuit breakers and graceful degradation ensure reliable operation even when APIs fail.