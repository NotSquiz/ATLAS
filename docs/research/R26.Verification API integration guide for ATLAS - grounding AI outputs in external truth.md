# Verification API integration guide for ATLAS: grounding AI outputs in external truth

Building an autonomous AI life assistant that makes health, fitness, and nutrition recommendations demands rigorous verification against authoritative external sources. This guide provides a complete integration strategy across four domains—health/supplements, exercise/workouts, nutrition/recipes, and content creation—optimized for a **$5/month verification budget** alongside Exa.ai, with **<500ms latency** requirements and **offline fallback** for safety-critical data.

The key finding: a robust verification layer is achievable within budget by combining **free government APIs** (USDA, OpenFDA, RxNorm) with **open-source databases** (Wger, Free Exercise DB, Open Food Facts) and strategic local caching. Only supplement safety and web plagiarism checking warrant paid API allocation.

---

## Health and supplement verification demands careful API selection

Drug-supplement interaction checking presents the highest-stakes verification challenge. Three free APIs form the foundation, with one paid option worth the budget allocation.

### Supp.AI delivers comprehensive supplement-drug interactions at zero cost

The Allen Institute's Supp.AI provides **2,044 supplements × 2,866 drugs × 59,096 interactions** extracted from 22 million scientific articles—the most comprehensive free supplement interaction database available.

**API integration:**
```python
import requests
from typing import Optional, Dict, List

class SuppAI:
    BASE_URL = "https://supp.ai/api"
    
    def search_agent(self, query: str, page: int = 1) -> Dict:
        """Search supplements or drugs by name"""
        resp = requests.get(
            f"{self.BASE_URL}/agent/search",
            params={"q": query, "p": page},
            timeout=2.0
        )
        return resp.json()
    
    def get_interactions(self, cui: str) -> List[Dict]:
        """Get all interactions for a supplement/drug by UMLS CUI"""
        resp = requests.get(
            f"{self.BASE_URL}/agent/{cui}/interactions",
            timeout=2.0
        )
        return resp.json()
    
    def get_evidence(self, interaction_id: str) -> Dict:
        """Get evidence sentences for specific interaction"""
        resp = requests.get(
            f"{self.BASE_URL}/interaction/{interaction_id}",
            timeout=2.0
        )
        return resp.json()
```

**Key specifications:**
- **Cost**: Free, no API key required
- **Rate limits**: Not documented (recommend <60 req/min)
- **Latency**: ~200-400ms typical
- **Format**: JSON with UMLS CUIs, evidence sentences, paper metadata
- **Limitation**: Literature-derived only; may miss FDA-approved interactions

### OpenFDA provides authoritative pharmaceutical data

For FDA-approved drug information and adverse event data, OpenFDA offers **120,000 requests/day free** with an API key.

```python
class OpenFDA:
    BASE_URL = "https://api.fda.gov"
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key  # Free at open.fda.gov
    
    def search_drug_labels(self, drug_name: str, limit: int = 5) -> Dict:
        """Search drug labels for interaction sections"""
        params = {
            "search": f'openfda.brand_name:"{drug_name}"',
            "limit": limit
        }
        if self.api_key:
            params["api_key"] = self.api_key
        
        resp = requests.get(
            f"{self.BASE_URL}/drug/label.json",
            params=params,
            timeout=2.0
        )
        return resp.json()
    
    def get_adverse_events(self, drug_name: str, limit: int = 10) -> Dict:
        """Query FAERS adverse event reports"""
        params = {
            "search": f'patient.drug.medicinalproduct:"{drug_name}"',
            "limit": limit
        }
        resp = requests.get(
            f"{self.BASE_URL}/drug/event.json",
            params=params,
            timeout=2.0
        )
        return resp.json()
```

**Specifications:**
- **Cost**: Free (API key at open.fda.gov)
- **Rate limits**: 240/min, 120,000/day with key
- **Coverage**: ~140,000 drug labels, FAERS adverse events
- **Limitation**: No dedicated interaction endpoint; requires label parsing

### RxNorm handles drug identification and normalization

RxNorm from NLM provides the canonical drug identification layer—essential for normalizing user input before querying interaction databases.

```python
class RxNorm:
    BASE_URL = "https://rxnav.nlm.nih.gov/REST"
    
    def get_rxcui(self, drug_name: str) -> Optional[str]:
        """Get RxCUI for drug name (canonical identifier)"""
        resp = requests.get(
            f"{self.BASE_URL}/rxcui.json",
            params={"name": drug_name},
            timeout=1.0
        )
        data = resp.json()
        ids = data.get("idGroup", {}).get("rxnormId", [])
        return ids[0] if ids else None
    
    def get_drug_info(self, rxcui: str) -> Dict:
        """Get full drug properties"""
        resp = requests.get(
            f"{self.BASE_URL}/rxcui/{rxcui}/properties.json",
            timeout=1.0
        )
        return resp.json()
```

**Important note**: The RxNorm Drug-Drug Interaction API was **discontinued January 2, 2024**. Use Supp.AI or DrugBank for interaction checking.

### Budget allocation: Natural Medicines Database for critical safety

For the **$5/month verification budget**, the highest-value paid option is **NatMed Pro** (Natural Medicines Database) from TRC Healthcare—the gold standard for evidence-based supplement safety ratings.

- **Pricing**: $182/year (~$15/month) for full access; no API under $182/year
- **Alternative**: Allocate $5/month toward Copyscape for plagiarism ($5 = ~166 checks)
- **Recommendation**: Use free Supp.AI for interactions; reserve paid budget for content verification where free alternatives are weaker

---

## Exercise verification requires building a local rule engine

No single API provides comprehensive exercise contraindication data. The optimal approach combines **open-source exercise databases** with a **custom safety rules engine**.

### Free Exercise DB provides the foundation

The **Free Exercise DB** (public domain, 800+ exercises) offers the best free structured exercise data for building a local verification layer.

```python
import json
from pathlib import Path
from typing import List, Dict, Optional

class ExerciseDB:
    def __init__(self, db_path: str = "exercises.json"):
        # Download from: https://raw.githubusercontent.com/yuhonas/free-exercise-db/main/dist/exercises.json
        with open(db_path) as f:
            self.exercises = {ex["id"]: ex for ex in json.load(f)}
    
    def get_exercise(self, exercise_id: str) -> Optional[Dict]:
        return self.exercises.get(exercise_id)
    
    def find_by_muscle(self, muscle: str) -> List[Dict]:
        return [
            ex for ex in self.exercises.values()
            if muscle.lower() in [m.lower() for m in ex.get("primaryMuscles", [])]
        ]
    
    def find_by_equipment(self, equipment: str) -> List[Dict]:
        return [
            ex for ex in self.exercises.values()
            if ex.get("equipment", "").lower() == equipment.lower()
        ]
```

**Data schema per exercise:**
```json
{
  "id": "Barbell_Bench_Press",
  "name": "Barbell Bench Press",
  "force": "push",
  "level": "intermediate",
  "mechanic": "compound",
  "equipment": "barbell",
  "primaryMuscles": ["chest"],
  "secondaryMuscles": ["shoulders", "triceps"],
  "instructions": ["...", "..."],
  "category": "strength"
}
```

### Wger provides self-hostable workout management

For a more complete solution with workout tracking, **Wger Workout Manager** offers an AGPL-licensed API you can self-host.

- **API docs**: https://wger.readthedocs.io/
- **Public API**: https://wger.de/api/v2/
- **Self-hosting**: Docker deployment available
- **Cost**: Free

### Build custom safety rules from authoritative guidelines

The safety rules engine must encode medical guidelines manually extracted from ACSM and ACOG sources:

```python
from dataclasses import dataclass
from enum import Enum
from typing import Set, Optional

class ContraindicationLevel(Enum):
    ABSOLUTE = "absolute"  # Exercise prohibited
    RELATIVE = "relative"  # Modifications required
    CAUTION = "caution"    # Extra monitoring needed

@dataclass
class SafetyRule:
    condition: str
    contraindicated_exercises: Set[str]
    level: ContraindicationLevel
    guidance: str
    source: str

class ExerciseSafetyEngine:
    def __init__(self):
        self.rules = self._load_rules()
        self.recovery_hours = {
            "legs": (48, 72), "back": (48, 72), "chest": (48, 72),
            "shoulders": (48, 72), "arms": (24, 48), "core": (24, 48)
        }
    
    def _load_rules(self) -> List[SafetyRule]:
        """Load contraindication rules from ACSM/ACOG guidelines"""
        return [
            SafetyRule(
                condition="pregnancy",
                contraindicated_exercises={"contact_sports", "hot_yoga", "scuba", "high_altitude"},
                level=ContraindicationLevel.ABSOLUTE,
                guidance="Avoid contact sports, activities with fall risk, hot environments",
                source="ACOG Committee Opinion 804"
            ),
            SafetyRule(
                condition="uncontrolled_hypertension",
                contraindicated_exercises={"heavy_resistance", "isometric_holds", "valsalva"},
                level=ContraindicationLevel.ABSOLUTE,
                guidance="Contraindicated if BP >180/105 mmHg",
                source="ACSM Guidelines 12th Ed"
            ),
            SafetyRule(
                condition="cardiac_arrhythmia",
                contraindicated_exercises={"hiit", "max_effort"},
                level=ContraindicationLevel.RELATIVE,
                guidance="Avoid high-intensity intervals; monitor heart rate",
                source="AHA Scientific Statement"
            ),
        ]
    
    def check_progression(self, current_weight: float, proposed_weight: float) -> Dict:
        """Enforce 10% weekly progression limit"""
        max_allowed = current_weight * 1.10
        is_safe = proposed_weight <= max_allowed
        return {
            "safe": is_safe,
            "max_recommended": max_allowed,
            "proposed": proposed_weight,
            "rule": "NASM 10% weekly progression limit"
        }
    
    def check_recovery(self, muscle: str, hours_since_last: int) -> Dict:
        """Check muscle recovery requirements"""
        min_hours, max_hours = self.recovery_hours.get(muscle.lower(), (48, 72))
        is_recovered = hours_since_last >= min_hours
        return {
            "recovered": is_recovered,
            "hours_since": hours_since_last,
            "minimum_required": min_hours,
            "optimal": max_hours,
            "recommendation": "Ready to train" if is_recovered else f"Wait {min_hours - hours_since_last} more hours"
        }
    
    def check_contraindications(self, conditions: List[str], exercise_type: str) -> List[Dict]:
        """Check exercise against user health conditions"""
        warnings = []
        for rule in self.rules:
            if rule.condition in conditions:
                if exercise_type in rule.contraindicated_exercises:
                    warnings.append({
                        "level": rule.level.value,
                        "condition": rule.condition,
                        "guidance": rule.guidance,
                        "source": rule.source
                    })
        return warnings
```

---

## Nutrition verification leverages robust free APIs

The nutrition domain has excellent free API coverage through government and open-source databases.

### USDA FoodData Central is the authoritative free source

USDA FDC provides **research-grade nutritional data** for 300,000+ foods under CC0 public domain license.

```python
class USDAFoodData:
    BASE_URL = "https://api.nal.usda.gov/fdc/v1"
    
    def __init__(self, api_key: str):
        # Free key at https://fdc.nal.usda.gov/api-key-signup.html
        self.api_key = api_key
    
    def search_foods(self, query: str, page_size: int = 10) -> Dict:
        """Search foods by keyword"""
        resp = requests.post(
            f"{self.BASE_URL}/foods/search",
            params={"api_key": self.api_key},
            json={
                "query": query,
                "pageSize": page_size,
                "dataType": ["Foundation", "SR Legacy", "Branded"]
            },
            timeout=2.0
        )
        return resp.json()
    
    def get_food(self, fdc_id: int) -> Dict:
        """Get detailed nutrients for specific food"""
        resp = requests.get(
            f"{self.BASE_URL}/food/{fdc_id}",
            params={"api_key": self.api_key},
            timeout=2.0
        )
        return resp.json()
    
    def get_nutrients(self, fdc_id: int) -> Dict[str, float]:
        """Extract key nutrients from food data"""
        food = self.get_food(fdc_id)
        nutrients = {}
        for n in food.get("foodNutrients", []):
            name = n.get("nutrient", {}).get("name", "")
            nutrients[name] = n.get("amount", 0)
        return nutrients
```

**Specifications:**
- **Cost**: Free
- **Rate limits**: 1,000 req/hour per IP (free key removes limits)
- **Latency**: 300-400ms typical
- **Coverage**: Foundation Foods (research-grade), SR Legacy, Branded (900K+ products)

### Open Food Facts enables barcode scanning

For barcode-scanned products and international coverage, Open Food Facts provides **3.5 million products** under open license.

```python
class OpenFoodFacts:
    BASE_URL = "https://world.openfoodfacts.org/api/v2"
    
    def __init__(self, user_agent: str = "ATLAS/1.0"):
        self.headers = {"User-Agent": user_agent}  # Required
    
    def get_product(self, barcode: str) -> Optional[Dict]:
        """Lookup product by barcode"""
        resp = requests.get(
            f"{self.BASE_URL}/product/{barcode}",
            headers=self.headers,
            timeout=2.0
        )
        data = resp.json()
        if data.get("status") == 1:
            return data.get("product")
        return None
    
    def get_allergens(self, barcode: str) -> List[str]:
        """Extract allergens from product"""
        product = self.get_product(barcode)
        if product:
            return product.get("allergens_tags", [])
        return []
    
    def get_nutriscore(self, barcode: str) -> Optional[str]:
        """Get Nutri-Score grade (A-E)"""
        product = self.get_product(barcode)
        if product:
            return product.get("nutriscore_grade")
        return None
```

**Specifications:**
- **Cost**: Free (ODbL license, attribution required)
- **Rate limits**: 100 req/min (products), 10 req/min (searches)
- **US coverage**: Good for branded products; variable data quality (crowdsourced)

### Spoonacular free tier handles dietary compliance

For automatic allergen detection and dietary filtering, Spoonacular's free tier provides **50 points/day**—sufficient for ~50 recipe analyses.

```python
class Spoonacular:
    BASE_URL = "https://api.spoonacular.com"
    
    def __init__(self, api_key: str):
        self.api_key = api_key  # Free at spoonacular.com
    
    def analyze_recipe(self, ingredients: List[str]) -> Dict:
        """Analyze recipe for nutrition and diets"""
        resp = requests.post(
            f"{self.BASE_URL}/recipes/parseIngredients",
            params={"apiKey": self.api_key},
            data={
                "ingredientList": "\n".join(ingredients),
                "servings": 1,
                "includeNutrition": True
            },
            timeout=3.0
        )
        return resp.json()
    
    def check_diet_compliance(self, recipe_id: int, diet: str) -> bool:
        """Check if recipe meets dietary restriction"""
        resp = requests.get(
            f"{self.BASE_URL}/recipes/{recipe_id}/information",
            params={
                "apiKey": self.api_key,
                "includeNutrition": False
            },
            timeout=2.0
        )
        info = resp.json()
        diet_map = {
            "vegan": info.get("vegan"),
            "vegetarian": info.get("vegetarian"),
            "gluten_free": info.get("glutenFree"),
            "dairy_free": info.get("dairyFree")
        }
        return diet_map.get(diet, False)
```

**Built-in dietary filters**: Vegan, Vegetarian, Pescetarian, Gluten-Free, Grain-Free, Dairy-Free, Keto, Paleo, FODMAP, Whole30

---

## Content creation verification prioritizes free tools

Given the tight budget, content verification relies primarily on free libraries and APIs.

### Readability scoring with textstat costs nothing

The **textstat** Python library provides all standard readability metrics locally:

```python
import textstat

class ReadabilityAnalyzer:
    def analyze(self, text: str) -> Dict:
        return {
            "flesch_reading_ease": textstat.flesch_reading_ease(text),
            "flesch_kincaid_grade": textstat.flesch_kincaid_grade(text),
            "gunning_fog": textstat.gunning_fog(text),
            "smog_index": textstat.smog_index(text),
            "automated_readability_index": textstat.automated_readability_index(text),
            "coleman_liau_index": textstat.coleman_liau_index(text),
            "reading_time_minutes": textstat.reading_time(text, ms_per_char=14.69) / 60000,
            "consensus_grade": textstat.text_standard(text, float_output=True)
        }
    
    def get_recommendations(self, text: str, target_grade: float = 8.0) -> List[str]:
        """Suggest improvements for target grade level"""
        analysis = self.analyze(text)
        recommendations = []
        
        if analysis["flesch_reading_ease"] < 60:
            recommendations.append("Simplify sentence structure—aim for shorter sentences")
        if analysis["flesch_kincaid_grade"] > target_grade:
            recommendations.append(f"Reduce complexity—currently grade {analysis['flesch_kincaid_grade']:.1f}, target {target_grade}")
        
        return recommendations
```

### AI detection uses GPTZero free tier

GPTZero provides **10,000 words/month free**—sufficient for spot-checking generated content:

```python
class GPTZeroDetector:
    BASE_URL = "https://api.gptzero.me/v2"
    
    def __init__(self, api_key: str):
        self.api_key = api_key
    
    def detect(self, text: str) -> Dict:
        resp = requests.post(
            f"{self.BASE_URL}/predict/text",
            headers={"x-api-key": self.api_key},
            json={"document": text},
            timeout=5.0
        )
        data = resp.json()
        return {
            "completely_generated_prob": data.get("documents", [{}])[0].get("completely_generated_prob"),
            "average_generated_prob": data.get("documents", [{}])[0].get("average_generated_prob"),
            "sentences": data.get("documents", [{}])[0].get("sentences", [])
        }
```

**Important caveat**: All AI detectors show **70-90% real-world accuracy** with significant false positive rates for non-native English writers. Never use as sole basis for decisions.

### YouTube Data API is free for platform optimization

For YouTube content verification, the Data API v3 provides **10,000 quota units/day free**:

```python
class YouTubeAnalyzer:
    BASE_URL = "https://www.googleapis.com/youtube/v3"
    
    def __init__(self, api_key: str):
        self.api_key = api_key
    
    def analyze_video_fit(self, video_id: str) -> Dict:
        """Analyze video metadata for optimization insights"""
        resp = requests.get(
            f"{self.BASE_URL}/videos",
            params={
                "key": self.api_key,
                "id": video_id,
                "part": "snippet,statistics,contentDetails"
            },
            timeout=2.0
        )
        video = resp.json().get("items", [{}])[0]
        return {
            "title_length": len(video.get("snippet", {}).get("title", "")),
            "description_length": len(video.get("snippet", {}).get("description", "")),
            "tags_count": len(video.get("snippet", {}).get("tags", [])),
            "category": video.get("snippet", {}).get("categoryId"),
            "duration": video.get("contentDetails", {}).get("duration"),
            "view_count": int(video.get("statistics", {}).get("viewCount", 0))
        }
```

### Plagiarism: allocate remaining budget to Copyscape

For web plagiarism checking, **Copyscape Premium** at **$0.03/search** provides the best value:

- **$5/month = ~166 plagiarism checks**
- API documentation: https://www.copyscape.com/api-guide.php
- Returns matching URLs, percentage match, and snippets

```python
class CopyscapeChecker:
    BASE_URL = "https://www.copyscape.com/api/"
    
    def __init__(self, username: str, api_key: str):
        self.username = username
        self.api_key = api_key
    
    def check_text(self, text: str) -> Dict:
        """Check text for plagiarism ($0.03 per check)"""
        resp = requests.post(
            self.BASE_URL,
            params={
                "u": self.username,
                "o": self.api_key,
                "t": text,
                "f": "json"
            },
            timeout=10.0
        )
        return resp.json()
```

---

## Caching strategy optimizes cost and latency

Intelligent caching is essential for meeting the <500ms latency requirement while minimizing API calls.

### TTL recommendations by data volatility

| Data Type | TTL | Rationale |
|-----------|-----|-----------|
| Drug interactions | 7-30 days | Rarely changes; literature-derived |
| USDA nutritional data | 14-30 days | Updated quarterly |
| Exercise database | 7-14 days | Stable reference data |
| Open Food Facts products | 24-72 hours | Crowdsourced, moderate churn |
| Content trends/hashtags | 1-6 hours | Highly volatile |

### Caching implementation with Redis

```python
import redis
import json
import hashlib
from typing import Optional, Callable, Any

class VerificationCache:
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis = redis.from_url(redis_url)
        self.ttl_config = {
            "drug_interaction": 86400 * 14,  # 14 days
            "nutrition": 86400 * 14,          # 14 days
            "exercise": 86400 * 7,            # 7 days
            "product": 86400 * 3,             # 3 days
            "content": 3600 * 6               # 6 hours
        }
    
    def _cache_key(self, namespace: str, query: str) -> str:
        query_hash = hashlib.md5(query.encode()).hexdigest()[:12]
        return f"atlas:{namespace}:{query_hash}"
    
    async def get_or_fetch(
        self, 
        namespace: str, 
        query: str, 
        fetch_fn: Callable[[], Any]
    ) -> dict:
        """Cache-aside pattern with automatic TTL"""
        key = self._cache_key(namespace, query)
        
        # Try cache first
        cached = self.redis.get(key)
        if cached:
            return {"data": json.loads(cached), "cached": True}
        
        # Fetch from API
        data = await fetch_fn()
        
        # Cache with appropriate TTL
        ttl = self.ttl_config.get(namespace, 3600)
        self.redis.setex(key, ttl, json.dumps(data))
        
        return {"data": data, "cached": False}
```

### Rate limit handling with exponential backoff

```python
import asyncio
from typing import TypeVar, Callable

T = TypeVar('T')

async def fetch_with_backoff(
    fetch_fn: Callable[[], T],
    max_retries: int = 3,
    base_delay: float = 1.0
) -> T:
    """Exponential backoff for rate-limited APIs"""
    for attempt in range(max_retries):
        try:
            return await fetch_fn()
        except Exception as e:
            if "429" in str(e) or "rate limit" in str(e).lower():
                delay = base_delay * (2 ** attempt)
                await asyncio.sleep(delay)
            else:
                raise
    raise Exception(f"Max retries ({max_retries}) exceeded")
```

---

## Offline fallback architecture ensures safety-critical availability

Safety-critical verification data must be available offline for the <500ms voice response requirement.

### SQLite schema for local safety data

```sql
-- Drug-supplement interactions (sync from Supp.AI)
CREATE TABLE drug_interactions (
    id INTEGER PRIMARY KEY,
    drug_cui TEXT NOT NULL,
    supplement_cui TEXT NOT NULL,
    severity TEXT CHECK(severity IN ('minor', 'moderate', 'major', 'unknown')),
    description TEXT NOT NULL,
    evidence_count INTEGER DEFAULT 0,
    last_updated DATE NOT NULL,
    UNIQUE(drug_cui, supplement_cui)
);
CREATE INDEX idx_drug ON drug_interactions(drug_cui);
CREATE INDEX idx_supplement ON drug_interactions(supplement_cui);

-- Exercise contraindications (manual from ACSM/ACOG)
CREATE TABLE exercise_contraindications (
    id INTEGER PRIMARY KEY,
    condition_code TEXT NOT NULL,
    exercise_category TEXT NOT NULL,
    level TEXT CHECK(level IN ('absolute', 'relative', 'caution')),
    guidance TEXT NOT NULL,
    source TEXT NOT NULL
);
CREATE INDEX idx_condition ON exercise_contraindications(condition_code);

-- Allergen reference data
CREATE TABLE allergens (
    id INTEGER PRIMARY KEY,
    allergen_name TEXT NOT NULL,
    fda_major_allergen BOOLEAN DEFAULT FALSE,
    synonyms TEXT,  -- JSON array
    cross_reactivity TEXT  -- JSON array of related allergens
);

-- Offline cache metadata
CREATE TABLE sync_metadata (
    table_name TEXT PRIMARY KEY,
    last_sync_timestamp INTEGER NOT NULL,
    record_count INTEGER,
    source_version TEXT
);
```

### Sync strategy implementation

```python
import sqlite3
from datetime import datetime, timedelta

class OfflineDataSync:
    SYNC_INTERVALS = {
        "drug_interactions": timedelta(days=7),
        "exercise_contraindications": timedelta(days=30),
        "allergens": timedelta(days=30)
    }
    
    def __init__(self, db_path: str = "atlas_offline.db"):
        self.conn = sqlite3.connect(db_path)
        self._init_schema()
    
    def needs_sync(self, table: str) -> bool:
        cursor = self.conn.execute(
            "SELECT last_sync_timestamp FROM sync_metadata WHERE table_name = ?",
            (table,)
        )
        row = cursor.fetchone()
        if not row:
            return True
        
        last_sync = datetime.fromtimestamp(row[0])
        interval = self.SYNC_INTERVALS.get(table, timedelta(days=7))
        return datetime.now() - last_sync > interval
    
    async def sync_drug_interactions(self, supp_ai: SuppAI):
        """Sync all interactions from Supp.AI"""
        if not self.needs_sync("drug_interactions"):
            return
        
        # Fetch common supplements
        common_supplements = ["vitamin_d", "fish_oil", "magnesium", "zinc", "melatonin"]
        
        for supp in common_supplements:
            results = supp_ai.search_agent(supp)
            for agent in results.get("agents", []):
                cui = agent["cui"]
                interactions = supp_ai.get_interactions(cui)
                
                for interaction in interactions:
                    self._upsert_interaction(interaction)
        
        self._update_sync_metadata("drug_interactions")
    
    def get_interaction_offline(self, drug: str, supplement: str) -> Optional[Dict]:
        """Query local database for interaction"""
        cursor = self.conn.execute("""
            SELECT severity, description, evidence_count 
            FROM drug_interactions 
            WHERE drug_cui = ? AND supplement_cui = ?
        """, (drug, supplement))
        row = cursor.fetchone()
        if row:
            return {
                "severity": row[0],
                "description": row[1],
                "evidence_count": row[2],
                "source": "offline_cache"
            }
        return None
```

### Degraded mode behavior

```python
class VerificationService:
    def __init__(self, online_apis: Dict, offline_db: OfflineDataSync, cache: VerificationCache):
        self.online = online_apis
        self.offline = offline_db
        self.cache = cache
    
    async def verify_drug_interaction(
        self, 
        drug: str, 
        supplement: str,
        require_online: bool = False
    ) -> Dict:
        """Verify interaction with graceful offline fallback"""
        
        # Try cache first (fastest)
        cache_result = await self.cache.get_or_fetch(
            "drug_interaction",
            f"{drug}:{supplement}",
            lambda: None  # Don't fetch yet
        )
        if cache_result.get("data"):
            return {**cache_result["data"], "source": "cache"}
        
        # Try online API
        try:
            online_result = await asyncio.wait_for(
                self.online["supp_ai"].get_interactions(drug, supplement),
                timeout=2.0
            )
            return {**online_result, "source": "online"}
        except (asyncio.TimeoutError, Exception) as e:
            if require_online:
                raise
        
        # Fall back to offline database
        offline_result = self.offline.get_interaction_offline(drug, supplement)
        if offline_result:
            return {
                **offline_result,
                "source": "offline",
                "warning": "Using cached data. Information may not be current."
            }
        
        return {
            "found": False,
            "source": "none",
            "warning": "No interaction data available. Consult a healthcare provider."
        }
```

---

## Privacy compliance requires data minimization

Health data demands careful handling even when HIPAA doesn't technically apply to personal projects.

### Washington MHMDA sets the strictest standard

The **My Health My Data Act** (effective March 2024) applies to any entity targeting Washington consumers and requires:

- Explicit consent before collecting consumer health data
- Right to access, delete, and withdraw consent
- List of all third parties data is shared with
- **Private right of action** (lawsuit risk)

### Data minimization for API calls

```python
import re

class PrivacyPreprocessor:
    """Strip PII before sending to external APIs"""
    
    PII_PATTERNS = [
        (r'\b[A-Z][a-z]+ [A-Z][a-z]+\b', '[NAME]'),           # Names
        (r'\b\d{3}-\d{2}-\d{4}\b', '[SSN]'),                   # SSN
        (r'\b\d{1,2}/\d{1,2}/\d{2,4}\b', '[DATE]'),           # Dates
        (r'\b\d{10}\b', '[PHONE]'),                            # Phone
        (r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]'),  # Email
    ]
    
    def sanitize_query(self, query: str) -> str:
        """Remove PII from query before API call"""
        sanitized = query
        for pattern, replacement in self.PII_PATTERNS:
            sanitized = re.sub(pattern, replacement, sanitized)
        return sanitized
    
    def extract_medical_entities_only(self, query: str) -> Dict:
        """Extract only drug/supplement names, not patient context"""
        # Use simple pattern matching or a medical NER model
        # Return only the entities needed for API lookup
        return {
            "drugs": self._extract_drugs(query),
            "supplements": self._extract_supplements(query),
            "conditions": self._extract_conditions(query)
        }
```

### Required disclaimers

Every health-related response must include appropriate disclaimers:

```python
HEALTH_DISCLAIMER = """
**Disclaimer**: This information is for educational purposes only and is not 
intended as medical advice. Always consult a qualified healthcare provider 
before making changes to medications, supplements, or exercise routines.
"""

EMERGENCY_DISCLAIMER = """
**Emergency**: If you're experiencing a medical emergency, call 911 immediately.
"""

OFFLINE_DISCLAIMER = """
**Offline Mode**: Using cached data that may not reflect the latest information. 
Verify with a healthcare provider for critical decisions.
"""
```

---

## Budget allocation maximizes value within $5 constraint

Given the $5/month verification budget (with ~$15 allocated to Exa.ai), here's the optimal allocation:

### Recommended budget split

| Category | Solution | Monthly Cost |
|----------|----------|--------------|
| Drug interactions | Supp.AI + OpenFDA | $0 (free) |
| Nutrition data | USDA FDC + Open Food Facts | $0 (free) |
| Exercise verification | Free Exercise DB + custom rules | $0 (free) |
| Readability | textstat library | $0 (free) |
| AI detection | GPTZero free tier | $0 (free) |
| Platform APIs | YouTube + Search Console | $0 (free) |
| **Plagiarism** | **Copyscape Premium** | **$5 (~166 checks)** |

### Priority ranking for budget allocation

If budget increases, prioritize in this order:

1. **Copyscape** ($5/month) — Web plagiarism has no good free alternative
2. **Originality.ai pay-as-you-go** ($30 one-time for 3,000 credits) — Better AI detection accuracy
3. **Spoonacular paid tier** ($29/month) — More dietary compliance checks
4. **NatMed Pro** ($182/year) — Gold-standard supplement safety data

---

## Implementation checklist

**Immediate setup (free)**:
- [ ] Download Free Exercise DB JSON to local storage
- [ ] Register for USDA FDC API key
- [ ] Register for OpenFDA API key
- [ ] Set up YouTube Data API credentials
- [ ] Install textstat: `pip install textstat`
- [ ] Create SQLite offline database with safety schema

**Week 1**:
- [ ] Implement Supp.AI integration for supplement-drug interactions
- [ ] Build exercise safety rules engine from ACSM/ACOG guidelines
- [ ] Set up Redis caching layer with appropriate TTLs
- [ ] Implement privacy preprocessor for API queries

**Week 2**:
- [ ] Implement USDA FDC + Open Food Facts integration
- [ ] Build allergen detection from FDA Big 9 list
- [ ] Create offline sync mechanism for safety-critical data
- [ ] Add Copyscape integration for plagiarism checking

**Week 3**:
- [ ] Implement degraded mode with offline fallbacks
- [ ] Add comprehensive disclaimers to all health responses
- [ ] Test latency meets <500ms requirement
- [ ] Document MHMDA compliance measures

This architecture provides comprehensive verification grounding for ATLAS while respecting the strict budget, latency, and privacy constraints. The combination of free government APIs, open-source databases, and strategic caching delivers production-ready verification without ongoing costs beyond the $5 plagiarism allocation.