# SQLite for AI agent memory: A 2025 architecture guide

Your planned stack of **SQLite + sqlite-vec + FTS5 + all-MiniLM-L6-v2** is fundamentally sound, though one upgrade is strongly recommended: swap the embedding model to **BGE-small-en-v1.5**, which delivers ~10% better retrieval accuracy at comparable size. For 100K records at 384 dimensions, expect **<75ms hybrid search latency** and **500-700MB total storage**—well within your 6GB RAM budget. The architecture will handle years of growth before requiring migration.

## sqlite-vec is production-ready with important caveats

Version **v0.1.6** (November 2024) represents the current stable release, backed by Mozilla Builders and corporate sponsors including Fly.io and Turso. Written in pure C with zero dependencies, it's deployable anywhere SQLite runs—including WASM and mobile. The critical limitation: **brute-force KNN only** (no ANN indexes yet), which constrains practical scale to roughly 500K vectors for sub-100ms queries.

Memory benchmarks at your target scale of **100K vectors × 384 dimensions**:

| Metric | Float32 | Binary Quantized |
|--------|---------|------------------|
| Storage | ~147 MB | ~4.6 MB |
| Query latency | <75ms | ~3ms |
| RAM during query | Chunked (not all in RAM) | Minimal |

**Filtered vector search is now supported** as of v0.1.6, implementing bitmap intersection that applies metadata filters *before* distance calculations. This enables efficient queries like "find similar memories within the last 7 days for user X":

```sql
CREATE VIRTUAL TABLE memories USING vec0(
    memory_id INTEGER PRIMARY KEY,
    embedding float[384],
    user_id TEXT PARTITION KEY,    -- Pre-shards index (fastest)
    importance INTEGER,            -- Metadata filter
    created_at TEXT,               -- Metadata filter
    +content TEXT                  -- Auxiliary (SELECT-only, unindexed)
);

SELECT memory_id, content, distance
FROM memories  
WHERE embedding MATCH ?query_vector
  AND k = 10
  AND user_id = 'user123'
  AND importance >= 3;
```

**Why not alternatives?** ChromaDB requires ~400MB minimum RAM and loads everything into memory. sqlite-vss (the predecessor) was deprecated due to Faiss dependency complexity and write performance issues. For your embedded, RAM-constrained use case, sqlite-vec wins on simplicity and resource efficiency.

## FTS5 configuration and hybrid search fusion

For conversational AI memory in English, use the **Porter stemmer wrapping Unicode61**:

```sql
CREATE VIRTUAL TABLE fts_memory USING fts5(
    content,
    content='memory_docs', content_rowid='id',
    tokenize = 'porter unicode61 remove_diacritics 1',
    prefix = '2 3'
);
```

This allows "remember," "remembered," and "remembering" to match the same stem while gracefully handling Unicode edge cases. FTS5's built-in BM25 uses hardcoded k1=1.2 and b=0.75—the empirically optimal defaults that Elastic's research confirmed require no tuning for most datasets.

**Hybrid search via Reciprocal Rank Fusion (RRF)** is the gold standard for combining keyword and vector results. RRF uses ranks rather than raw scores, eliminating normalization headaches:

```sql
WITH 
vec_matches AS (
    SELECT memory_id, ROW_NUMBER() OVER (ORDER BY distance) AS rank_num
    FROM vec_memory WHERE embedding MATCH :query_embedding AND k = 20
),
fts_matches AS (
    SELECT rowid AS memory_id, ROW_NUMBER() OVER (ORDER BY rank) AS rank_num
    FROM fts_memory WHERE fts_memory MATCH :query_text LIMIT 20
),
rrf AS (
    SELECT 
        COALESCE(v.memory_id, f.memory_id) AS memory_id,
        (COALESCE(1.0/(60 + f.rank_num), 0) * 0.4 +
         COALESCE(1.0/(60 + v.rank_num), 0) * 0.6) AS score
    FROM vec_matches v FULL OUTER JOIN fts_matches f ON v.memory_id = f.memory_id
)
SELECT m.*, r.score FROM rrf r JOIN memories m ON m.id = r.memory_id
ORDER BY r.score DESC LIMIT 10;
```

The constant 60 comes from Cormack et al.'s research; weights of **0.4 FTS / 0.6 vector** work well for semantic-heavy AI memory retrieval. At 100K documents, expect **<10ms FTS queries** and ~700MB-1GB index size with `detail=column` optimization.

## BGE-small-en-v1.5 should replace all-MiniLM-L6-v2

All-MiniLM-L6-v2 was excellent when released in 2021, but newer models offer meaningful improvements:

| Model | MTEB Score | Dimensions | INT8 Size | Context | Speed |
|-------|-----------|------------|-----------|---------|-------|
| all-MiniLM-L6-v2 | ~56 | 384 | ~23MB | 256 tokens | ~14k/sec |
| **BGE-small-en-v1.5** | **~62** | 384 | ~32MB | 512 tokens | ~10k/sec |
| E5-small-v2 | ~61 | 384 | ~32MB | 512 tokens | ~10k/sec |
| nomic-embed-text-v1.5 | ~62 | 64-768 | ~140MB | 8192 tokens | ~3k/sec |

BGE-small delivers **~10% better retrieval accuracy** at only 9MB larger when INT8 quantized. The 512-token context (vs 256) handles longer conversation turns without truncation.

**INT8 quantization impact**: Expect **95-99% accuracy retention** with 1-5% similarity score deviation. For ranking tasks (your primary use case), relative ordering is preserved. Use ONNX dynamic quantization:

```python
from sentence_transformers import SentenceTransformer, export_dynamic_quantized_onnx_model

model = SentenceTransformer("BAAI/bge-small-en-v1.5", backend="onnx")
export_dynamic_quantized_onnx_model(
    model=model,
    quantization_config="avx512_vnni",  # or "arm64" for Apple Silicon
    model_name_or_path="./bge-small-int8"
)
```

**Late 2024 development**: Model2Vec (October 2024) offers 100-500x faster embeddings via static distillation at ~85% quality—worth considering for extreme latency requirements but unnecessary for your use case.

## Schema design for three-tier memory architecture

Based on MemGPT/Letta and Mem0 patterns, implement tiered memory with soft-delete and versioning:

```sql
PRAGMA journal_mode = WAL;
PRAGMA synchronous = NORMAL;
PRAGMA mmap_size = 6000000000;

-- Core memory: Always in-context identity/persona
CREATE TABLE core_memory (
    id INTEGER PRIMARY KEY,
    agent_id TEXT NOT NULL,
    block_label TEXT NOT NULL,  -- 'persona', 'human', 'system'
    content TEXT NOT NULL,
    UNIQUE(agent_id, block_label)
);

-- Semantic memory: Facts and preferences
CREATE TABLE semantic_memory (
    id INTEGER PRIMARY KEY,
    agent_id TEXT NOT NULL,
    content TEXT NOT NULL,
    importance REAL DEFAULT 0.5,
    access_count INTEGER DEFAULT 0,
    last_accessed TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_active BOOLEAN DEFAULT 1,
    embedding BLOB,
    supersedes_id INTEGER REFERENCES semantic_memory(id)
);

-- Episodic memory: Conversation history
CREATE TABLE episodic_memory (
    id INTEGER PRIMARY KEY,
    conversation_id INTEGER,
    role TEXT NOT NULL,
    content TEXT NOT NULL,
    importance REAL DEFAULT 0.5,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    embedding BLOB
);

-- Vector indexes (sync via triggers)
CREATE VIRTUAL TABLE vec_semantic USING vec0(
    memory_id INTEGER PRIMARY KEY,
    embedding float[384]
);
```

**Store embeddings as both BLOB and vec0**: BLOBs enable backup/portability; vec0 enables fast KNN. Sync via triggers for atomic updates.

For **importance scoring**, use Mem0's production-tested formula:

```sql
SELECT id, content,
    (relevance_score * 0.5 +
     (1.0 / (1.0 + (julianday('now') - julianday(last_accessed)) / 7.0)) * 0.3 +
     importance * 0.2 +
     MIN(access_count / 100.0, 0.1)) AS score
FROM semantic_memory
WHERE is_active = 1
ORDER BY score DESC;
```

**Decay and pruning**: Implement exponential decay with ~7-day half-life. Mark memories inactive when retrievability drops below 10%:

```sql
UPDATE semantic_memory SET is_active = 0
WHERE importance < 0.3 AND access_count < 3
  AND last_accessed < datetime('now', '-90 days');
```

## Performance reality at 100K records

**Hybrid search latency breakdown**:
- Vector search (384-dim float32): **50-80ms**
- FTS5 BM25 query: **<10ms**
- RRF fusion overhead: **~5ms**
- **Total hybrid search: 60-100ms**

With binary quantization (96 bytes/vector vs 1.5KB), vector search drops to **<10ms** with ~95% recall.

**WAL mode is essential**—it delivers **10-12x faster writes** in concurrent scenarios (3,600 writes/sec vs 291 in rollback mode). Real-time embedding + insert is feasible since SQLite insert takes <1ms; the 20-100ms embedding generation is the actual bottleneck.

**File size at 100K records**:
| Component | Size |
|-----------|------|
| Semantic metadata | ~20 MB |
| Float32 embeddings (384-dim) | ~150 MB |
| Episodic messages | ~50 MB |
| Indexes + FTS5 | ~150 MB |
| **Total** | **~500-700 MB** |

With binary quantized embeddings: **200-300 MB total**.

## When to consider LanceDB or other alternatives

**LanceDB** emerges as the most compelling alternative for vector-heavy workloads. It's disk-based (minimal RAM), supports native hybrid search with Tantivy, and handles 200M+ vectors in production. Query latency: **~25ms at 1M vectors** with IVF-PQ indexing.

**Decision framework**:

- **Stick with SQLite** when: <500K vectors, need graph traversal (CTEs), value single-file simplicity, mixed workload requirements
- **Consider LanceDB** when: Vector search is primary bottleneck, planning >1M vectors, need multi-modal storage, want automatic versioning
- **Avoid DuckDB vss** for production: Index persistence is experimental, must fit entirely in RAM

For graph relationships, **SQLite CTEs handle 100K nodes at depth 5 in 200-500ms**:

```sql
WITH RECURSIVE related(id, depth, path) AS (
    SELECT id, 0, CAST(id AS TEXT) FROM memories WHERE id = ?
    UNION
    SELECT m.id, r.depth + 1, r.path || '->' || m.id
    FROM related r
    JOIN memory_links l ON l.source_id = r.id
    JOIN memories m ON m.id = l.target_id
    WHERE r.depth < 5 AND r.path NOT LIKE '%' || m.id || '%'
)
SELECT DISTINCT id, MIN(depth) FROM related GROUP BY id;
```

Only add NetworkX for complex graph algorithms (PageRank, community detection) on subgraphs—load relevant portions from SQLite, analyze in-memory.

## Recommended final architecture

```
┌─────────────────────────────────────────────────────────┐
│  Embedding: BGE-small-en-v1.5 (INT8 ONNX, ~32MB)       │
├─────────────────────────────────────────────────────────┤
│  SQLite Database (WAL mode, single file)               │
│  ├── Core Memory (persona, human blocks)               │
│  ├── Semantic Memory (facts + BLOB embeddings)         │
│  ├── Episodic Memory (conversations + BLOB embeddings) │
│  ├── vec0 Virtual Tables (KNN search)                  │
│  ├── FTS5 Index (keyword search)                       │
│  └── Memory Links (relationship graph via CTEs)        │
├─────────────────────────────────────────────────────────┤
│  Hybrid Search: RRF fusion (60% vector, 40% FTS)       │
│  Scoring: relevance×0.5 + recency×0.3 + importance×0.2 │
│  Decay: Exponential, 7-day half-life, prune at <10%    │
└─────────────────────────────────────────────────────────┘
```

**Resource utilization at 100K records**: ~300MB model + ~600MB database = **<1GB total**, leaving 5GB+ headroom in your 6GB budget. Scale to ~500K records before reconsidering architecture.

## Conclusion

The SQLite-based stack remains the optimal choice for embedded AI memory at your scale. The key refinements: upgrade to BGE-small-en-v1.5 for better retrieval, implement RRF hybrid search for combining FTS5 and vector results, use WAL mode with chunked vec0 queries, and apply the three-tier schema with importance scoring and exponential decay.

The architecture provides a clear migration path: if vector latency becomes a bottleneck beyond 500K records, LanceDB can handle vectors while SQLite continues managing relationships and FTS. For now, the single-file simplicity and proven reliability of SQLite is the pragmatic choice for a personal AI assistant with long-term memory requirements.