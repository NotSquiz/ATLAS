# Hybrid neural-symbolic architecture for ATLAS personal AI assistant

A personal AI assistant running on **6GB RAM with 4GB VRAM** is now feasible using quantized models like Qwen3-4B combined with symbolic workflow orchestration. The optimal architecture separates neural intent understanding from symbolic execution, using LangGraph's StateGraph for reliable workflows and a CLAUDE.md-style learning system for continuous improvement. This hybrid approach achieves sub-2-second voice interactions while maintaining safety through Constitutional AI principles and human-in-the-loop approval gates for irreversible actions.

The convergence of small language models, efficient quantization (GGUF Q4_K_M), and mature frameworks like LangGraph and Semantic Kernel has made sophisticated personal assistants deployable on consumer hardware. Research from 2024-2025 confirms that hybrid neural-symbolic systems outperform pure approaches—neural for flexibility and understanding, symbolic for reliability and determinism.

---

## Hybrid architecture patterns establish clear neural-symbolic boundaries

Production frameworks have converged on a common pattern: **neural components handle language understanding and reasoning while symbolic components manage deterministic execution and validation**. Microsoft's Semantic Kernel uses a plugin architecture where the LLM decides when to invoke typed functions. LangGraph implements directed graphs where nodes can be LLM calls or pure code, with conditional edges enabling runtime routing. CrewAI separates Flows (deterministic orchestration) from Crews (autonomous agent teams).

The most effective integration pattern is **Neural Planning with Symbolic Execution**: the LLM generates an action plan, a symbolic validator ensures feasibility, a deterministic executor runs validated steps, and results feed back to the LLM for reflection. This creates clear handoff points between paradigms while leveraging each component's strengths.

LangGraph's StateGraph has emerged as the dominant pattern for AI workflows, offering three critical advantages over traditional finite state machines. First, it supports **conditional branching at runtime** based on LLM decisions or confidence scores. Second, it enables **cyclic execution** for iterative refinement loops essential to agentic behavior. Third, automatic **checkpointing** at every node provides fault tolerance and time-travel debugging. The typed state schema (TypedDict with Annotated reducers) enforces structure while remaining flexible:

```python
class WorkflowState(TypedDict):
    messages: Annotated[List[Message], add_messages]  # Neural context
    current_step: str                                   # Symbolic control
    validated_data: dict                                # Symbolic outputs
    confidence: float                                   # Routing signal
```

DSPy introduces a compelling alternative through **declarative signatures** that define typed input/output contracts, separating interface ("what should the LLM do?") from implementation ("how to prompt?"). This enables automatic prompt optimization and model-agnostic programs.

---

## Neural intent understanding requires confidence-based disambiguation

The neural layer's primary responsibility is transforming ambiguous user requests into structured intents suitable for symbolic execution. Research shows that **hybrid intent detection systems**—combining LLM classification with contrastively fine-tuned sentence transformers—achieve within 2% of pure LLM accuracy at 50% less latency, critical for voice-first interactions.

For ATLAS, implement a tiered confidence strategy: queries above **0.9 confidence** proceed automatically, those between **0.7-0.9** request implicit confirmation through response framing ("I'll schedule that meeting for 3 PM"), and those below **0.7** trigger explicit clarification. This maps directly to LangGraph's conditional edges:

```python
def route_by_confidence(state) -> str:
    if state["confidence"] > 0.9: return "execute"
    if state["confidence"] > 0.7: return "confirm"
    return "clarify"
```

Intent rewriting—transforming ambiguous inputs into clear, actionable goals—proves essential for agentic planning. The RECAP framework demonstrates that **DPO-trained rewriters yield significant utility gains** when handling intent drift, vagueness, and mixed-goal conversations common in voice interactions.

Context-aware resolution requires maintaining hierarchical memory: session context (current conversation), episodic memory (past interactions), and user profile (persistent preferences). The neural layer retrieves relevant context through embedding similarity and injects it into the prompt window for grounded intent resolution.

---

## Symbolic workflow execution ensures deterministic, recoverable behavior

Once intent is understood, the symbolic layer takes over for reliable execution. LangGraph's StateGraph pattern provides the optimal foundation for ATLAS workflows, offering several critical capabilities that pure state machines lack.

**Checkpointing enables fault recovery and human review**. Every super-step (node completion) automatically persists state. Use SqliteSaver for local deployment—it handles the 6GB RAM constraint efficiently. When workflows fail, they resume from the last checkpoint without re-executing completed nodes. Time-travel debugging allows replaying from any historical state:

```python
config = {
    "configurable": {
        "thread_id": "session_123",
        "checkpoint_id": "specific_checkpoint_uuid"
    }
}
graph.invoke(None, config=config)  # Forks execution from checkpoint
```

**Tool orchestration follows a three-layer validation strategy**: strict schema validation first, pattern recovery for minor format issues, then graceful degradation. Implement circuit breakers to prevent cascade failures when external services are unavailable—after 5 failures within 60 seconds, the circuit opens and immediately returns fallback responses until a timeout period passes.

For error handling, wrap all tool calls with retry logic using exponential backoff with jitter (prevents thundering herd when services recover). Configure timeouts appropriately: **30-60 seconds for LLM inference, 10-30 seconds for tool execution, 5-10 seconds for database operations**.

The **Saga pattern** handles multi-step operations requiring transaction-like semantics. Each step records a compensation action; if any step fails, compensations execute in reverse order. This is essential for operations like "book flight and hotel" where partial completion is problematic.

---

## Compound learning through CLAUDE.md prevents recurring mistakes

Boris Cherny's team at Anthropic has developed a production-proven approach to continuous improvement through evolving context files. The **CLAUDE.md system** captures team learnings, corrections, and best practices in a version-controlled file that the AI reads at every session. When Claude makes an error, the team adds a correction to CLAUDE.md so the mistake doesn't recur.

Implement a multi-tier configuration for ATLAS:

- **System CLAUDE.md**: Core assistant behaviors and safety constraints
- **User CLAUDE.md** (~/.atlas/context.md): Personal preferences and corrections
- **Domain CLAUDE.md**: Domain-specific knowledge (health, productivity, content)
- **Local overrides**: Session-specific context not persisted

The key insight is that **verification feedback loops dramatically improve output quality**. Cherny notes that giving the AI a way to verify its work (running tests, checking results, iterating) yields 2-3x quality improvement. For ATLAS, implement verification hooks after tool execution that validate results against expected patterns before presenting to users.

Learning what to capture requires distinguishing **one-off instructions from general preferences**. "Make this button pink" shouldn't trigger "make all buttons pink forever." The solution: synthesize patterns from many interaction logs before adding to persistent context. Implement a threshold—only preferences demonstrated **3+ times across separate sessions** graduate to persistent memory.

Memory architecture should separate storage concerns: SQLite for episodic traces (action sequences, outcomes), local vector store (ChromaDB) for semantic memory (documents, learned facts), and the CLAUDE.md text file for procedural knowledge (rules, corrections, workflows).

---

## Constitutional AI principles enable inference-time safety

Anthropic's Constitutional AI provides a framework for building safe assistants without requiring human feedback for every decision. While CAI is primarily a training method, its principles apply at inference time through **self-critique chains**: generate initial response, critique against principles, revise if violations detected.

Implement a layered safety architecture for ATLAS:

1. **Input validation**: Content filtering, prompt injection detection
2. **Constitutional guardrails**: Self-critique against explicit principles
3. **Risk classification**: Categorize actions as high/medium/low risk based on irreversibility and impact
4. **Human-in-the-loop gates**: Require approval for high-risk actions

**High-risk actions that always require approval**: data deletion, financial transactions, external communications (emails, posts), access changes. **Medium-risk actions with configurable approval**: modifications to files or records. **Low-risk actions auto-approved**: read operations, searches, draft generation.

For confidence-based escalation, combine risk classification with model uncertainty. If the model's confidence is below **0.7**, escalate to human review regardless of action risk level. First-time patterns (actions the agent hasn't performed before) also trigger review until the user has approved 3+ similar actions.

Audit trails are mandatory for trustworthy assistants. Log: user ID, session ID, model version, input prompt, output response, confidence score, reasoning chain, approval decision if applicable. Use immutable storage (append-only logs) and cryptographic verification for compliance.

**Sandboxing protects execution**: run tool code in isolated environments with explicit permission boundaries. Define what filesystem paths, network domains, and action types the agent can access. Implement dry-run mode for uncertain operations—show what would happen without executing.

---

## Resource-constrained deployment requires careful model and pipeline selection

For 6GB RAM and 4GB VRAM on WSL2, **Qwen3-4B with Q4_K_M quantization** emerges as the optimal choice—it requires approximately 2.75GB VRAM while matching or exceeding older 70B+ models on reasoning benchmarks. Alternative options include Phi-3 Mini (3.8B) and Llama 3.2-3B, both fitting comfortably within constraints.

The voice pipeline architecture must optimize for end-to-end latency under 2 seconds:

| Component | Recommended | Latency | Memory |
|-----------|-------------|---------|--------|
| VAD | Silero-VAD | ~20ms | Minimal |
| STT | faster-whisper small | 300-500ms | ~1GB VRAM |
| LLM | Qwen3-4B Q4_K_M | 500-1000ms | ~2.75GB VRAM |
| TTS | Piper (CPU) | 50-100ms | CPU only |

**KV cache quantization is critical** for fitting larger contexts in limited VRAM. Enable Flash Attention and q8_0 KV cache quantization in Ollama, reducing cache memory by 50% with minimal quality impact:

```bash
export OLLAMA_FLASH_ATTENTION=1
export OLLAMA_KV_CACHE_TYPE=q8_0
export OLLAMA_NUM_PARALLEL=1
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_CONTEXT_LENGTH=4096
```

WSL2 configuration requires explicit memory limits to prevent Windows starvation. Create `.wslconfig` with 5GB memory limit (leaving 1GB for Windows) and 8GB swap on SSD for handling model loading spikes.

**Hybrid local/cloud architecture** provides fallback for complex tasks. Route to local inference first for simple queries and privacy-sensitive data. Fall back to cloud APIs (Claude, GPT-4) for complex reasoning, long context requirements (>4K tokens), or when local resources are constrained. Research indicates this routing strategy reduces cloud API calls by **40%** with no quality degradation.

---

## Multi-domain skills require modular capability architecture

ATLAS needs to handle health tracking, content creation, and productivity management within a unified interface. Implement a **router agent** that classifies domains and delegates to specialized sub-workflows, each with domain-appropriate tools and context.

For proactive versus reactive behavior, define clear trigger conditions. **Proactive triggers**: scheduled events (reminders, check-ins), detected anomalies (unusual patterns in health data), and context transitions (arriving at work location). **Reactive behavior** remains the default—the assistant responds to explicit requests. Users should be able to configure proactivity level, with conservative defaults.

Voice-first design imposes specific constraints: responses must be concise (1-2 sentences by default), avoid complex formatting that doesn't translate to speech, and support interruption. Implement **barge-in detection** to stop TTS when users start speaking. Use progressive disclosure—provide brief answers with offers to elaborate.

Privacy architecture follows local-first principles. Sensitive data (health records, financial info, passwords) never leaves the device. Process with local models exclusively. Non-sensitive queries can route to cloud with appropriate anonymization. Implement explicit consent flows for any cloud data sharing.

---

## Implementation roadmap for ATLAS

Based on the research synthesis, here's the recommended phased approach:

**Phase 1 - Core Architecture (Weeks 1-4)**
- Set up WSL2 with optimized memory configuration
- Deploy Qwen3-4B via Ollama with quantized KV cache
- Implement LangGraph StateGraph for basic request→process→respond workflow
- Add SQLite checkpointing for state persistence
- Create initial CLAUDE.md with core behaviors and safety constraints

**Phase 2 - Voice Pipeline (Weeks 5-8)**
- Integrate faster-whisper for STT with Silero-VAD preprocessing
- Add Piper TTS for speech output
- Implement streaming inference for reduced time-to-first-token
- Build barge-in detection and response interruption

**Phase 3 - Safety and Governance (Weeks 9-12)**
- Implement risk classification for all tool actions
- Add human-in-the-loop approval flows for high-risk operations
- Create audit logging with structured traces
- Build dry-run and preview modes for uncertain operations

**Phase 4 - Learning and Personalization (Weeks 13-16)**
- Implement user-specific CLAUDE.md with preference capture
- Add ChromaDB for semantic memory
- Build feedback loops for capturing corrections
- Create graduation logic for temporary→persistent preferences

---

## Conclusion

Building a capable personal AI assistant within 6GB RAM and 4GB VRAM constraints is achievable through thoughtful hybrid architecture. The key insights from this research:

**Architecture separation is essential**: neural components excel at understanding and generation while symbolic components provide reliability and determinism. LangGraph's StateGraph offers the optimal workflow foundation with built-in checkpointing, conditional routing, and human-in-the-loop support.

**Learning without retraining works**: Boris Cherny's CLAUDE.md pattern demonstrates that continuous improvement happens through evolving context files rather than model updates—version-controlled, team-collaborative, and immediately effective.

**Safety requires multiple layers**: Constitutional principles, risk-based classification, human approval gates, sandboxed execution, and comprehensive audit trails combine to create trustworthy autonomous behavior.

**Resource constraints drive smart design choices**: Qwen3-4B with Q4_K_M quantization, KV cache compression, faster-whisper STT, and Piper TTS create a complete voice pipeline within hardware limits. Hybrid local/cloud routing provides fallback for complex tasks while preserving privacy for sensitive data.

The academic literature confirms these practical approaches: 2024-2025 research on neural-symbolic integration, ReAct-style reasoning, and memory-augmented agents provides theoretical grounding for the ATLAS architecture. The convergence of small but capable models, mature orchestration frameworks, and proven learning patterns makes sophisticated personal AI assistants deployable on consumer hardware today.