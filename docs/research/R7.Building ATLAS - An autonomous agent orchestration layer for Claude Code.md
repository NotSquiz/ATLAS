# Building ATLAS: An autonomous agent orchestration layer for Claude Code

**Claude Code provides native multi-agent orchestration capabilities** that can power ATLAS as an autonomous Chief of Staff without requiring external frameworks. The system combines four core components: **subagents for parallel task delegation**, **hooks for deterministic safety enforcement**, **skills for domain expertise**, and **MCP tools for external integrations**. The most effective architecture uses a hierarchical delegation pattern where the main orchestrator decomposes tasks into specialized subagent workflows, with checkpoint-based recovery and risk-tiered human-in-the-loop triggers.

The key insight from examining production implementations is that Claude Code's built-in primitives—particularly subagents and hooks—already provide the delegation and safety mechanisms that external frameworks like LangChain or AutoGen offer, but with tighter integration to the CLI workflow. The critical gap to address is **state persistence across sessions** and **async task queue management**, which require custom MCP servers or external infrastructure.

---

## Architecture for the ATLAS orchestration layer

The recommended architecture layers Claude Code's native capabilities with minimal external dependencies:

```
┌─────────────────────────────────────────────────────────────────────┐
│                        ATLAS Orchestration Layer                     │
├─────────────────────────────────────────────────────────────────────┤
│  ┌───────────────────┐    ┌───────────────────┐    ┌─────────────┐ │
│  │   Main Agent      │    │   Task Queue      │    │   Memory    │ │
│  │   (Orchestrator)  │◀──▶│   MCP Server      │◀──▶│   MCP       │ │
│  │   Claude Code CLI │    │   (SQLite/Redis)  │    │   Server    │ │
│  └────────┬──────────┘    └───────────────────┘    └─────────────┘ │
│           │                                                         │
│     ┌─────┴─────────────────────────────────────┐                  │
│     │              Subagent Pool                 │                  │
│     ├──────────────┬──────────────┬─────────────┤                  │
│     │ code-agent   │ test-agent   │ research-   │                  │
│     │ (Read,Write  │ (Bash,Read)  │ agent       │                  │
│     │  Edit,Bash)  │              │ (WebSearch) │                  │
│     └──────────────┴──────────────┴─────────────┘                  │
│           │                                                         │
│  ┌────────┴────────────────────────────────────────────────────┐   │
│  │                    Safety Layer (Hooks)                      │   │
│  │  PreToolUse: Risk scoring, allowlist validation, audit log   │   │
│  │  PostToolUse: Auto-format, test validation, checkpoint       │   │
│  │  Stop: Completion notification, human escalation triggers    │   │
│  └──────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────┘
```

The orchestrator maintains task state through a custom MCP server that provides three tools: `enqueue_task`, `get_next_task`, and `mark_complete`. Each task includes priority (1-5), dependencies (list of task IDs), and assigned subagent type. The memory MCP server persists conversation context and intermediate results across sessions using SQLite with optional vector search for semantic retrieval.

---

## Hooks, skills, and MCP tools serve distinct purposes

Understanding when to use each component is essential for clean architecture:

**Hooks** provide deterministic lifecycle control—shell commands that execute at specific points regardless of LLM behavior. Use hooks for security enforcement (blocking dangerous commands), audit logging, auto-formatting, and notifications. The **PreToolUse** hook is critical for ATLAS because it can inspect and block any tool call before execution with exit code 2.

**Skills** load domain expertise on-demand through progressive disclosure. When Claude detects relevance from the skill's description, it loads the full instructions (under 5,000 tokens recommended). Skills are ideal for teaching procedures—code review checklists, deployment workflows, or domain-specific patterns—without consuming context tokens until needed.

**MCP tools** connect to external systems through the standardized Model Context Protocol. Use MCP for database access, API integrations, file system operations beyond the sandbox, and custom orchestration infrastructure like task queues.

**Subagents** provide isolated execution contexts with their own tool permissions and context windows. The main orchestrator invokes subagents through tool calls, receiving their results. This isolation is essential—a research subagent with web access shouldn't have file write permissions, while a code-generation subagent needs Edit but not external network access.

| Component | Deterministic | Context Isolated | External Access | Best For |
|-----------|--------------|------------------|-----------------|----------|
| Hooks | Yes | No | Shell only | Safety, logging, formatting |
| Skills | No (LLM-invoked) | No | None | Domain expertise, procedures |
| MCP Tools | Yes (via tool call) | No | Unlimited | External integrations |
| Subagents | No | Yes | Per-agent config | Parallel tasks, specialization |

---

## Subagent configuration for task delegation

Define subagents in `.claude/agents/` with YAML frontmatter specifying tools, model, and permission mode:

```yaml
# .claude/agents/code-generator.md
---
name: code-generator
description: Implement features and fix bugs. Use PROACTIVELY when creating new code or modifying existing files.
tools: Read, Write, Edit, MultiEdit, Glob, Grep, Bash
model: sonnet
permissionMode: acceptEdits
skills: coding-standards, error-handling
---

You implement features following project conventions. Before writing code:
1. Read existing patterns in related files
2. Check CLAUDE.md for project-specific rules
3. Write tests alongside implementation

Output format: Brief summary of changes made, files modified, and tests added.
```

```yaml
# .claude/agents/test-runner.md
---
name: test-runner
description: Execute test suites and analyze failures. Use when validating changes or investigating test failures.
tools: Read, Bash, Grep
model: haiku
permissionMode: default
---

Run tests and provide actionable analysis. For failures:
- Identify root cause vs. symptoms
- Check if tests need updating or code has bugs
- Suggest minimal fixes

Never modify code directly—report findings for code-generator to fix.
```

The orchestrator delegates by natural language—Claude automatically routes to appropriate subagents based on task description. For explicit control, use the CLI format:

```bash
# Programmatic delegation with session management
session_id=$(claude -p "Implement user authentication with JWT tokens" \
  --output-format json \
  --allowedTools "Read,Write,Edit,Bash" \
  --max-turns 20 | jq -r '.session_id')

# Continue the session with follow-up
claude -p --resume "$session_id" "Add refresh token rotation"
```

---

## Task queue management requires custom infrastructure

Claude Code doesn't include built-in async task queues, but you can implement one through an MCP server. The pattern uses a SQLite database with priority scheduling:

```python
# task_queue_mcp.py
from mcp.server.fastmcp import FastMCP
import sqlite3
from datetime import datetime

mcp = FastMCP("task-queue")

@mcp.tool()
async def enqueue_task(
    description: str,
    task_type: str,  # "code", "test", "research", "review"
    priority: int = 3,  # 1=urgent, 5=background
    dependencies: str = ""  # comma-separated task IDs
) -> str:
    """Add a task to the queue for async processing."""
    conn = sqlite3.connect("tasks.db")
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO tasks (description, task_type, priority, status, dependencies, created_at)
        VALUES (?, ?, ?, 'pending', ?, ?)
    """, (description, task_type, priority, dependencies, datetime.utcnow().isoformat()))
    task_id = cursor.lastrowid
    conn.commit()
    return f"Task {task_id} queued with priority {priority}"

@mcp.tool()
async def get_next_task(task_type: str = "") -> str:
    """Get the highest-priority pending task, optionally filtered by type."""
    conn = sqlite3.connect("tasks.db")
    cursor = conn.cursor()
    query = """
        SELECT id, description, task_type, priority FROM tasks 
        WHERE status = 'pending' AND (? = '' OR task_type = ?)
        ORDER BY priority ASC, created_at ASC LIMIT 1
    """
    cursor.execute(query, (task_type, task_type))
    row = cursor.fetchone()
    if row:
        cursor.execute("UPDATE tasks SET status = 'in_progress' WHERE id = ?", (row[0],))
        conn.commit()
        return f"Task {row[0]}: {row[1]} (type: {row[2]}, priority: {row[3]})"
    return "No pending tasks"

@mcp.tool()
async def mark_complete(task_id: int, result_summary: str) -> str:
    """Mark a task as complete with results."""
    conn = sqlite3.connect("tasks.db")
    cursor = conn.cursor()
    cursor.execute("""
        UPDATE tasks SET status = 'complete', result = ?, completed_at = ?
        WHERE id = ?
    """, (result_summary, datetime.utcnow().isoformat(), task_id))
    conn.commit()
    return f"Task {task_id} marked complete"
```

Configure in `.mcp.json`:
```json
{
  "mcpServers": {
    "task-queue": {
      "type": "stdio",
      "command": "python",
      "args": ["./mcp-servers/task_queue_mcp.py"]
    }
  }
}
```

For checkpointing long-running tasks, combine git commits with task state. Create a checkpoint before each major operation by committing a marker, enabling rollback if the operation fails:

```bash
# PreToolUse hook for checkpointing
git add -A && git commit -m "CHECKPOINT: Before $TOOL_NAME on $FILE_PATH" --allow-empty
```

---

## Safety guardrails through hooks and risk scoring

The safety layer uses hooks to intercept all tool calls, score risk, and enforce policies. This PreToolUse hook implements the essential controls:

```python
#!/usr/bin/env python3
# .claude/hooks/safety_gate.py
import sys
import json
import re
import os
from datetime import datetime

# Risk scoring configuration
RISK_SCORES = {
    "Read": 10, "Grep": 10, "Glob": 10,
    "Write": 50, "Edit": 50, "MultiEdit": 60,
    "Bash": 70, "mcp__*": 40,
}

DANGEROUS_PATTERNS = [
    r"rm\s+.*-[rf]",           # rm -rf
    r"sudo\s+",                # sudo anything
    r"chmod\s+777",            # dangerous permissions
    r">\s*/etc/",              # write to system dirs
    r"curl.*\|\s*bash",        # pipe to bash
    r"\.env|\.ssh|\.aws",      # sensitive files
]

BLOCKED_PATHS = [
    "~/.ssh", "~/.aws", "~/.config", "/etc", "/var",
    ".env", ".git/config", "node_modules"
]

def calculate_risk(tool_name: str, tool_input: dict) -> int:
    base_score = RISK_SCORES.get(tool_name, 30)
    
    # Check for dangerous command patterns
    if tool_name == "Bash":
        command = tool_input.get("command", "")
        for pattern in DANGEROUS_PATTERNS:
            if re.search(pattern, command, re.IGNORECASE):
                return 100  # Maximum risk
    
    # Check for blocked paths
    file_path = tool_input.get("file_path", "") or tool_input.get("path", "")
    for blocked in BLOCKED_PATHS:
        if blocked in file_path:
            return 95
    
    return base_score

def log_action(tool_name: str, tool_input: dict, risk_score: int, decision: str):
    log_entry = {
        "timestamp": datetime.utcnow().isoformat(),
        "tool": tool_name,
        "input_hash": hash(json.dumps(tool_input, sort_keys=True)),
        "risk_score": risk_score,
        "decision": decision,
        "session": os.environ.get("CLAUDE_SESSION_ID", "unknown")
    }
    with open(os.path.expanduser("~/.claude/audit.jsonl"), "a") as f:
        f.write(json.dumps(log_entry) + "\n")

if __name__ == "__main__":
    data = json.load(sys.stdin)
    tool_name = data.get("tool_name", "")
    tool_input = data.get("tool_input", {})
    
    risk_score = calculate_risk(tool_name, tool_input)
    
    # Risk tier decisions
    if risk_score >= 90:
        log_action(tool_name, tool_input, risk_score, "BLOCKED")
        print(f"BLOCKED: High-risk operation (score: {risk_score})", file=sys.stderr)
        sys.exit(2)  # Exit 2 blocks and feeds error to Claude
    
    if risk_score >= 70:
        log_action(tool_name, tool_input, risk_score, "FLAGGED")
        # In production, trigger human approval webhook here
        print(f"WARNING: Elevated risk (score: {risk_score}), proceeding with logging")
    
    log_action(tool_name, tool_input, risk_score, "ALLOWED")
    sys.exit(0)  # Allow execution
```

Configure hooks in `.claude/settings.json`:
```json
{
  "hooks": {
    "PreToolUse": [
      {
        "matcher": "*",
        "hooks": [{
          "type": "command",
          "command": "python3 $CLAUDE_PROJECT_DIR/.claude/hooks/safety_gate.py"
        }]
      }
    ],
    "PostToolUse": [
      {
        "matcher": "Edit|Write|MultiEdit",
        "hooks": [{
          "type": "command",
          "command": "git add -A && git commit -m 'Agent edit: $(date +%s)' --allow-empty"
        }]
      }
    ],
    "Stop": [
      {
        "hooks": [{
          "type": "command",
          "command": "echo 'Session complete' | tee -a ~/.claude/session.log"
        }]
      }
    ]
  }
}
```

---

## Escalation thresholds determine when agents stop and ask

The human-in-the-loop decision matrix should trigger escalation based on four factors:

**Confidence-based triggers**: When the agent expresses uncertainty ("I'm not sure if...", "This might break..."), the orchestrator should pause. Implement this through a PostToolUse hook that scans Claude's response for uncertainty markers.

**Risk-based triggers**: Operations scoring above **70** on the risk scale require explicit approval before execution. Implement through PreToolUse hooks that return exit code 2 with an approval request.

**Anomaly triggers**: When an operation deviates significantly from the established baseline—for example, a code-generation task suddenly requesting network access—escalate immediately.

**Policy triggers**: Certain operations should always require approval regardless of context: deleting user data, modifying authentication logic, external API calls to payment systems, or publishing/deployment actions.

The escalation workflow uses webhooks to external systems (Slack, email) combined with a polling mechanism:

```python
# Simplified escalation flow
async def request_approval(action: dict) -> bool:
    approval_id = create_approval_request(action)
    send_slack_notification(approval_id, action["summary"])
    
    # Poll for response (or use webhook callback)
    while True:
        status = check_approval_status(approval_id)
        if status == "approved":
            return True
        if status == "denied" or timeout_exceeded():
            return False
        await asyncio.sleep(30)
```

---

## Example workflows demonstrate the orchestration patterns

### Autonomous code review workflow

```
User: "Review PR #123 and fix any issues you find"

Orchestrator decomposes into tasks:
1. [Research Agent] Fetch PR diff from GitHub MCP
2. [Code Reviewer] Analyze for correctness, security, performance (parallel)
3. [Test Runner] Execute existing test suite
4. [Code Generator] Fix identified issues (sequential, depends on 2-3)
5. [Test Runner] Verify fixes don't break tests
6. [Orchestrator] Summarize changes and request human approval before merge
```

### Automated test execution with failure recovery

```
User: "Run the full test suite and fix any failures"

Orchestrator workflow:
1. Create git checkpoint (PreToolUse hook automatic)
2. [Test Runner] Execute: npm test -- --reporter=json
3. Parse results: If all pass → report success and exit
4. If failures:
   a. [Research Agent] Analyze failure patterns, group by root cause
   b. For each unique root cause:
      - [Code Generator] Implement fix with risk_score < 70
      - [Test Runner] Verify specific test passes
   c. [Test Runner] Full suite regression check
5. If regression: Rollback to checkpoint, escalate to human
6. If success: Commit with descriptive message
```

### Research and information gathering

```
User: "Research best practices for implementing rate limiting in our API"

Orchestrator workflow:
1. [Research Agent] Web search for rate limiting patterns (parallel queries):
   - "API rate limiting algorithms comparison"
   - "token bucket vs sliding window rate limiting"
   - "rate limiting Redis implementation"
2. [Research Agent] Analyze existing codebase for current patterns
3. [Code Reviewer] Review similar implementations in project dependencies
4. Synthesize findings into recommendation document
5. [Code Generator] Create implementation plan with code sketches
6. Store results in Memory MCP for future reference
```

---

## Memory persistence requires explicit infrastructure

Claude Code's built-in memory (`CLAUDE.md`, conversation history) persists within sessions but doesn't survive across separate CLI invocations in headless mode. For ATLAS to maintain long-term context, implement a Memory MCP server:

```python
# memory_mcp.py
from mcp.server.fastmcp import FastMCP
import sqlite3
import json

mcp = FastMCP("memory")

@mcp.tool()
async def store_memory(
    content: str,
    context_type: str,  # "decision", "learning", "task_result"
    tags: str = ""
) -> str:
    """Store information for long-term recall."""
    conn = sqlite3.connect("memory.db")
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO memories (content, context_type, tags, created_at)
        VALUES (?, ?, ?, datetime('now'))
    """, (content, context_type, tags))
    conn.commit()
    return f"Stored memory with tags: {tags}"

@mcp.tool()
async def recall_memories(query: str, limit: int = 5) -> str:
    """Retrieve relevant memories by keyword search."""
    conn = sqlite3.connect("memory.db")
    cursor = conn.cursor()
    cursor.execute("""
        SELECT content, context_type, tags, created_at FROM memories
        WHERE content LIKE ? OR tags LIKE ?
        ORDER BY created_at DESC LIMIT ?
    """, (f"%{query}%", f"%{query}%", limit))
    results = cursor.fetchall()
    return json.dumps([{
        "content": r[0], "type": r[1], "tags": r[2], "date": r[3]
    } for r in results])
```

For semantic search over memories, integrate a vector database like Chroma or use the `@zilliz/claude-context-mcp` package which provides **~40% token reduction** through hybrid BM25 + dense vector search.

---

## Practical implementation path for ATLAS

**Phase 1: Core infrastructure** (Week 1)
- Create `.claude/agents/` with specialized subagents for code, tests, research
- Implement PreToolUse safety hook with risk scoring and audit logging
- Set up PostToolUse hook for automatic git checkpointing
- Configure CLAUDE.md with project context and orchestration rules

**Phase 2: Task management** (Week 2)
- Build task queue MCP server with priority scheduling
- Implement memory MCP server for persistent context
- Create orchestration skill that teaches the main agent how to decompose and delegate

**Phase 3: Automation workflows** (Week 3)
- Configure CI/CD integration using `claude-code-action` for PR reviews
- Implement escalation webhooks to Slack/email
- Add voice pipeline integration if needed (Picovoice for local STT/TTS)

**Phase 4: Safety hardening** (Week 4)
- Implement sandbox configuration for filesystem/network isolation
- Add comprehensive audit logging with retention policies
- Create rollback procedures and test disaster recovery
- Define escalation matrix and approval workflows

The architecture leverages Claude Code's native primitives rather than reimplementing orchestration logic externally. The **critical success factor** is clear subagent specialization—each agent should have minimal tool permissions and a focused purpose. The orchestrator's role is decomposition and synthesis, while subagents handle isolated execution with their own context windows preventing cross-contamination of concerns.