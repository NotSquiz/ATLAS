# MidJourney V7's Reference System and Video Features: Complete Technical Guide

**The most critical finding for V7 users: --cref no longer works.** MidJourney V7 replaced Character Reference (--cref) with Omni Reference (--oref) when V7 launched April 3, 2025 and became default June 17, 2025. Neither --cref nor --oref work with video generation—a major limitation for creators seeking character consistency across animated content. Here's everything you need to know about the current state of these features as of February 2026.

## Omni Reference (--oref) replaces --cref entirely in V7

MidJourney's official documentation states explicitly: "Character Reference is not compatible with version 7, please use Omni Reference instead." This change fundamentally alters workflows for anyone who relied on --cref for character consistency.

**Key differences between the old and new systems:**

| Parameter | V6 (--cref) | V7 (--oref) |
|-----------|-------------|-------------|
| Weight control | --cw 0-100 | --ow 0-1000 |
| Multiple images | Yes | **Only ONE image allowed** |
| Default weight | 100 | 100 |
| GPU cost | Standard | **2x more GPU time** |
| Scope | Characters only | Characters, objects, vehicles, creatures |

The Omni Reference system handles camera angles far more intelligently than --cref did. V6 sometimes placed heads backward when the camera POV was behind a character; V7 understands spatial relationships much better. External images (including real photographs) now work equally well as MidJourney-generated images—a significant improvement over V6, where --cref worked best with MJ-generated content.

**Recommended --ow (Omni Weight) values from experienced users:**

- **--ow 25-50**: Style transfers like photo-to-anime; at these values, the system preserves core features while allowing significant stylistic change
- **--ow 100** (default): Balanced starting point for most use cases
- **--ow 100-250**: Optimal range according to testing by MidJourneyV6.org
- **--ow 300-400**: Strong fidelity for preserving facial features and clothing details
- **--ow 400-500**: TitanXT testing found this range often produced better similarity results

MidJourney's official guidance warns: "If you aren't using extremely high --s and --exp values, you probably should not go over values like --ow 400 or things may actually be worse."

## --sref behavior has evolved with a six-version system

V7 introduced a **six-version style reference system (--sv)** that fundamentally changes how style references work:

| Version | Behavior | Best use case |
|---------|----------|---------------|
| --sv 1 | Reliable enhancer, adds warmth, boosts contrast | Client work, consistent results |
| --sv 2 | Creative wild card, unpredictable transformations | Experimentation |
| --sv 3 | Artistic balancer, thoughtful modifications | Art theory-based enhancement |
| --sv 4 | Professional standard (pre-June 16, 2025 model) | Legacy sref codes |
| --sv 5 | Vintage specialist, warm tones, nostalgic film look | Retro branding |
| --sv 6 | **Default** - Safe, balanced, contemporary | General use |

**Critical note for sref codes:** Sref codes only work with --sv 4 or --sv 6. Uploading your own images unlocks all six versions.

**Optimal --sw (style weight) values for brand aesthetics:**

For maintaining brand consistency, experienced users recommend starting with **--sw 200-400**. Testing with permutations is essential:
```
[prompt] --sref [URL] --sw {0, 200, 400, 600, 800, 1000} --v 7
```

At **--sw 100** (default) you get balanced transfer; **--sw 500-700** produces very strong style influence; **--sw 800-1000** closely mimics the reference but differences can be subtle—always test first.

**Color palette swatches work as --sref, but with limitations.** Create a 600–1200px wide image with 4–6 horizontal, solid-color swatches. MidJourney interprets colors contextually based on scene lighting and surface materials rather than hard-locking exact HEX values. Rectilinear swatch designs may unintentionally influence architectural elements in generated images—use "flat color, minimal gradients" in your prompt and start with **--sw 180** for palettes.

Lighting and glow effects transfer moderately well through --sref. For ethereal glow, the community recommends `--sref 1007775450`, which creates minimalist style with glowing edges and dreamy atmosphere. Higher --sw values (300+) strengthen lighting effect preservation.

## Video generation does not support reference features

This is perhaps the most significant limitation: **neither --cref, --oref, nor --sref work with video generation**. Official documentation explicitly states these are "not compatible with video generations."

**The workaround for character-consistent video:**
1. Generate your still image with --oref first
2. Upscale (U1-U4)
3. Use "Animate (High motion)" or "Animate (Low motion)" buttons
4. This animates that specific consistent character image

**Video specifications and quality degradation:**

MidJourney's V1 Video Model (launched June 19, 2025) creates 5-second clips at 24fps, extendable to **21 seconds maximum** via 4-second extensions. Quality degrades progressively:

| Segment | Quality |
|---------|---------|
| 0-5 seconds | Best quality |
| 5-9 seconds | Good quality |
| 13-17 seconds | Noticeable degradation |
| 17-21 seconds | Lowest quality |

Plan critical content for the first 5 seconds and limit extensions to two for quality consistency.

**Best video prompt structure:**
Subject + Active Action Verb + Camera Movement + Environmental Details

Remove image-focused keywords like "8K," "HDR," or "f/1.8"—these are ineffective for video. Use natural descriptions and the two-part structure:
```
Camera: slow push-in, steady. Subject: hair and fabric move gently in the wind; subtle eye movement; natural --motion high --raw --video 1
```

**Motion and camera parameters:**
- `--motion low`: Subtle/calm motion for ambient scenes
- `--motion high`: Dynamic motion with both subject and camera movement
- `--raw`: Stricter prompt adherence
- `--loop`: Makes end frame match start frame
- `--end <URL>`: Specifies ending frame image

**Success rates by content type (from GamsGo testing):**
- Landscapes & still life: ~80-90%
- Portraits: ~66.7% (failures from facial distortion, drifting features)
- Human motion: ~50-60%
- Abstract/artistic: ~70-80%

Known bugs include motion stutters, subject warping with dramatic movements, camera drift on "static camera" requests, and text rendering that's often illegible.

## Maintaining a stylized 3D character across 20+ generations is achievable

The key is **always referencing your original base image** rather than chaining references (Image A → Image B → Image C degrades consistency).

**Recommended workflow:**

**Step 1 - Create excellent base character:**
```
A stylized 3D character, [detailed description], Pixar style, Cinema 4D render, clean white background, full body, front view --ar 2:3 --v 7
```
Use simple/solid colored clothing (patterns are hard to replicate), clean backgrounds, and portrait orientation (2:3 or 4:5).

**Step 2 - Generate character reference sheet:**
```
character reference sheet, [character description], multiple views, front view, side view, three-quarter view, various expressions, clean white background, professional --oref [URL] --ow 100 --v 7
```

**Step 3 - Apply across generations:**
```
3D stylized character, [ACTION/SCENE], [key character traits: hair color, eye color, outfit colors], Pixar render style, soft lighting --oref [original_base_URL] --ow 150 --sref [3D_style_reference_URL] --sw 100 --seed [consistent_seed] --v 7 --ar 16:9
```

**For clay/3D aesthetic specifically**, use these prompt terms:
- "Claymation" - whimsical, surreal with vivid colors
- "3D clay animation" - stop-motion look with tactile textures
- "Clay render" - clean 3D with clay-like materials

Example:
```
miniature, Super cute clay world, isometric view of garden, flowers, cute clay freeze frame animation, tilt shift, excellent lighting, volume, 3D, super detail --chaos 0 --style raw
```

## Seed discipline has limited relevance in V7

Official MidJourney documentation explicitly states seeds "shouldn't be relied on for the same results over different prompting sessions" and "can't capture or bookmark a specific style, character, or appearance across different prompts."

**Seeds remain useful for:**
- A/B testing prompt phrasing while keeping starting noise constant
- Creating thematically cohesive image sets
- Controlled experimentation with parameter differences

**Seeds are NOT recommended for:**
- True character consistency (use --oref instead)
- Style preservation (use --sref instead)
- Multi-session projects

Official guidance: "For consistency in your images, we recommend style references, omni references, and personalization."

## Common failure modes and their workarounds

**Character looks different than reference:**
Use higher quality reference images with clear, unobstructed faces. Always reference the original base image rather than chaining. Try rerolling and use the same seed for related generations.

**Character ignores text prompt (too similar to reference):**
Lower --ow to 50-100. This gives the text prompt more influence.

**Fine details not preserved:**
Intricate details like specific freckles, logos, and tattoos won't reproduce accurately. Reinforce key details in the text prompt rather than relying solely on the reference.

**V7 --oref incompatibilities:**
Not compatible with Fast Mode, Draft Mode, Conversational Mode, --q 4, Vary Region, Pan, Zoom Out, or inpainting/outpainting (which still uses V6.1).

**Multiple characters in scene:**
Since V7 --oref only allows one reference image, create both characters in a single reference image and describe both in your prompt. Alternatively, create the first character with --oref, send to Editor, switch to V6.1, and use --cref for the second character.

## Key sources and dates

| Source | URL | Date |
|--------|-----|------|
| MidJourney Official - Omni Reference | docs.midjourney.com/hc/en-us/articles/36285124473997-Omni-Reference | Current |
| MidJourney Official - Character Reference | docs.midjourney.com/hc/en-us/articles/32162917505293-Character-Reference | Current |
| MidJourney Official - Style Reference | docs.midjourney.com/hc/en-us/articles/32180011136653-Style-Reference | Current |
| MidJourney Official - Video | docs.midjourney.com/hc/en-us/articles/37460773864589-Video | Current |
| MidJourney Official - Seeds | docs.midjourney.com/hc/en-us/articles/32604356340877-Seeds | Current |
| MidJourney Updates - Omni Reference Launch | updates.midjourney.com/omni-reference-oref/ | May 1, 2025 |
| MidJourney Updates - V1 Video Model | updates.midjourney.com/introducing-our-v1-video-model/ | June 19, 2025 |
| Geeky Curiosity - Style Reference Testing | geekycuriosity.substack.com | August 8, 2025 |
| TitanXT - Omni Reference Testing | titanxt.io/post/testing-midjourney-v7s-new-omni-reference-feature | May 2025 |
| PromptsRef - Camera Prompts Guide | promptsref.com | November 27, 2025 |
| Tom's Guide - Character Consistency | tomsguide.com | 2025 |

## Conclusion

V7's reference system represents a fundamental architectural shift from V6. The replacement of --cref with --oref offers broader capability (objects, vehicles, creatures—not just characters) and better handling of external images, but at the cost of multi-image support and 2x GPU usage. The complete incompatibility of all reference features with video generation remains the most significant limitation for creators building character-driven content.

For 20+ generation consistency with stylized 3D characters, the proven approach combines --oref (100-250 weight) with --sref for style, always referencing your original base image rather than chaining references. Seeds provide compositional consistency for A/B testing but are not the primary consistency tool—reference features have definitively replaced that role in V7's architecture.