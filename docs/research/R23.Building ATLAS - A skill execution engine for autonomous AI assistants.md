# Building ATLAS: A skill execution engine for autonomous AI assistants

For a voice-based personal assistant with a **3-second response budget** on RAM-constrained hardware (4-6GB usable), the optimal architecture combines **Chain-of-Thought for simple queries**, **lightweight ReAct (1-2 loops) for tool-dependent tasks**, and **template-matching for common patterns**. Advanced methods like Tree of Thoughts, LATS, and ReAcTree are too slow for real-time voice but excel at background processing. The key insight from 2024-2025 research: separating planning from execution enables using different model sizes for each phase, dramatically improving speed while maintaining quality.

---

## Reasoning patterns: choosing the right tool for each task

Five reasoning patterns dominate current agent architectures, each with distinct tradeoffs between quality and latency. Chain-of-Thought (CoT) adds step-by-step reasoning to a single LLM call, completing in **50-500ms** with minimal memory overhead. ReAct interleaves thinking with tool calls in Thought→Action→Observation loops, typically requiring **500ms-2s** for 1-2 iterations but reducing hallucination rates from 14% to 6% compared to pure CoT.

Tree of Thoughts explores multiple reasoning paths via BFS/DFS search, achieving **74% success** on Game of 24 versus 4% for CoT—but requires 5-50+ LLM calls and **3-10+ seconds** minimum. ReAcTree builds hierarchical agent trees with control flow nodes (sequence, fallback, parallel), nearly doubling ReAct performance on complex tasks (61% vs 31% on WAH-NL) at the cost of **5-15+ seconds** latency. LATS (Language Agent Tree Search) combines Monte Carlo Tree Search with LLM agents, achieving state-of-the-art **92.7% pass@1** on HumanEval but requiring **10-60+ seconds** per task.

| Pattern | Latency | Quality | RAM Usage | Voice UI Fit |
|---------|---------|---------|-----------|--------------|
| **CoT** | 50-500ms | Good | Low | ✅ Excellent |
| **ReAct** | 500ms-2s | Better | Medium | ✅ Good (1-2 loops) |
| **ToT** | 3-10s+ | Highest | High | ⚠️ Background only |
| **ReAcTree** | 5-15s+ | Excellent | Very High | ❌ Not suitable |
| **LATS** | 10-60s+ | Best | Very High | ❌ Not suitable |

**For ATLAS, implement a tiered approach**: direct response or zero-shot CoT for simple queries under 500ms, few-shot CoT for moderate complexity under 2s, and lightweight ReAct with 1-2 action loops for retrieval tasks under 3s. Reserve ToT/LATS for background task execution where users can wait.

```python
class ATLASRouter:
    def route_query(self, query: str, complexity_score: float) -> str:
        if complexity_score < 0.3:
            return self.cot_response(query)  # ~500ms
        elif complexity_score < 0.6:
            return self.react_response(query, max_steps=2)  # ~1.5s
        else:
            task_id = self.background_execute(query)  # Acknowledge immediately
            return f"Working on that. I'll notify you when ready. [task:{task_id}]"
```

---

## Task decomposition: breaking down complex workflows

The workout preparation example—"prepare my morning workout based on Garmin recovery data and my sore shoulder"—illustrates optimal decomposition strategy. This task naturally breaks into **four parallel-capable subtasks**: (1) fetch Garmin recovery score, (2) retrieve injury context from user profile, (3) filter exercises based on both constraints, and (4) generate the final plan. Using a **DAG (directed acyclic graph)** structure enables tasks 1 and 2 to execute simultaneously, with task 3 waiting on both, achieving **3.6x speedup** versus sequential execution.

Static decomposition using predetermined templates handles common requests with zero planning latency. Dynamic decomposition via LLM adds **300-800ms** but handles novel requests. The hybrid approach matches templates first (confidence threshold >0.85), falling back to dynamic decomposition only when needed—saving ~500ms on 70-80% of typical assistant queries.

```python
# DAG-based parallel execution for the workout example
plan_dag = {
    "t1": {"tool": "garmin_api", "args": "recovery", "deps": []},
    "t2": {"tool": "user_profile", "args": "injuries", "deps": []},
    "t3": {"tool": "filter_exercises", "deps": ["t1", "t2"]},  # Waits for both
    "t4": {"tool": "generate_plan", "deps": ["t3"]}
}

async def execute_dag(dag):
    results = {}
    for level in topological_sort(dag):
        level_tasks = [execute(task, results) for task in level]
        level_results = await asyncio.gather(*level_tasks)
        results.update(zip([t.id for t in level], level_results))
    return results
```

**Decomposition depth** should stay at **2 levels maximum** with **5 subtasks per level** for voice assistants. Beyond this, latency exceeds the 3-second budget. The ADaPT pattern ("As-Needed Decomposition") from 2024 research only decomposes when the LLM cannot execute directly, reducing overhead for simpler tasks within complex requests.

---

## Planning strategies: when to plan upfront versus adapt

Plan-then-Execute creates a full plan upfront, then executes—ideal when tasks have clear, predictable steps and parallel execution opportunities. Research shows **3.6x speedup** via parallel task execution (LLMCompiler architecture). The key advantage: using a larger model (GPT-4 class) for planning and a smaller, faster model for execution dramatically reduces per-step latency.

Interleaved reasoning (ReAct pattern) adapts to observations in real-time, excelling when intermediate results significantly alter the approach or when exploring unfamiliar territory. However, it requires an LLM call for **every action**, making it slower and more expensive for multi-step tasks. The critical safeguard: hard limits on iterations (5 max) and execution time (2.5s for voice).

**Hybrid approaches work best for ATLAS**. The recommended pattern: high-level planning in one LLM call (~400ms), then low-level interleaved execution with per-step budgets (500ms each). If the plan fails, implement **localized replanning**—regenerate only the affected subtree rather than the entire task.

```python
class HybridPlanExecuteAgent:
    async def run(self, query: str) -> str:
        # Phase 1: Quick high-level plan (one LLM call, ~400ms)
        plan = await self.planner.plan(query, max_steps=5, timeout=0.5)
        
        # Phase 2: Execute with local adaptation
        results = []
        for step in plan.steps:
            result = await self.executor.execute_step(
                step, 
                context=results,
                max_local_iterations=2,  # Limited ReAct within each step
                timeout_per_step=0.5
            )
            if result.failed:
                # Localized replan - only regenerate affected portion
                remaining_plan = await self.replan_from_failure(
                    original_goal=query, completed=results, error=result.error
                )
                plan.steps = remaining_plan
            results.append(result)
        
        return self.synthesize(results)
```

---

## Observations and reflection: grounding agents in reality

A good observation in ReAct-style agents is **concise, actionable, and includes success/failure status**. Observations should contain only task-relevant information—raw API responses with 500 fields must be compressed to essential facts. For voice assistants, keep observations under **100 tokens** to preserve response time budget.

```python
# Good observation format
"[SUCCESS] Recovery score: 72/100 (moderate fatigue). Sleep: 6.2hrs. HRV: 45ms."

# Bad observation (too verbose, wastes context)
"{'user_id': 12345, 'created_at': '2023-01-15T10:30:22Z', ...500 more fields...}"
```

**Reflection frequency** must balance insight against latency. Research on Reflexion shows reflecting after every step adds 2x latency—too expensive for voice. The optimal pattern for ATLAS: **reflect on failure only**. When an action fails or the agent detects a loop (same action repeated 2+ times), trigger reflection to generate an alternative approach. Store reflections in episodic memory, limited to the **last 3 reflections** per Reflexion paper findings.

Critical insight from 2024-2025 research: **intrinsic self-correction without external feedback is unreliable** and often degrades performance. Effective self-correction requires external signals—tool execution results, API responses, or test failures. Without grounded feedback, LLMs tend to overcorrect or hallucinate corrections.

```python
class PlanFailureDetector:
    def check_failure(self, action, observation, step_num):
        # Loop detection: same action twice with similar inputs
        action_sig = f"{action.tool}:{hash(str(action.input))}"
        if self.action_history[-3:].count(action_sig) >= 2:
            return FailureType.LOOP_DETECTED
        
        # Stagnation: no new information in 3 steps
        obs_hash = hash(observation)
        if obs_hash in self.observation_hashes:
            self.stagnation_count += 1
            if self.stagnation_count >= 3:
                return FailureType.STAGNATION
        
        # Budget exhaustion
        if step_num >= self.max_steps:
            return FailureType.BUDGET_EXCEEDED
        
        return None
```

---

## Memory architecture for RAM-constrained systems

The MemGPT architecture provides the foundation for persistent memory within context window limits. It divides memory into **in-context** (always loaded: system prompt, core memory blocks, recent messages) and **external storage** (disk-backed: archival memory, conversation history). The LLM manages its own memory via function calls, creating a "virtual context" that exceeds physical context window limits.

For ATLAS with ~4000 token context budget, allocate: system prompt (**400 tokens, 10%**), core memory blocks (**800 tokens, 20%**), message buffer (**1600 tokens, 40%**), working task state (**800 tokens, 20%**), and retrieved context (**400 tokens, 10%**).

**Memory prioritization** determines what survives eviction. Critical items (user identity, preferences) persist permanently. High-priority items (task requirements, constraints) survive until task completion. Medium-priority items (intermediate results) persist within the session. Low-priority items (verbose tool outputs) face immediate eviction.

```python
class ATLASMemoryManager:
    def __init__(self):
        # In-memory (fast access, ~2KB total)
        self.system_prompt = load_system_prompt()      # 400 tokens
        self.core_memory = CoreMemoryBlocks()           # 800 tokens
        self.message_buffer = MessageBuffer(max=10)     # 1600 tokens
        self.working_memory = WorkingMemory()           # 800 tokens
        
        # Disk-backed (slower, unlimited)
        self.episodic_db = SQLiteStore("episodic.db")
        self.semantic_db = SQLiteStore("semantic.db")
        self.vector_index = FaissIndexIVF(dim=384)     # MiniLM embeddings
```

**Summarization strategies** prevent context explosion during long workflows. Observation masking—replacing old observations with placeholders like "[Observation from step 3: details omitted]"—is often as effective as LLM summarization per JetBrains research, while adding zero latency. For voice conversation context, maintain only **3-5 recent turns** with incremental summarization of older exchanges.

The distinction between **episodic memory** (specific timestamped events) and **semantic memory** (extracted general facts) matters for retrieval. Query semantic memory directly for user preferences ("prefers window seats"). Query episodic memory via recency and relevance for past interactions ("what did user ask about Paris?").

---

## Production implementation: ReAct in the real world

The ReAct loop structure in production uses three components: a **model node** (generates thoughts and tool calls), a **tools node** (executes actions), and **state management** (maintains trajectory). Use structured outputs via Pydantic models rather than text parsing for reliable action extraction.

```python
from pydantic import BaseModel, Field
from typing import Optional

class AgentAction(BaseModel):
    thought: str = Field(description="Reasoning about current state")
    action: Optional[str] = Field(description="Tool name or 'final_answer'")
    action_input: Optional[dict] = Field(description="Tool parameters")
    final_answer: Optional[str] = Field(description="Final response if done")

# Use tool calling (preferred over text parsing)
response = llm.bind_tools(tools, tool_choice="auto").invoke(messages)
```

**Tool failure handling** requires a circuit breaker pattern: after 5 consecutive failures to a service, stop calling it for 60 seconds to prevent cascade failures. Implement exponential backoff with jitter for retries (1s, 2s, 4s delays). Create fallback chains: primary API → backup API → cached result → graceful acknowledgment ("I'm having trouble accessing that information").

**Meeting the 3-second budget** requires streaming everywhere. Start TTS output on first LLM tokens—don't wait for complete response. Provide immediate acknowledgment ("Let me check...") within **500ms** while processing continues. Use semantic caching to avoid LLM calls for similar queries, reducing calls by **30-40%** for typical assistants.

```python
async def stream_with_acknowledgment(query, websocket):
    # Immediate acknowledgment (within 500ms)
    await tts.speak("Let me check...")
    
    # Stream agent processing
    async for event in agent.astream_events({"messages": [query]}):
        if event["event"] == "on_tool_start":
            await websocket.send({"status": f"Checking {event['name']}..."})
        elif event["event"] == "on_llm_stream":
            await tts.stream_chunk(event["data"]["chunk"])
```

**Logging and debugging** must capture: iteration count, thought content (truncated to 500 chars), tool calls with arguments and duration, success/failure status, and token usage per step. LangSmith provides production-standard observability with waterfall views for identifying bottlenecks.

---

## Voice interface patterns for natural interaction

Handling interruptions requires classifying user speech during processing: **cancel** ("Stop", "Never mind"), **redirect** (new question), **clarify** ("Wait, I meant..."), or **acknowledge** (backchannels like "uh-huh" that should be ignored). On cancel, immediately stop TTS and task execution. On redirect, pause the current task, preserve context, and handle the new query with that context available.

```python
class InterruptionHandler:
    async def on_user_speech(self, transcript, current_task):
        intent = self.classify_interruption(transcript)
        
        if intent == InterruptionType.CANCEL:
            await current_task.cancel()
            await tts.stop()
            await tts.speak("Okay, cancelled.")
        
        elif intent == InterruptionType.REDIRECT:
            await current_task.pause()
            await tts.stop()
            await self.handle_query(transcript, context=current_task.state)
```

**Progressive disclosure** structures responses for natural voice delivery: immediate acknowledgment, first substantive piece, then details on request. For long tasks exceeding 3 seconds, acknowledge and process in background: "This is a bigger task. I'll work on it and let you know when it's ready."

Error communication must translate technical failures to natural speech. "Tool timeout" becomes "I'm having trouble accessing that information right now. Let me try another way." Never expose raw error messages or technical jargon to voice users.

---

## Key papers informing this architecture

The foundational work includes **ReAct** (ICLR 2023), which introduced interleaved reasoning and acting, achieving +34% success on ALFWorld over baselines. **Tree of Thoughts** (NeurIPS 2023) demonstrated 74% success on Game of 24 versus 4% for CoT through deliberate exploration. **LATS** (ICML 2024) unified reasoning, acting, and planning via Monte Carlo Tree Search, reaching **92.7% pass@1** on HumanEval.

For memory architectures, **MemGPT** (2023) established the OS-inspired hierarchical memory pattern now implemented in the Letta framework. **Mem0** (2025) introduced graph-based memory with 91% lower latency and 90%+ token savings versus full-context approaches. **Reflexion** (NeurIPS 2023) pioneered verbal self-correction, achieving 91% pass@1 on HumanEval through iterative reflection.

Recent hierarchical work includes **EPO** (EMNLP 2024) for environment-grounded task decomposition and **Meta-Task Planning** for DAG-based collaborative agents (+42% success on TravelPlanner). The **Self-Refine** paper (NeurIPS 2023) showed ~20% improvement across diverse tasks using a simple generate→feedback→refine loop.

---

## Conclusion: recommended architecture for ATLAS

The optimal ATLAS implementation layers three approaches: **template matching** for common patterns (zero planning latency), **CoT** for reasoning-heavy single-turn tasks (~500ms), and **lightweight ReAct** with 1-2 loops for tool-dependent queries (~1.5s). Complex tasks exceeding the 3-second budget should acknowledge immediately and execute in background with status updates.

Key implementation priorities:
- **Use DAG-based parallel execution** for multi-tool tasks (3.6x speedup)
- **Implement MemGPT-style tiered memory** with aggressive summarization
- **Cache semantically similar queries** to reduce LLM calls by 30-40%
- **Stream TTS from first tokens** to minimize perceived latency
- **Reflect on failure only**, not every step, using external feedback signals
- **Hard-limit ReAct to 5 iterations and 2.5s** to preserve response budget

The research consensus: sophisticated patterns (ToT, LATS, ReAcTree) generate excellent results but cannot meet real-time voice constraints. Their value lies in background processing, generating training data to improve single-shot models, and handling complex tasks where users explicitly accept longer wait times.