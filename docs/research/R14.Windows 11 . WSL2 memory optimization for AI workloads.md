# Windows 11 + WSL2 memory optimization for AI workloads

Running local AI inference on a **16GB Windows 11 system is achievable but requires careful optimization**. With the configuration detailed below, you can reliably allocate **5-6GB to WSL2** while maintaining a responsive Windows environment. The biggest immediate wins come from fixing the active DoSvc memory leak (currently affecting 24H2) and enabling WSL2's `autoMemoryReclaim` feature—together these can recover 1-4GB of wasted memory.

## The .wslconfig file: current syntax and critical settings

The `.wslconfig` file (located at `%UserProfile%\.wslconfig`) controls WSL2's virtual machine settings. It doesn't exist by default—you must create it manually. After any changes, run `wsl --shutdown` to apply them.

**Complete working configuration for 16GB RAM / 6GB WSL2:**

```ini
# .wslconfig - Optimized for 16GB RAM, ~6GB for WSL2 AI workloads
# Location: C:\Users\<YourName>\.wslconfig

[wsl2]
memory=6GB
processors=6
swap=4GB
pageReporting=true
localhostForwarding=true
# guiApplications=false  # Uncomment to disable WSLg and save resources

[experimental]
autoMemoryReclaim=dropcache
sparseVhd=true
```

The **autoMemoryReclaim** setting is the most important for memory-constrained systems. Introduced in WSL 2.0.0 (September 2023) and made default in WSL 2.1.3 (February 2024), it accepts three values: `disabled`, `gradual`, and `dropcache`. Use `dropcache` rather than `gradual`—the gradual mode causes documented hangs with Docker Desktop's Resource Saver mode (GitHub issue #11066), systemd (issue #10675), and WSLg GUI apps (issue #13542).

The **sparseVhd** option enables automatic VHD shrinking when files are deleted inside WSL2. Enable it for new installations, and for existing distros also run `wsl --manage <DistroName> --set-sparse true`. Note that some users report high CPU usage during container builds when this is enabled.

**Docker Desktop interaction**: Docker Desktop creates its own WSL2 distros (`docker-desktop` and `docker-desktop-data`) that share the memory pool defined in `.wslconfig`. The memory limit applies to the entire WSL2 VM—Docker cannot exceed it. Docker's "Resource Saver" mode reduces CPU on WSL but doesn't reduce memory; Microsoft recommends using `autoMemoryReclaim` instead.

## Memory reclamation now works—with the right settings

WSL2 historically held onto memory aggressively due to Linux's page cache behavior. As of 2024-2025, **memory reclamation works reliably** when properly configured:

| Setting | What it does | When memory is reclaimed |
|---------|-------------|-------------------------|
| `pageReporting=true` | Lets Windows reclaim unused WSL memory | Automatic for freed process memory |
| `autoMemoryReclaim=dropcache` | Actively drops Linux page cache | After ~10 minutes idle |
| `autoMemoryReclaim=gradual` | Slowly reclaims over 30 minutes | Every minute after 5-min idle (has bugs) |

**Manual reclamation commands** for when you need memory immediately:

```bash
# Inside WSL - drop page cache (run before heavy AI workloads)
sudo sh -c 'echo 1 > /proc/sys/vm/drop_caches'

# More aggressive - includes dentries/inodes
sudo sh -c 'echo 3 > /proc/sys/vm/drop_caches'

# Check current memory usage
free -h
```

```powershell
# Windows side - nuclear option to release all WSL memory
wsl --shutdown
```

**Monitoring tools**: From Windows, watch the `Vmmem` or `VmmemWSL` process in Task Manager. Inside WSL, use `free -h`, `htop`, or `cat /proc/meminfo`. The WSL process in Task Manager represents all WSL2 memory—it's not additional overhead.

## Active memory leak in DoSvc demands immediate attention

A critical finding: **Delivery Optimization Service (DoSvc) has an active memory leak** in Windows 11 24H2 as of December 2025. Microsoft's KB5072033 update changed AppXSVC to "Automatic" startup, exposing DoSvc to a leak that grows from **200MB to 1-20GB over hours**. Multiple sources confirm this issue affects systems with 8-16GB RAM severely.

**Fix DoSvc immediately:**
```powershell
# Disable via PowerShell (recommended)
Stop-Service -Force -Name "DoSvc"
Set-Service -Name "DoSvc" -StartupType Disabled

# Or via Settings: Windows Update → Advanced options → 
# Delivery Optimization → Toggle OFF
```

Disabling DoSvc means slightly slower Windows Update downloads but is completely safe.

**Other services worth disabling** for memory optimization:

| Service | RAM saved | Safe if... |
|---------|-----------|-----------|
| Windows Search (WSearch) | 50-200 MB | Using Everything search instead |
| Widgets | 130-300 MB | Uninstalled via `winget uninstall "Windows web experience Pack"` |
| SysMain (Superfetch) | Variable | Using SSD (benefits minimal on fast storage) |
| Xbox services | 20-50 MB | Not gaming |
| Print Spooler | 5-20 MB | No printer connected |

The **NDU registry fix** remains relevant if you have Killer Network or MSI network adapters. Set `HKEY_LOCAL_MACHINE\SYSTEM\ControlSet001\Services\Ndu\Start` to `4` to disable, or run `sc config NDU start= disabled`. This prevents non-paged pool memory from growing to 4GB+ over time.

## GPU passthrough and CUDA work excellently in WSL2

CUDA support in WSL2 is **production-ready for inference workloads** as of 2025. The architecture uses GPU-PV (partitioning), not PCIe passthrough—the Windows driver is automatically exposed to WSL2 as `libcuda.so`.

**Critical installation rule**: Install the NVIDIA driver on Windows only. Inside WSL2, install only the CUDA Toolkit (not the driver):

```bash
# WSL2 - Install CUDA toolkit only (NOT cuda or cuda-drivers packages)
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin
sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-6
```

**VRAM is completely separate from WSL2 memory**. The `.wslconfig` memory setting controls system RAM only—your GPU's VRAM is accessed directly and doesn't count against the limit. However, full Unified Memory (managed memory) is not supported in WSL2, which may affect some workloads.

| Framework | Status | Notes |
|-----------|--------|-------|
| PyTorch CUDA | ✅ Works | Use matching CUDA wheel (cu121/cu124) |
| ONNX Runtime GPU | ✅ Works | CUDA EP and TensorRT EP both functional |
| TensorFlow | ✅ Works | TF 2.16+ with CUDA 12.3 confirmed |

**Performance**: WSL2 achieves **85-95% of native Linux GPU performance** for sustained workloads. NVIDIA's 2024 optimizations brought Blender CUDA benchmarks within 1% of native. Small kernel launches have higher latency, but pipelined AI inference masks this overhead.

**Verification commands:**
```bash
# Add nvidia-smi to PATH
export PATH=/usr/lib/wsl/lib:$PATH
nvidia-smi

# Test PyTorch
python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python3 -c "import torch; print(torch.cuda.get_device_name(0))"
```

## Audio works out-of-box via WSLg—use PulseAudio, not PipeWire

WSL2 on Windows 11 includes **native audio support through WSLg** with zero configuration required. WSLg runs a PulseAudio server that connects to Windows audio via RDP transport, providing both output (RDPSink) and microphone input (RDPSource).

**Do not install PipeWire** in your WSL2 distro—it conflicts with WSLg's built-in PulseAudio server. Ubuntu 23.04+ installs PipeWire by default, which can break WSLg audio. If you're on a newer Ubuntu, disable PipeWire:

```bash
systemctl --user stop pipewire pipewire-pulse
systemctl --user disable pipewire pipewire-pulse
systemctl --user mask pipewire pipewire-pulse
```

**Microphone setup** requires enabling Windows permissions: Settings → Privacy & Security → Microphone → Enable "Let desktop apps access your microphone."

**Latency consideration**: WSLg audio adds **50-200ms overhead** compared to native Windows applications due to RDP encoding. For voice assistant development this is acceptable, but for production deployment with strict latency requirements, consider running audio capture natively on Windows and passing data to WSL2.

**Test audio:**
```bash
# Install utilities
sudo apt install pulseaudio-utils alsa-utils libasound2-plugins

# Test output
speaker-test -c 2 -t wav -l 1

# Test microphone (record 3 seconds, play back)
arecord -d 3 test.wav && aplay test.wav
```

## Realistic memory budget: expect 4-6GB for AI workloads

On a **16GB system running Windows 11 with optimization**, here's what to actually expect:

| Component | Minimal use | Typical use |
|-----------|-------------|-------------|
| Windows 11 + services (optimized) | 3-3.5 GB | 4-5 GB |
| Browser (5-10 tabs) | 1.5-2 GB | 2-4 GB |
| WSL2 kernel overhead | ~300 MB | ~300 MB |
| **Available for AI containers** | **5-6 GB** | **4-5 GB** |

With aggressive optimization (DoSvc disabled, Widgets removed, minimal browser use), you can achieve **6-7GB available** for WSL2. The swap file provides additional headroom—configure **4-8GB swap on your SSD** to handle memory spikes during model loading.

**What AI workloads fit in this budget:**
- 7B parameter quantized models (4-bit GGUF): ~4GB RAM ✅
- 13B parameter quantized models: ~7GB needed—requires swap, marginal ⚠️
- Whisper medium: ~2GB ✅
- Multiple simultaneous models: Limited—run one at a time

## Docker Desktop versus Podman: memory matters here

Docker Desktop consumes **2-3GB immediately at startup** and may not release memory even after containers stop. For memory-constrained systems, **Podman uses 65% less memory** due to its daemonless architecture.

| Metric | Docker Desktop | Podman |
|--------|---------------|--------|
| Idle memory | 2-3 GB | ~50 MB |
| Per-container overhead | ~45 MB + daemon | ~45 MB |
| Setup complexity | Easy | Moderate |
| Compose compatibility | Native | 95% via podman-compose |

**If staying with Docker Desktop**: Enable Resource Saver mode (Settings → Resources), set idle timeout to 5 minutes, and ensure `.wslconfig` has `autoMemoryReclaim=gradual`. Memory will drop near zero when Docker is idle for 5+ minutes.

**If switching to Podman**: You gain 1-2GB of usable memory. Install Podman Desktop or run `podman machine init --memory 6144` to create a machine with 6GB allocation.

## Complete optimization checklist with PowerShell commands

**Step 1: Create optimized .wslconfig**
```powershell
notepad "$env:USERPROFILE\.wslconfig"
# Paste the configuration from earlier, save, then:
wsl --shutdown
```

**Step 2: Disable memory-leaking services**
```powershell
# Disable DoSvc (active memory leak)
Stop-Service -Force -Name "DoSvc" -ErrorAction SilentlyContinue
Set-Service -Name "DoSvc" -StartupType Disabled

# Disable Windows Search (use Everything instead)
Stop-Service -Force -Name "WSearch" -ErrorAction SilentlyContinue
Set-Service -Name "WSearch" -StartupType Disabled

# Optional: Disable SysMain on SSD systems
Stop-Service -Force -Name "SysMain" -ErrorAction SilentlyContinue
Set-Service -Name "SysMain" -StartupType Disabled
```

**Step 3: Remove Widgets**
```powershell
winget uninstall "Windows web experience Pack"
```

**Step 4: Apply NDU fix (if using Killer/MSI network)**
```cmd
sc config NDU start= disabled
```

**Step 5: Monitor memory**
```powershell
# Check top memory consumers
Get-Process | Sort-Object WorkingSet64 -Descending | 
Select-Object -First 15 ProcessName, @{N='Memory(MB)';E={[math]::Round($_.WorkingSet64/1MB,2)}}

# Check non-paged pool (for NDU issues)
Get-Counter '\Memory\Pool Nonpaged Bytes' | 
ForEach-Object { "$([math]::Round($_.CounterSamples.CookedValue/1MB, 2)) MB" }
```

**Step 6: Verify WSL2 configuration**
```bash
# Inside WSL - verify memory limit applied
free -h  # Should show ~6GB total

# Verify CPU count
nproc  # Should show 6

# Verify CUDA
nvidia-smi
python3 -c "import torch; print(torch.cuda.is_available())"
```

## Conclusion

The combination of `.wslconfig` memory limits, `autoMemoryReclaim=dropcache`, and Windows service optimization creates a viable environment for local AI inference on 16GB systems. **Fix the DoSvc memory leak first**—it's actively affecting Windows 11 24H2 and can waste gigabytes. Use `dropcache` over `gradual` for memory reclamation stability with Docker. Consider Podman if every megabyte matters. GPU/CUDA works at near-native performance with VRAM completely separate from WSL2's memory budget. For your ATLAS AI assistant targeting 6GB, this configuration should provide reliable headroom with 4-5GB consistently available and swap handling occasional spikes.