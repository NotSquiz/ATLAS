# Implementing knowledge graphs with YAML/JSON files

File-based knowledge graphs work exceptionally well at your scale—**146+ nodes with 150+ relationships consume only ~50-200KB in memory** using NetworkX, making full in-memory loading the practical default. This guide provides production-ready patterns for graph representation, querying, hybrid semantic search, and future database migration.

## NetworkX handles your scale effortlessly

Your knowledge graph size falls well within comfortable territory for pure Python solutions. NetworkX uses approximately **100 bytes per edge**, meaning a 500-node graph with 1,000 edges needs only 100-200KB RAM. Load times from YAML run 50-200ms for graphs this size, dropping to under 10ms when using pickle caching. The critical insight: skip lazy loading complexity initially—full graph loading is both simpler and performant at this scale.

For algorithm-heavy workloads, **igraph runs 10-50x faster** than NetworkX on operations like shortest paths and centrality calculations, using roughly half the memory (183 bytes per vertex measured on large graphs). The tradeoff is slightly more complex attribute handling.

## Recommended file structure for multi-domain graphs

Organize your knowledge base around a manifest-driven multi-file structure that maps cleanly to future database imports:

```
knowledge_graph/
├── manifest.yaml              # Master index of all files
├── ontology/
│   ├── entity_types.yaml      # Node type definitions
│   └── evidence_tiers.yaml    # Confidence level schema
├── domains/
│   ├── montessori/
│   │   ├── nodes/
│   │   │   ├── activities.yaml
│   │   │   └── materials.yaml
│   │   └── relationships/
│   │       ├── contains.yaml
│   │       └── requires.yaml
│   └── health_fitness/
│       ├── nodes/
│       │   ├── exercises.yaml
│       │   ├── protocols.yaml
│       │   └── metrics.yaml
│       └── relationships/
│           ├── targets.yaml
│           ├── improves.yaml
│           └── correlates_with.yaml
├── mappings/
│   └── garmin_activity_mapping.yaml
└── cache/
    ├── graph.pickle           # Fast reload cache
    └── embeddings.npy         # Cached vector embeddings
```

The manifest file coordinates loading across domains:

```yaml
# manifest.yaml
name: unified_knowledge_graph
version: "1.0"
domains:
  montessori:
    node_files:
      - domains/montessori/nodes/activities.yaml
      - domains/montessori/nodes/materials.yaml
    relationship_files:
      - domains/montessori/relationships/contains.yaml
  health_fitness:
    node_files:
      - domains/health_fitness/nodes/exercises.yaml
      - domains/health_fitness/nodes/protocols.yaml
    relationship_files:
      - domains/health_fitness/relationships/targets.yaml
      - domains/health_fitness/relationships/improves.yaml
```

## Node and edge file formats following Neo4j conventions

Structure nodes with an `_label` field for database compatibility. Use **PascalCase for labels**, **camelCase for properties**, and **SCREAMING_SNAKE_CASE for relationship types**:

```yaml
# nodes/exercises.yaml
nodes:
  ex_barbell_squat_v1:
    _label: Exercise
    name: "Barbell Back Squat"
    category: "strength"
    difficulty: "intermediate"
    equipment_required: ["barbell", "squat_rack"]
    evidence:
      tier: 1
      sources: ["https://pubmed.ncbi.nlm.nih.gov/..."]

  ex_morning_walk_v1:
    _label: Exercise
    name: "Morning Sunlight Walk"
    category: "circadian"
    duration_minutes: 10
    timing: "within_30min_wake"
```

```yaml
# relationships/targets.yaml
relationships:
  - source: ex_barbell_squat_v1
    target: muscle_gluteus_maximus
    type: TARGETS
    properties:
      activation_level: "primary"
      evidence_tier: 1

  - source: ex_barbell_squat_v1
    target: muscle_quadriceps
    type: TARGETS
    properties:
      activation_level: "primary"
      evidence_tier: 1
```

## Core Python class for graph operations

This unified class handles loading, querying, caching, and export:

```python
import yaml
import json
import pickle
import networkx as nx
from pathlib import Path
from typing import Dict, List, Optional, Any, Callable
from collections import defaultdict, deque
from functools import lru_cache

class KnowledgeGraph:
    """File-based knowledge graph with NetworkX backend."""
    
    def __init__(self, manifest_path: str = None):
        self.graph = nx.DiGraph()
        self.manifest_path = manifest_path
        self._index = None
        
    def load_from_manifest(self, manifest_path: str = None) -> 'KnowledgeGraph':
        """Load graph from manifest file."""
        path = Path(manifest_path or self.manifest_path)
        with open(path) as f:
            manifest = yaml.safe_load(f)
        
        base_dir = path.parent
        
        for domain_name, domain_config in manifest.get('domains', {}).items():
            # Load nodes
            for node_file in domain_config.get('node_files', []):
                self._load_nodes(base_dir / node_file, domain_name)
            
            # Load relationships
            for rel_file in domain_config.get('relationship_files', []):
                self._load_relationships(base_dir / rel_file)
        
        return self
    
    def _load_nodes(self, filepath: Path, domain: str):
        with open(filepath) as f:
            data = yaml.safe_load(f)
        
        for node_id, attrs in data.get('nodes', {}).items():
            label = attrs.pop('_label', 'Node')
            self.graph.add_node(
                node_id,
                _label=label,
                _domain=domain,
                **attrs
            )
    
    def _load_relationships(self, filepath: Path):
        with open(filepath) as f:
            data = yaml.safe_load(f)
        
        for rel in data.get('relationships', []):
            self.graph.add_edge(
                rel['source'],
                rel['target'],
                _type=rel.get('type', 'RELATED_TO'),
                **rel.get('properties', {})
            )
    
    # === Caching for fast reload ===
    
    def save_pickle(self, path: str):
        """Save graph as pickle for 10x faster reload."""
        with open(path, 'wb') as f:
            pickle.dump(self.graph, f)
    
    def load_pickle(self, path: str) -> 'KnowledgeGraph':
        """Load from pickle cache."""
        with open(path, 'rb') as f:
            self.graph = pickle.load(f)
        return self
    
    # === Query Interface ===
    
    def query(self) -> 'GraphQuery':
        """Return fluent query builder."""
        return GraphQuery(self.graph)
    
    def get_node(self, node_id: str) -> Optional[Dict]:
        """Get node with all attributes."""
        if node_id in self.graph:
            return {'id': node_id, **self.graph.nodes[node_id]}
        return None
    
    def get_neighbors(self, node_id: str, 
                      direction: str = 'out',
                      rel_type: str = None) -> List[str]:
        """Get neighboring nodes, optionally filtered by relationship type."""
        if direction == 'out':
            edges = self.graph.out_edges(node_id, data=True)
            neighbors = [(t, d) for _, t, d in edges]
        elif direction == 'in':
            edges = self.graph.in_edges(node_id, data=True)
            neighbors = [(s, d) for s, _, d in edges]
        else:  # both
            neighbors = list(self.get_neighbors(node_id, 'out', rel_type))
            neighbors.extend(self.get_neighbors(node_id, 'in', rel_type))
            return [n for n, _ in neighbors]
        
        if rel_type:
            neighbors = [(n, d) for n, d in neighbors if d.get('_type') == rel_type]
        
        return [n for n, _ in neighbors]
    
    # === Traversal ===
    
    def bfs(self, start: str, depth_limit: int = None,
            node_filter: Callable = None,
            edge_filter: Callable = None) -> List[tuple]:
        """BFS traversal with filtering."""
        visited = {start}
        queue = deque([(start, 0)])
        results = []
        
        while queue:
            node, depth = queue.popleft()
            
            if node_filter and node != start:
                if not node_filter(node, self.graph.nodes.get(node, {})):
                    continue
            
            results.append((node, depth))
            
            if depth_limit is not None and depth >= depth_limit:
                continue
            
            for neighbor in self.graph.successors(node):
                if neighbor in visited:
                    continue
                
                if edge_filter:
                    edge_data = self.graph.edges[node, neighbor]
                    if not edge_filter(node, neighbor, edge_data):
                        continue
                
                visited.add(neighbor)
                queue.append((neighbor, depth + 1))
        
        return results
    
    def find_paths(self, source: str, target: str, 
                   max_length: int = 5) -> List[List[str]]:
        """Find all simple paths between nodes."""
        return list(nx.all_simple_paths(
            self.graph, source, target, cutoff=max_length
        ))
    
    def shortest_path(self, source: str, target: str) -> Optional[List[str]]:
        """Find shortest path."""
        try:
            return nx.shortest_path(self.graph, source, target)
        except nx.NetworkXNoPath:
            return None
    
    # === Aggregation ===
    
    def count_by_label(self) -> Dict[str, int]:
        """Count nodes by label."""
        counts = defaultdict(int)
        for _, data in self.graph.nodes(data=True):
            counts[data.get('_label', 'Unknown')] += 1
        return dict(counts)
    
    def count_by_relationship(self) -> Dict[str, int]:
        """Count edges by relationship type."""
        counts = defaultdict(int)
        for _, _, data in self.graph.edges(data=True):
            counts[data.get('_type', 'UNKNOWN')] += 1
        return dict(counts)
    
    # === Export for Database Migration ===
    
    def export_neo4j_csv(self, output_dir: str):
        """Export to Neo4j CSV format."""
        output = Path(output_dir)
        output.mkdir(parents=True, exist_ok=True)
        
        # Group nodes by label
        nodes_by_label = defaultdict(list)
        for node_id, data in self.graph.nodes(data=True):
            label = data.get('_label', 'Node')
            nodes_by_label[label].append((node_id, data))
        
        # Write node CSVs
        for label, nodes in nodes_by_label.items():
            self._write_node_csv(output / f"{label.lower()}.csv", nodes, label)
        
        # Group relationships by type
        rels_by_type = defaultdict(list)
        for source, target, data in self.graph.edges(data=True):
            rel_type = data.get('_type', 'RELATED_TO')
            rels_by_type[rel_type].append((source, target, data))
        
        # Write relationship CSVs
        for rel_type, rels in rels_by_type.items():
            self._write_rel_csv(output / f"{rel_type.lower()}.csv", rels, rel_type)
    
    def _write_node_csv(self, path: Path, nodes: list, label: str):
        import csv
        # Collect all property keys
        all_keys = set()
        for _, data in nodes:
            all_keys.update(k for k in data.keys() if not k.startswith('_'))
        
        with open(path, 'w', newline='') as f:
            writer = csv.writer(f)
            headers = ['nodeId:ID'] + list(all_keys) + [':LABEL']
            writer.writerow(headers)
            
            for node_id, data in nodes:
                row = [node_id]
                for key in all_keys:
                    val = data.get(key, '')
                    if isinstance(val, list):
                        val = ';'.join(str(v) for v in val)
                    row.append(val)
                row.append(label)
                writer.writerow(row)
    
    def _write_rel_csv(self, path: Path, rels: list, rel_type: str):
        import csv
        all_keys = set()
        for _, _, data in rels:
            all_keys.update(k for k in data.keys() if not k.startswith('_'))
        
        with open(path, 'w', newline='') as f:
            writer = csv.writer(f)
            headers = [':START_ID'] + list(all_keys) + [':END_ID', ':TYPE']
            writer.writerow(headers)
            
            for source, target, data in rels:
                row = [source]
                for key in all_keys:
                    row.append(data.get(key, ''))
                row.extend([target, rel_type])
                writer.writerow(row)
```

## Fluent query builder for readable queries

Chain query conditions for expressive, Cypher-like queries without a database:

```python
class GraphQuery:
    """Fluent query interface for NetworkX graphs."""
    
    def __init__(self, graph: nx.DiGraph):
        self.graph = graph
        self._node_filters = []
        self._edge_filters = []
        self._start_node = None
        self._depth_limit = None
    
    def from_node(self, node_id: str) -> 'GraphQuery':
        """Set traversal starting point."""
        self._start_node = node_id
        return self
    
    def within_hops(self, n: int) -> 'GraphQuery':
        """Limit traversal depth."""
        self._depth_limit = n
        return self
    
    def where_label(self, label: str) -> 'GraphQuery':
        """Filter nodes by label."""
        self._node_filters.append(
            lambda nid, data: data.get('_label') == label
        )
        return self
    
    def where_property(self, key: str, op: str, value: Any) -> 'GraphQuery':
        """Filter by property with comparison operator."""
        ops = {
            '==': lambda a, b: a == b,
            '!=': lambda a, b: a != b,
            '>': lambda a, b: a > b,
            '<': lambda a, b: a < b,
            'in': lambda a, b: a in b,
            'contains': lambda a, b: b in a if a else False,
        }
        self._node_filters.append(
            lambda nid, data: ops[op](data.get(key), value) if data.get(key) is not None else False
        )
        return self
    
    def where_relationship(self, rel_type: str) -> 'GraphQuery':
        """Filter edges by relationship type."""
        self._edge_filters.append(
            lambda s, t, data: data.get('_type') == rel_type
        )
        return self
    
    def get_nodes(self) -> List[str]:
        """Execute query and return matching node IDs."""
        if self._start_node:
            return self._traverse()
        return self._filter_all_nodes()
    
    def _passes_filters(self, node_id: str) -> bool:
        data = self.graph.nodes.get(node_id, {})
        return all(f(node_id, data) for f in self._node_filters)
    
    def _traverse(self) -> List[str]:
        visited = {self._start_node}
        queue = deque([(self._start_node, 0)])
        results = []
        
        while queue:
            node, depth = queue.popleft()
            if self._passes_filters(node):
                results.append(node)
            
            if self._depth_limit and depth >= self._depth_limit:
                continue
            
            for neighbor in self.graph.successors(node):
                if neighbor in visited:
                    continue
                
                # Check edge filters
                edge_data = self.graph.edges[node, neighbor]
                if self._edge_filters:
                    if not all(f(node, neighbor, edge_data) for f in self._edge_filters):
                        continue
                
                visited.add(neighbor)
                queue.append((neighbor, depth + 1))
        
        return results
    
    def _filter_all_nodes(self) -> List[str]:
        return [n for n in self.graph.nodes() if self._passes_filters(n)]
    
    def count(self) -> int:
        return len(self.get_nodes())
    
    def group_by(self, attr: str) -> Dict[Any, List[str]]:
        """Group results by attribute value."""
        groups = defaultdict(list)
        for node in self.get_nodes():
            val = self.graph.nodes[node].get(attr)
            groups[val].append(node)
        return dict(groups)
```

**Example queries:**

```python
kg = KnowledgeGraph().load_from_manifest('manifest.yaml')

# Find all exercises targeting gluteus maximus within 2 hops
glute_exercises = (kg.query()
    .from_node('muscle_gluteus_maximus')
    .within_hops(2)
    .where_label('Exercise')
    .where_relationship('TARGETS')
    .get_nodes())

# Find all protocols with high-confidence evidence
high_evidence = (kg.query()
    .where_label('Protocol')
    .where_property('evidence_tier', '==', 1)
    .get_nodes())

# Group exercises by category
by_category = (kg.query()
    .where_label('Exercise')
    .group_by('category'))
```

## Hybrid vector and graph search

Combine semantic embeddings with graph structure for powerful queries like "find concepts similar to X connected to Y":

```python
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss

class HybridKGSearch:
    """Combines semantic search with graph traversal."""
    
    def __init__(self, kg: KnowledgeGraph, 
                 model_name: str = 'all-MiniLM-L6-v2'):
        self.kg = kg
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
        self.index = None
        self.node_ids = []
        self.embeddings = None
    
    def build_index(self, cache_path: str = None):
        """Build FAISS index from node text representations."""
        if cache_path and self._load_cache(cache_path):
            return self
        
        # Create searchable text for each node
        texts = []
        self.node_ids = []
        
        for node_id, data in self.kg.graph.nodes(data=True):
            text = self._node_to_text(node_id, data)
            texts.append(text)
            self.node_ids.append(node_id)
        
        # Generate embeddings
        self.embeddings = self.model.encode(
            texts, 
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=True
        )
        
        # Build FAISS index (Flat for <100k nodes)
        self.index = faiss.IndexFlatIP(self.dimension)
        self.index.add(self.embeddings.astype('float32'))
        
        if cache_path:
            self._save_cache(cache_path)
        
        return self
    
    def _node_to_text(self, node_id: str, data: dict) -> str:
        """Convert node to searchable text."""
        parts = [
            data.get('name', node_id),
            data.get('description', ''),
            f"Type: {data.get('_label', '')}"
        ]
        return ' | '.join(filter(None, parts))
    
    def _save_cache(self, base_path: str):
        np.save(f"{base_path}_embeddings.npy", self.embeddings)
        with open(f"{base_path}_ids.json", 'w') as f:
            json.dump(self.node_ids, f)
        faiss.write_index(self.index, f"{base_path}_index.faiss")
    
    def _load_cache(self, base_path: str) -> bool:
        try:
            self.embeddings = np.load(f"{base_path}_embeddings.npy")
            with open(f"{base_path}_ids.json") as f:
                self.node_ids = json.load(f)
            self.index = faiss.read_index(f"{base_path}_index.faiss")
            return True
        except FileNotFoundError:
            return False
    
    def semantic_search(self, query: str, k: int = 20) -> List[tuple]:
        """Pure semantic similarity search."""
        query_emb = self.model.encode([query], normalize_embeddings=True)
        scores, indices = self.index.search(query_emb.astype('float32'), k)
        
        return [(self.node_ids[idx], float(score)) 
                for score, idx in zip(scores[0], indices[0]) if idx >= 0]
    
    def hybrid_search(self, query: str, anchor_node: str = None,
                      k: int = 10, alpha: float = 0.6) -> List[dict]:
        """
        Combine semantic similarity with graph proximity.
        
        Args:
            query: Search text
            anchor_node: Reference node to measure graph distance from
            k: Number of results
            alpha: Weight for semantic score (0-1). Higher = more semantic.
        """
        # Get semantic candidates
        semantic_results = self.semantic_search(query, k=k*5)
        
        if not anchor_node:
            return [{'node_id': nid, 'score': score, 'semantic': score}
                    for nid, score in semantic_results[:k]]
        
        # Score with graph distance
        results = []
        for node_id, sem_score in semantic_results:
            path = self.kg.shortest_path(anchor_node, node_id)
            if path:
                distance = len(path) - 1
                graph_score = 1.0 / (1.0 + distance)
            else:
                graph_score = 0.0
                distance = float('inf')
            
            hybrid = alpha * sem_score + (1 - alpha) * graph_score
            results.append({
                'node_id': node_id,
                'score': hybrid,
                'semantic': sem_score,
                'graph_distance': distance,
                'data': self.kg.get_node(node_id)
            })
        
        results.sort(key=lambda x: -x['score'])
        return results[:k]
    
    def search_neighborhood(self, query: str, center: str,
                            max_hops: int = 2, k: int = 10) -> List[dict]:
        """Find semantically similar nodes within N hops of center."""
        # Get neighborhood
        neighborhood = set()
        for node, depth in self.kg.bfs(center, depth_limit=max_hops):
            neighborhood.add(node)
        
        if not neighborhood:
            return []
        
        # Get embeddings for neighborhood
        neighbor_indices = [self.node_ids.index(n) for n in neighborhood 
                           if n in self.node_ids]
        neighbor_embeddings = self.embeddings[neighbor_indices]
        
        # Search within neighborhood
        query_emb = self.model.encode([query], normalize_embeddings=True)
        similarities = neighbor_embeddings @ query_emb.T
        
        results = []
        for i, idx in enumerate(neighbor_indices):
            node_id = self.node_ids[idx]
            results.append({
                'node_id': node_id,
                'similarity': float(similarities[i][0]),
                'data': self.kg.get_node(node_id)
            })
        
        results.sort(key=lambda x: -x['similarity'])
        return results[:k]
```

**Example hybrid queries:**

```python
search = HybridKGSearch(kg).build_index(cache_path='cache/embeddings')

# Find sleep protocols semantically similar to query, 
# prioritizing those connected to HRV metrics
results = search.hybrid_search(
    query="improve sleep quality and recovery",
    anchor_node="metric_hrv_rmssd",
    k=5,
    alpha=0.7
)

# Find exercises similar to "core strength" within 2 hops of deadlift
core_exercises = search.search_neighborhood(
    query="core stability and strength",
    center="ex_deadlift_001",
    max_hops=2
)
```

## Health/fitness schema with evidence tiers

Define entity types with confidence tracking for protocols and research claims:

```yaml
# ontology/evidence_tiers.yaml
evidence_tiers:
  tier_1:
    name: "Clinical Trial"
    description: "RCTs, systematic reviews, meta-analyses"
    confidence_weight: 1.0
    
  tier_2:
    name: "Observational"
    description: "Cohort studies, expert consensus, mechanisms"
    confidence_weight: 0.7
    
  tier_3:
    name: "Anecdotal"
    description: "Case reports, personal experience, traditional"
    confidence_weight: 0.3
```

```yaml
# nodes/protocols/huberman_sleep.yaml
nodes:
  proto_huberman_sleep_v1:
    _label: Protocol
    name: "Huberman Sleep Optimization"
    description: "Light exposure and temperature protocol for sleep"
    duration_days: 30
    category: "sleep_optimization"
    evidence:
      tier: 2
      confidence_level: 0.75
      sources:
        - url: "https://hubermanlab.com/sleep"
          type: "expert_opinion"
```

**Key relationship types for health domain:**

| Relationship | From | To | Purpose |
|-------------|------|-----|---------|
| `TARGETS` | Exercise | Muscle | Which muscles an exercise activates |
| `IMPROVES` | Protocol | Metric | What health metrics a protocol affects |
| `TRACKS` | Metric | Condition | Metrics that indicate conditions |
| `CORRELATES_WITH` | Metric | Metric | Statistical relationships between metrics |
| `REQUIRES` | Exercise | Exercise | Prerequisite movements |
| `INDICATES` | Symptom | Condition | Diagnostic relationships |

## Garmin data mapping to graph nodes

Map wearable data to your knowledge graph through aggregated daily summaries:

```yaml
# mappings/garmin_activity_mapping.yaml
garmin_to_graph:
  activities:
    running:
      maps_to: ex_running_v1
      derived_metrics:
        - avg_heart_rate -> metric_heart_rate
        - distance -> metric_distance
        - pace -> metric_pace
        
    strength_training:
      maps_to: ex_strength_generic_v1
      requires_manual_mapping: true
      
  daily_metrics:
    restingHeartRate: metric_resting_hr
    hrvStatus: metric_hrv_status  
    sleepScore: metric_sleep_score
    stressLevel: metric_stress_avg
    bodyBattery: metric_body_battery
```

```python
# Time-series represented as daily summary nodes
daily_summary = {
    '_label': 'DailySummary',
    'id': 'summary_2024_01_15',
    'date': '2024-01-15',
    'hrv_avg': 65.5,
    'sleep_score': 82,
    'stress_avg': 28,
    'steps': 8500,
    'active_calories': 450
}

# Linked to metrics via HAS_READING relationships
# (summary_2024_01_15) -[HAS_READING]-> (metric_hrv_rmssd)
```

## Performance expectations at your scale

| Operation | 150 nodes | 500 nodes | 1000 nodes |
|-----------|-----------|-----------|------------|
| **Full YAML load** | 50-100ms | 100-200ms | 200-500ms |
| **Pickle load** | 5-10ms | 10-20ms | 20-50ms |
| **Memory usage** | ~50KB | ~150KB | ~300KB |
| **BFS (depth 3)** | <1ms | 1-2ms | 2-5ms |
| **Shortest path** | <1ms | 1-2ms | 2-5ms |
| **Embedding build (MiniLM)** | 2-5s | 5-10s | 10-20s |
| **Semantic search** | 1-5ms | 1-5ms | 1-5ms |

**Optimization recommendations:**
- Use pickle cache after initial YAML load (**10x faster** reload)
- Build embedding index once, cache to disk, reload on startup
- For 500+ nodes, consider igraph if running many graph algorithms
- SQLite intermediate layer only needed at 5,000+ nodes

## Migration path to Neo4j/Memgraph

Your file structure exports cleanly to database formats. Run the export when ready:

```python
# Export to Neo4j CSV format
kg.export_neo4j_csv('exports/neo4j/')

# Files created:
# - exports/neo4j/exercise.csv
# - exports/neo4j/protocol.csv
# - exports/neo4j/targets.csv
# - exports/neo4j/improves.csv
```

**Neo4j import command:**
```bash
neo4j-admin database import full health_fitness \
  --nodes=exports/neo4j/exercise.csv \
  --nodes=exports/neo4j/protocol.csv \
  --relationships=exports/neo4j/targets.csv \
  --relationships=exports/neo4j/improves.csv
```

**Memgraph LOAD CSV:**
```cypher
LOAD CSV FROM "/imports/exercises.csv" WITH HEADER AS row
CREATE (e:Exercise {
  id: row.nodeId,
  name: row.name,
  category: row.category
});
```

## Conclusion

File-based knowledge graphs with NetworkX provide **production-ready performance** for graphs under 5,000 nodes—well beyond your current 150+ node scale. The architecture described here delivers sub-millisecond queries, hybrid semantic-graph search, and clean migration to Neo4j/Memgraph when resources permit.

Key implementation priorities: start with full in-memory loading (simplest approach at your scale), add pickle caching immediately for fast restarts, build the embedding index for semantic search, and use the fluent query API for readable graph traversals. The Cypher-compatible file structure ensures your data investment carries forward when migrating to a proper graph database.